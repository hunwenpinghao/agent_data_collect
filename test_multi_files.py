#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•å¤šæ–‡ä»¶æ•°æ®åŠ è½½åŠŸèƒ½
"""

import os
import sys
import json
import tempfile
from transformers import AutoTokenizer

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from fine_tune_qwen import SFTDataset

def create_test_data_files():
    """åˆ›å»ºæµ‹è¯•ç”¨çš„JSONLæ–‡ä»¶"""
    test_data_1 = [
        {
            "instruction": "æ ¹æ®ä»¥ä¸‹åº—é“ºä¿¡æ¯ï¼Œç”Ÿæˆä¸€æ®µå°çº¢ä¹¦é£æ ¼çš„æ–‡æ¡ˆï¼Œé£æ ¼ä¸ºã€Œç”œç¾ã€ï¼š",
            "input": "åº—é“ºåç§°ï¼šæ˜Ÿå·´å…‹\nå“ç±»ï¼šå’–å•¡\nåœ°å€ï¼šéƒ‘å·æ­£å¼˜åŸL8",
            "output": "âœ¨éƒ‘å·æ­£å¼˜åŸæ˜Ÿå·´å…‹ï¼Œæˆ‘çš„å’–å•¡å°å¤©åœ°ï¼"
        },
        {
            "instruction": "æ ¹æ®ä»¥ä¸‹åº—é“ºä¿¡æ¯ï¼Œç”Ÿæˆä¸€æ®µå°çº¢ä¹¦é£æ ¼çš„æ–‡æ¡ˆï¼Œé£æ ¼ä¸ºã€Œæ´»åŠ›ã€ï¼š",
            "input": "åº—é“ºåç§°ï¼šNike\nå“ç±»ï¼šè¿åŠ¨ç”¨å“\nåœ°å€ï¼šæ­£å¼˜åŸL2",
            "output": "ğŸƒâ€â™€ï¸æ­£å¼˜åŸNikeï¼Œç‚¹ç‡ƒä½ çš„è¿åŠ¨æ¿€æƒ…ï¼"
        }
    ]
    
    test_data_2 = [
        {
            "instruction": "æ ¹æ®ä»¥ä¸‹åº—é“ºä¿¡æ¯ï¼Œç”Ÿæˆä¸€æ®µå°çº¢ä¹¦é£æ ¼çš„æ–‡æ¡ˆï¼Œé£æ ¼ä¸ºã€Œæ¸©é¦¨ã€ï¼š",
            "input": "åº—é“ºåç§°ï¼šæµ·åº•æ\nå“ç±»ï¼šç«é”…\nåœ°å€ï¼šæ­£å¼˜åŸL7",
            "output": "ğŸ²æµ·åº•æï¼Œå’Œæœ‹å‹ä¸€èµ·æš–å¿ƒèšé¤çš„å¥½åœ°æ–¹ï¼"
        }
    ]
    
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    temp_dir = tempfile.mkdtemp()
    
    file1_path = os.path.join(temp_dir, "test_data_1.jsonl")
    file2_path = os.path.join(temp_dir, "test_data_2.jsonl")
    
    # å†™å…¥æµ‹è¯•æ•°æ®
    with open(file1_path, 'w', encoding='utf-8') as f:
        for item in test_data_1:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    with open(file2_path, 'w', encoding='utf-8') as f:
        for item in test_data_2:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    return file1_path, file2_path, temp_dir

def test_single_file(file1_path):
    """æµ‹è¯•å•æ–‡ä»¶åŠ è½½"""
    print("=== æµ‹è¯•å•æ–‡ä»¶åŠ è½½ ===")
    try:
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„tokenizerç”¨äºæµ‹è¯•
        tokenizer = type('MockTokenizer', (), {
            'pad_token': None,
            'eos_token': '</s>',
            '__call__': lambda self, text, **kwargs: {'input_ids': [1, 2, 3]},
            'decode': lambda self, ids, **kwargs: 'mock_output'
        })()
        
        dataset = SFTDataset(file1_path, tokenizer, max_seq_length=512)
        print(f"âœ… å•æ–‡ä»¶åŠ è½½æˆåŠŸï¼Œæ•°æ®é‡: {len(dataset)}")
        return True
    except Exception as e:
        print(f"âŒ å•æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        return False

def test_multi_files(file1_path, file2_path):
    """æµ‹è¯•å¤šæ–‡ä»¶åŠ è½½"""
    print("\n=== æµ‹è¯•å¤šæ–‡ä»¶åŠ è½½ ===")
    try:
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„tokenizerç”¨äºæµ‹è¯•
        tokenizer = type('MockTokenizer', (), {
            'pad_token': None,
            'eos_token': '</s>',
            '__call__': lambda self, text, **kwargs: {'input_ids': [1, 2, 3]},
            'decode': lambda self, ids, **kwargs: 'mock_output'
        })()
        
        # æµ‹è¯•é€—å·åˆ†éš”çš„å¤šæ–‡ä»¶
        multi_path = f"{file1_path},{file2_path}"
        dataset = SFTDataset(multi_path, tokenizer, max_seq_length=512)
        print(f"âœ… å¤šæ–‡ä»¶åŠ è½½æˆåŠŸï¼Œæ•°æ®é‡: {len(dataset)}")
        
        # æµ‹è¯•å¸¦ç©ºæ ¼çš„å¤šæ–‡ä»¶
        multi_path_with_spaces = f"{file1_path}, {file2_path}"
        dataset2 = SFTDataset(multi_path_with_spaces, tokenizer, max_seq_length=512)
        print(f"âœ… å¸¦ç©ºæ ¼çš„å¤šæ–‡ä»¶åŠ è½½æˆåŠŸï¼Œæ•°æ®é‡: {len(dataset2)}")
        
        return True
    except Exception as e:
        print(f"âŒ å¤šæ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        return False

def test_nonexistent_file():
    """æµ‹è¯•ä¸å­˜åœ¨çš„æ–‡ä»¶"""
    print("\n=== æµ‹è¯•ä¸å­˜åœ¨çš„æ–‡ä»¶ ===")
    try:
        tokenizer = type('MockTokenizer', (), {
            'pad_token': None,
            'eos_token': '</s>',
            '__call__': lambda self, text, **kwargs: {'input_ids': [1, 2, 3]},
            'decode': lambda self, ids, **kwargs: 'mock_output'
        })()
        
        # æµ‹è¯•ä¸å­˜åœ¨çš„æ–‡ä»¶
        dataset = SFTDataset("nonexistent.jsonl", tokenizer, max_seq_length=512)
        print(f"âŒ åº”è¯¥å¤±è´¥ä½†æˆåŠŸäº†")
        return False
    except Exception as e:
        print(f"âœ… æ­£ç¡®å¤„ç†äº†ä¸å­˜åœ¨çš„æ–‡ä»¶: {e}")
        return True

def test_mixed_files(file1_path):
    """æµ‹è¯•æ··åˆå­˜åœ¨å’Œä¸å­˜åœ¨çš„æ–‡ä»¶"""
    print("\n=== æµ‹è¯•æ··åˆæ–‡ä»¶ï¼ˆå­˜åœ¨+ä¸å­˜åœ¨ï¼‰===")
    try:
        tokenizer = type('MockTokenizer', (), {
            'pad_token': None,
            'eos_token': '</s>',
            '__call__': lambda self, text, **kwargs: {'input_ids': [1, 2, 3]},
            'decode': lambda self, ids, **kwargs: 'mock_output'
        })()
        
        # æµ‹è¯•ä¸€ä¸ªå­˜åœ¨ä¸€ä¸ªä¸å­˜åœ¨çš„æ–‡ä»¶
        mixed_path = f"{file1_path},nonexistent.jsonl"
        dataset = SFTDataset(mixed_path, tokenizer, max_seq_length=512)
        print(f"âœ… æ··åˆæ–‡ä»¶åŠ è½½æˆåŠŸï¼Œæ•°æ®é‡: {len(dataset)} (åº”è¯¥åªæœ‰å­˜åœ¨æ–‡ä»¶çš„æ•°æ®)")
        return True
    except Exception as e:
        print(f"âŒ æ··åˆæ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
        return False

def cleanup_test_files(temp_dir):
    """æ¸…ç†æµ‹è¯•æ–‡ä»¶"""
    import shutil
    try:
        shutil.rmtree(temp_dir)
        print(f"\nâœ… æ¸…ç†ä¸´æ—¶æ–‡ä»¶æˆåŠŸ: {temp_dir}")
    except Exception as e:
        print(f"\nâš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")

def test_multi_file_parsing():
    """æµ‹è¯•å¤šæ–‡ä»¶è·¯å¾„è§£æåŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•å¤šæ–‡ä»¶è·¯å¾„è§£æåŠŸèƒ½")
    print("=" * 40)
    
    test_cases = [
        ("å•æ–‡ä»¶", "file1.jsonl", ["file1.jsonl"]),
        ("å¤šæ–‡ä»¶(é€—å·åˆ†éš”)", "file1.jsonl,file2.jsonl", ["file1.jsonl", "file2.jsonl"]),
        ("å¤šæ–‡ä»¶(å¸¦ç©ºæ ¼)", "file1.jsonl, file2.jsonl, file3.jsonl", ["file1.jsonl", "file2.jsonl", "file3.jsonl"]),
        ("å¤šæ–‡ä»¶(ä¸è§„åˆ™ç©ºæ ¼)", " file1.jsonl , file2.jsonl,file3.jsonl ", ["file1.jsonl", "file2.jsonl", "file3.jsonl"]),
    ]
    
    for test_name, input_path, expected in test_cases:
        print(f"\næµ‹è¯•: {test_name}")
        print(f"è¾“å…¥: '{input_path}'")
        
        # æ¨¡æ‹Ÿ load_data ä¸­çš„è·¯å¾„è§£æé€»è¾‘
        if ',' in input_path:
            file_paths = [path.strip() for path in input_path.split(',') if path.strip()]
        else:
            file_paths = [input_path.strip()]
        
        print(f"è§£æç»“æœ: {file_paths}")
        print(f"é¢„æœŸç»“æœ: {expected}")
        
        if file_paths == expected:
            print("âœ… é€šè¿‡")
        else:
            print("âŒ å¤±è´¥")

def create_sample_files():
    """åˆ›å»ºç¤ºä¾‹è®­ç»ƒæ–‡ä»¶"""
    print("\nğŸ“ åˆ›å»ºç¤ºä¾‹è®­ç»ƒæ–‡ä»¶")
    print("=" * 40)
    
    # ç¤ºä¾‹æ•°æ®1
    data1 = [
        {
            "instruction": "æ ¹æ®ä»¥ä¸‹åº—é“ºä¿¡æ¯ï¼Œç”Ÿæˆä¸€æ®µå°çº¢ä¹¦é£æ ¼çš„æ–‡æ¡ˆï¼Œé£æ ¼ä¸ºã€Œç”œç¾ã€ï¼š",
            "input": "åº—é“ºåç§°ï¼šæ˜Ÿå·´å…‹\nå“ç±»ï¼šå’–å•¡",
            "output": "âœ¨æ˜Ÿå·´å…‹çš„å’–å•¡é¦™æ°”ï¼Œæ²»æ„ˆæ¯ä¸€ä¸ªåˆåï½"
        },
        {
            "instruction": "æ ¹æ®ä»¥ä¸‹åº—é“ºä¿¡æ¯ï¼Œç”Ÿæˆä¸€æ®µå°çº¢ä¹¦é£æ ¼çš„æ–‡æ¡ˆï¼Œé£æ ¼ä¸ºã€Œæ´»åŠ›ã€ï¼š",
            "input": "åº—é“ºåç§°ï¼šNike\nå“ç±»ï¼šè¿åŠ¨ç”¨å“",
            "output": "ğŸƒâ€â™€ï¸Nikeè¿åŠ¨è£…å¤‡ï¼Œè®©ä½ æ´»åŠ›æ»¡æ»¡ï¼"
        }
    ]
    
    # ç¤ºä¾‹æ•°æ®2
    data2 = [
        {
            "instruction": "æ ¹æ®ä»¥ä¸‹åº—é“ºä¿¡æ¯ï¼Œç”Ÿæˆä¸€æ®µå°çº¢ä¹¦é£æ ¼çš„æ–‡æ¡ˆï¼Œé£æ ¼ä¸ºã€Œæ¸©é¦¨ã€ï¼š",
            "input": "åº—é“ºåç§°ï¼šæµ·åº•æ\nå“ç±»ï¼šç«é”…",
            "output": "ğŸ²æµ·åº•æï¼Œå’Œæœ‹å‹ä¸€èµ·çš„æ¸©é¦¨æ—¶å…‰ï½"
        }
    ]
    
    # åˆ›å»ºæ–‡ä»¶
    files_created = []
    
    try:
        with open("sample_data_1.jsonl", "w", encoding="utf-8") as f:
            for item in data1:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")
        files_created.append("sample_data_1.jsonl")
        print("âœ… åˆ›å»º sample_data_1.jsonl (2æ¡æ•°æ®)")
        
        with open("sample_data_2.jsonl", "w", encoding="utf-8") as f:
            for item in data2:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")
        files_created.append("sample_data_2.jsonl")
        print("âœ… åˆ›å»º sample_data_2.jsonl (1æ¡æ•°æ®)")
        
        return files_created
        
    except Exception as e:
        print(f"âŒ åˆ›å»ºç¤ºä¾‹æ–‡ä»¶å¤±è´¥: {e}")
        return []

def show_usage_examples():
    """æ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹"""
    print("\nğŸ“– å¤šæ–‡ä»¶ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 40)
    
    examples = [
        {
            "title": "å•æ–‡ä»¶è®­ç»ƒ",
            "config": '{\n    "data_path": "store_xhs_sft_samples.jsonl"\n}',
            "command": "python3 fine_tune_qwen.py --config_file train_config.json"
        },
        {
            "title": "å¤šæ–‡ä»¶è®­ç»ƒ",
            "config": '{\n    "data_path": "file1.jsonl,file2.jsonl,file3.jsonl"\n}',
            "command": "python3 fine_tune_qwen.py --config_file train_config_multi_files.json"
        },
        {
            "title": "å‘½ä»¤è¡Œå‚æ•°",
            "config": None,
            "command": 'python3 fine_tune_qwen.py --data_path "file1.jsonl,file2.jsonl"'
        }
    ]
    
    for i, example in enumerate(examples, 1):
        print(f"\n{i}. {example['title']}:")
        if example['config']:
            print(f"   é…ç½®æ–‡ä»¶:")
            print(f"   {example['config']}")
        print(f"   å‘½ä»¤:")
        print(f"   {example['command']}")

def main():
    print("ğŸš€ Qwen3-0.5B å¤šæ–‡ä»¶è®­ç»ƒåŠŸèƒ½æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•è·¯å¾„è§£æ
    test_multi_file_parsing()
    
    # åˆ›å»ºç¤ºä¾‹æ–‡ä»¶
    files_created = create_sample_files()
    
    # æ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹
    show_usage_examples()
    
    print("\n" + "=" * 50)
    print("âœ¨ å¤šæ–‡ä»¶åŠŸèƒ½å·²å¯ç”¨ï¼")
    print("\nğŸ¯ ä¸»è¦ç‰¹æ€§:")
    print("   âœ… æ”¯æŒå•ä¸ªæ–‡ä»¶: 'data.jsonl'")
    print("   âœ… æ”¯æŒå¤šä¸ªæ–‡ä»¶: 'file1.jsonl,file2.jsonl,file3.jsonl'")
    print("   âœ… è‡ªåŠ¨å»é™¤ç©ºæ ¼: 'file1.jsonl, file2.jsonl'")
    print("   âœ… é”™è¯¯å¤„ç†: è·³è¿‡ä¸å­˜åœ¨çš„æ–‡ä»¶ï¼Œæ˜¾ç¤ºè­¦å‘Š")
    print("   âœ… è¯¦ç»†æ—¥å¿—: æ˜¾ç¤ºæ¯ä¸ªæ–‡ä»¶çš„åŠ è½½çŠ¶æ€")
    
    print("\nğŸ“‹ æµ‹è¯•å»ºè®®:")
    print("   1. ä½¿ç”¨ train_config_multi_files.json è¿›è¡Œå¤šæ–‡ä»¶è®­ç»ƒ")
    print("   2. æŸ¥çœ‹æ—¥å¿—è¾“å‡ºç¡®è®¤æ‰€æœ‰æ–‡ä»¶æ­£ç¡®åŠ è½½")
    print("   3. æ£€æŸ¥æ€»æ•°æ®é‡æ˜¯å¦ç¬¦åˆé¢„æœŸ")
    
    if files_created:
        print(f"\nğŸ§¹ æ¸…ç†ç¤ºä¾‹æ–‡ä»¶:")
        for file_path in files_created:
            try:
                os.remove(file_path)
                print(f"   âœ… åˆ é™¤ {file_path}")
            except:
                print(f"   âš ï¸ æ— æ³•åˆ é™¤ {file_path}")

if __name__ == "__main__":
    main() 