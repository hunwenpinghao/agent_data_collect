#!/bin/bash

# =============================================================================
# Qwen æ¨¡å‹è¯„ä¼°è„šæœ¬
# ç”¨æ³•: ./run_eval.sh [é€‰é¡¹]
# =============================================================================

set -e  # é‡åˆ°é”™è¯¯æ—¶é€€å‡º

# é¢œè‰²è¾“å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# æ—¥å¿—å‡½æ•°
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
show_help() {
    cat << EOF
Qwen æ¨¡å‹è¯„ä¼°è„šæœ¬

ç”¨æ³•: $0 [é€‰é¡¹]

é€‰é¡¹:
    -m, --model-path PATH       å¾®è°ƒåçš„æ¨¡å‹è·¯å¾„ (é»˜è®¤: ./output/checkpoint-best)
    -d, --data-path PATH        è¯„ä¼°æ•°æ®è·¯å¾„ (é»˜è®¤: ./data/test.jsonl)
    -o, --output-dir PATH       è¾“å‡ºç›®å½• (é»˜è®¤: ./eval_results)
    -b, --batch-size NUM        æ‰¹å¤„ç†å¤§å° (é»˜è®¤: 4)
    -t, --max-tokens NUM        æœ€å¤§ç”Ÿæˆtokenæ•° (é»˜è®¤: 512)
    --base-model PATH           åŸºç¡€æ¨¡å‹è·¯å¾„ (ç”¨äºå¯¹æ¯”)
    --temperature FLOAT         ç”Ÿæˆæ¸©åº¦ (é»˜è®¤: 0.7)
    --top-p FLOAT              Top-pé‡‡æ · (é»˜è®¤: 0.9)
    --device DEVICE            è®¾å¤‡: cpu|cuda|auto (é»˜è®¤: auto)
    --metrics METRICS          è¯„ä¼°æŒ‡æ ‡: bleu,rouge,exact_match (é»˜è®¤: bleu,rouge)
    --save-predictions         ä¿å­˜é¢„æµ‹ç»“æœ
    --verbose                  è¯¦ç»†è¾“å‡º
    -h, --help                 æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯

ç¤ºä¾‹:
    # åŸºæœ¬è¯„ä¼°
    $0 -m ./output/checkpoint-1000 -d ./data/test.jsonl
    
    # å®Œæ•´è¯„ä¼°
    $0 -m ./output/checkpoint-best -d ./data/test.jsonl \\
       --metrics bleu,rouge,exact_match \\
       --save-predictions --verbose
    
    # å¯¹æ¯”è¯„ä¼°
    $0 -m ./output/checkpoint-best --base-model Qwen/Qwen2.5-7B-Instruct \\
       -d ./data/test.jsonl

EOF
}

# é»˜è®¤é…ç½®
MODEL_PATH="./output/checkpoint-best"
DATA_PATH="./data/test.jsonl"
OUTPUT_DIR="./eval_results"
BATCH_SIZE=4
MAX_TOKENS=512
BASE_MODEL=""
TEMPERATURE=0.7
TOP_P=0.9
DEVICE="auto"
METRICS="bleu,rouge"
SAVE_PREDICTIONS=false
VERBOSE=false

# è§£æå‘½ä»¤è¡Œå‚æ•°
while [[ $# -gt 0 ]]; do
    case $1 in
        -m|--model-path)
            MODEL_PATH="$2"
            shift 2
            ;;
        -d|--data-path)
            DATA_PATH="$2"
            shift 2
            ;;
        -o|--output-dir)
            OUTPUT_DIR="$2"
            shift 2
            ;;
        -b|--batch-size)
            BATCH_SIZE="$2"
            shift 2
            ;;
        -t|--max-tokens)
            MAX_TOKENS="$2"
            shift 2
            ;;
        --base-model)
            BASE_MODEL="$2"
            shift 2
            ;;
        --temperature)
            TEMPERATURE="$2"
            shift 2
            ;;
        --top-p)
            TOP_P="$2"
            shift 2
            ;;
        --device)
            DEVICE="$2"
            shift 2
            ;;
        --metrics)
            METRICS="$2"
            shift 2
            ;;
        --save-predictions)
            SAVE_PREDICTIONS=true
            shift
            ;;
        --verbose)
            VERBOSE=true
            shift
            ;;
        -h|--help)
            show_help
            exit 0
            ;;
        *)
            log_error "æœªçŸ¥é€‰é¡¹: $1"
            show_help
            exit 1
            ;;
    esac
done

# éªŒè¯å‚æ•°
validate_args() {
    log_info "éªŒè¯å‚æ•°..."
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    if [[ ! -d "$MODEL_PATH" && ! -f "$MODEL_PATH" ]]; then
        log_error "æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: $MODEL_PATH"
        exit 1
    fi
    
    # æ£€æŸ¥æ•°æ®è·¯å¾„
    if [[ ! -f "$DATA_PATH" ]]; then
        log_error "æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: $DATA_PATH"
        exit 1
    fi
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    mkdir -p "$OUTPUT_DIR"
    
    log_success "å‚æ•°éªŒè¯é€šè¿‡"
}

# æ£€æŸ¥ä¾èµ–
check_dependencies() {
    log_info "æ£€æŸ¥ä¾èµ–..."
    
    # æ£€æŸ¥Pythonç¯å¢ƒ
    if ! command -v python &> /dev/null; then
        log_error "Pythonæœªå®‰è£…"
        exit 1
    fi
    
    # æ£€æŸ¥å¿…è¦çš„PythonåŒ…
    python -c "
import sys
required_packages = ['torch', 'transformers', 'datasets', 'numpy', 'pandas']
missing_packages = []

for package in required_packages:
    try:
        __import__(package)
    except ImportError:
        missing_packages.append(package)

if missing_packages:
    print(f'ç¼ºå°‘PythonåŒ…: {', '.join(missing_packages)}')
    sys.exit(1)
" 2>/dev/null
    
    if [[ $? -ne 0 ]]; then
        log_warning "éƒ¨åˆ†ä¾èµ–åŒ…å¯èƒ½ç¼ºå¤±ï¼Œä½†ç»§ç»­æ‰§è¡Œ..."
    fi
    
    log_success "ä¾èµ–æ£€æŸ¥å®Œæˆ"
}

# è®¾ç½®ç¯å¢ƒå˜é‡
setup_environment() {
    log_info "è®¾ç½®ç¯å¢ƒ..."
    
    # è®¾ç½®CUDAå¯è§æ€§
    if [[ "$DEVICE" == "auto" ]]; then
        export CUDA_VISIBLE_DEVICES="0"
    elif [[ "$DEVICE" == "cpu" ]]; then
        export CUDA_VISIBLE_DEVICES=""
    fi
    
    # è®¾ç½®ç¼“å­˜ç›®å½•
    export HF_HOME="./cache"
    export TRANSFORMERS_CACHE="./cache"
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if [[ "$VERBOSE" == "true" ]]; then
        export TRANSFORMERS_VERBOSITY="info"
    else
        export TRANSFORMERS_VERBOSITY="warning"
    fi
}

# åˆ›å»ºè¯„ä¼°è„šæœ¬
create_eval_script() {
    log_info "åˆ›å»ºè¯„ä¼°è„šæœ¬..."
    
    cat > "${OUTPUT_DIR}/eval_model.py" << 'EOF'
#!/usr/bin/env python3
"""
æ¨¡å‹è¯„ä¼°è„šæœ¬
"""

import os
import sys
import json
import argparse
import logging
import time
from pathlib import Path
from typing import List, Dict, Any

import torch
import pandas as pd
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    GenerationConfig
)
from tqdm import tqdm

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def load_model_and_tokenizer(model_path: str, device: str = "auto"):
    """åŠ è½½æ¨¡å‹å’Œtokenizer"""
    logger.info(f"åŠ è½½æ¨¡å‹: {model_path}")
    
    try:
        # åº”ç”¨ä¹‹å‰çš„ä¿®å¤
        sys.path.append(os.path.dirname(os.path.abspath(__file__)) + "/..")
        from fine_tune_qwen import apply_transformers_patch
        apply_transformers_patch()
        logger.info("å·²åº”ç”¨transformersä¿®å¤è¡¥ä¸")
    except Exception as e:
        logger.warning(f"æ— æ³•åº”ç”¨transformersä¿®å¤è¡¥ä¸: {e}")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_path, 
            trust_remote_code=True,
            padding_side="left"
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map=device if device != "cpu" else None,
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        if device == "cpu":
            model = model.to("cpu")
        
        logger.info("æ¨¡å‹åŠ è½½å®Œæˆ")
        return model, tokenizer
        
    except Exception as e:
        logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise

def load_eval_data(data_path: str) -> List[Dict]:
    """åŠ è½½è¯„ä¼°æ•°æ®"""
    logger.info(f"åŠ è½½è¯„ä¼°æ•°æ®: {data_path}")
    
    data = []
    try:
        with open(data_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line:
                    try:
                        data.append(json.loads(line))
                    except json.JSONDecodeError as e:
                        logger.warning(f"ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
                        continue
    except Exception as e:
        logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
        raise
    
    logger.info(f"æˆåŠŸåŠ è½½ {len(data)} æ¡è¯„ä¼°æ•°æ®")
    return data

def generate_response(
    model, 
    tokenizer, 
    prompt: str, 
    max_tokens: int = 512,
    temperature: float = 0.7,
    top_p: float = 0.9
) -> str:
    """ç”Ÿæˆå›å¤"""
    try:
        inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=1024)
        
        if model.device.type != "cpu":
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        generation_config = GenerationConfig(
            max_new_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            do_sample=True if temperature > 0 else False,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            repetition_penalty=1.1
        )
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                generation_config=generation_config
            )
        
        # è§£ç è¾“å‡ºï¼Œå»æ‰è¾“å…¥éƒ¨åˆ†
        input_length = inputs['input_ids'].shape[1]
        generated_tokens = outputs[0][input_length:]
        response = tokenizer.decode(generated_tokens, skip_special_tokens=True)
        
        return response.strip()
        
    except Exception as e:
        logger.warning(f"ç”Ÿæˆå¤±è´¥: {e}")
        return ""

def evaluate_generation(
    model, 
    tokenizer, 
    eval_data: List[Dict],
    batch_size: int = 4,
    max_tokens: int = 512,
    temperature: float = 0.7,
    top_p: float = 0.9,
    save_predictions: bool = False,
    output_dir: str = "./eval_results"
) -> Dict[str, Any]:
    """è¯„ä¼°ç”Ÿæˆä»»åŠ¡"""
    logger.info("å¼€å§‹ç”Ÿæˆè¯„ä¼°...")
    
    predictions = []
    references = []
    inputs_list = []
    
    for item in tqdm(eval_data, desc="ç”Ÿæˆè¯„ä¼°"):
        # æ„å»ºæç¤º
        if "instruction" in item and "input" in item:
            if item["input"].strip():
                prompt = f"æŒ‡ä»¤: {item['instruction']}\nè¾“å…¥: {item['input']}\nå›ç­”: "
            else:
                prompt = f"æŒ‡ä»¤: {item['instruction']}\nå›ç­”: "
        elif "question" in item:
            prompt = f"é—®é¢˜: {item['question']}\nå›ç­”: "
        elif "prompt" in item:
            prompt = item["prompt"]
        else:
            prompt = str(item.get("input", ""))
        
        inputs_list.append(prompt)
        
        # ç”Ÿæˆå›å¤
        prediction = generate_response(
            model, tokenizer, prompt, 
            max_tokens, temperature, top_p
        )
        predictions.append(prediction)
        
        # è·å–å‚è€ƒç­”æ¡ˆ
        reference = item.get("output", item.get("answer", item.get("response", "")))
        references.append(str(reference))
    
    # ä¿å­˜é¢„æµ‹ç»“æœ
    if save_predictions:
        results_file = os.path.join(output_dir, "predictions.jsonl")
        with open(results_file, 'w', encoding='utf-8') as f:
            for i, (inp, pred, ref) in enumerate(zip(inputs_list, predictions, references)):
                result = {
                    "id": i,
                    "input": inp,
                    "prediction": pred,
                    "reference": ref,
                    "original_data": eval_data[i]
                }
                f.write(json.dumps(result, ensure_ascii=False) + '\n')
        logger.info(f"é¢„æµ‹ç»“æœä¿å­˜åˆ°: {results_file}")
    
    return {
        "predictions": predictions,
        "references": references,
        "inputs": inputs_list,
        "num_samples": len(predictions)
    }

def calculate_metrics(predictions: List[str], references: List[str], metrics: List[str]) -> Dict[str, float]:
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    logger.info("è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    
    results = {}
    
    # è¿‡æ»¤æœ‰æ•ˆçš„é¢„æµ‹å’Œå‚è€ƒ
    valid_pairs = [(p.strip(), r.strip()) for p, r in zip(predictions, references) 
                   if p.strip() and r.strip()]
    
    if not valid_pairs:
        logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹-å‚è€ƒå¯¹")
        return {"error": "no_valid_pairs"}
    
    valid_predictions, valid_references = zip(*valid_pairs)
    
    # BLEUåˆ†æ•°
    if "bleu" in metrics:
        try:
            from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
            import nltk
            nltk.download('punkt', quiet=True)
            
            bleu_scores = []
            smoothing = SmoothingFunction().method1
            
            for pred, ref in zip(valid_predictions, valid_references):
                pred_tokens = pred.lower().split()
                ref_tokens = [ref.lower().split()]
                
                if len(pred_tokens) > 0 and len(ref_tokens[0]) > 0:
                    score = sentence_bleu(ref_tokens, pred_tokens, smoothing_function=smoothing)
                    bleu_scores.append(score)
            
            results["bleu"] = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0
            
        except Exception as e:
            logger.warning(f"BLEUè®¡ç®—å¤±è´¥: {e}")
            results["bleu"] = 0.0
    
    # ROUGEåˆ†æ•°
    if "rouge" in metrics:
        try:
            from rouge import Rouge
            rouge = Rouge()
            
            scores = rouge.get_scores(list(valid_predictions), list(valid_references), avg=True)
            results["rouge-1"] = scores["rouge-1"]["f"]
            results["rouge-2"] = scores["rouge-2"]["f"]
            results["rouge-l"] = scores["rouge-l"]["f"]
            
        except Exception as e:
            logger.warning(f"ROUGEè®¡ç®—å¤±è´¥: {e}")
            try:
                # ç®€å•çš„ROUGE-Lè¿‘ä¼¼
                def lcs_length(s1, s2):
                    m, n = len(s1), len(s2)
                    dp = [[0] * (n + 1) for _ in range(m + 1)]
                    for i in range(1, m + 1):
                        for j in range(1, n + 1):
                            if s1[i-1] == s2[j-1]:
                                dp[i][j] = dp[i-1][j-1] + 1
                            else:
                                dp[i][j] = max(dp[i-1][j], dp[i][j-1])
                    return dp[m][n]
                
                rouge_l_scores = []
                for pred, ref in zip(valid_predictions, valid_references):
                    pred_tokens = pred.lower().split()
                    ref_tokens = ref.lower().split()
                    lcs_len = lcs_length(pred_tokens, ref_tokens)
                    if len(ref_tokens) > 0:
                        rouge_l = lcs_len / len(ref_tokens)
                        rouge_l_scores.append(rouge_l)
                
                results["rouge-l"] = sum(rouge_l_scores) / len(rouge_l_scores) if rouge_l_scores else 0.0
                results["rouge-1"] = 0.0
                results["rouge-2"] = 0.0
                
            except Exception as e2:
                logger.warning(f"ç®€å•ROUGEè®¡ç®—ä¹Ÿå¤±è´¥: {e2}")
                results["rouge-1"] = 0.0
                results["rouge-2"] = 0.0
                results["rouge-l"] = 0.0
    
    # ç²¾ç¡®åŒ¹é…
    if "exact_match" in metrics:
        exact_matches = sum(1 for p, r in zip(valid_predictions, valid_references) 
                           if p.lower().strip() == r.lower().strip())
        results["exact_match"] = exact_matches / len(valid_predictions) if valid_predictions else 0.0
    
    # ç»Ÿè®¡ä¿¡æ¯
    results["num_valid_pairs"] = len(valid_pairs)
    results["avg_prediction_length"] = sum(len(p.split()) for p in valid_predictions) / len(valid_predictions)
    results["avg_reference_length"] = sum(len(r.split()) for r in valid_references) / len(valid_references)
    
    return results

def main():
    parser = argparse.ArgumentParser(description="æ¨¡å‹è¯„ä¼°")
    parser.add_argument("--model-path", required=True, help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--data-path", required=True, help="è¯„ä¼°æ•°æ®è·¯å¾„")
    parser.add_argument("--output-dir", default="./eval_results", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--batch-size", type=int, default=4, help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--max-tokens", type=int, default=512, help="æœ€å¤§ç”Ÿæˆtokenæ•°")
    parser.add_argument("--temperature", type=float, default=0.7, help="ç”Ÿæˆæ¸©åº¦")
    parser.add_argument("--top-p", type=float, default=0.9, help="Top-pé‡‡æ ·")
    parser.add_argument("--device", default="auto", help="è®¾å¤‡")
    parser.add_argument("--metrics", default="bleu,rouge,exact_match", help="è¯„ä¼°æŒ‡æ ‡")
    parser.add_argument("--save-predictions", action="store_true", help="ä¿å­˜é¢„æµ‹ç»“æœ")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    try:
        # åŠ è½½æ¨¡å‹å’Œæ•°æ®
        model, tokenizer = load_model_and_tokenizer(args.model_path, args.device)
        eval_data = load_eval_data(args.data_path)
        
        # è¯„ä¼°
        eval_results = evaluate_generation(
            model, tokenizer, eval_data,
            batch_size=args.batch_size,
            max_tokens=args.max_tokens,
            temperature=args.temperature,
            top_p=args.top_p,
            save_predictions=args.save_predictions,
            output_dir=args.output_dir
        )
        
        # è®¡ç®—æŒ‡æ ‡
        metrics_list = [m.strip() for m in args.metrics.split(',')]
        metrics = calculate_metrics(
            eval_results["predictions"],
            eval_results["references"],
            metrics_list
        )
        
        # ä¿å­˜ç»“æœ
        results = {
            "model_path": args.model_path,
            "data_path": args.data_path,
            "num_samples": eval_results["num_samples"],
            "metrics": metrics,
            "config": {
                "batch_size": args.batch_size,
                "max_tokens": args.max_tokens,
                "temperature": args.temperature,
                "top_p": args.top_p,
                "device": args.device,
                "metrics": args.metrics
            },
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        results_file = os.path.join(args.output_dir, "evaluation_results.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°ç»“æœ
        print("\n" + "="*60)
        print("æ¨¡å‹è¯„ä¼°ç»“æœ")
        print("="*60)
        print(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
        print(f"æ•°æ®è·¯å¾„: {args.data_path}")
        print(f"æ€»æ ·æœ¬æ•°: {eval_results['num_samples']}")
        if "num_valid_pairs" in metrics:
            print(f"æœ‰æ•ˆæ ·æœ¬æ•°: {metrics['num_valid_pairs']}")
        print(f"æ—¶é—´æˆ³: {results['timestamp']}")
        print("\nè¯„ä¼°æŒ‡æ ‡:")
        for metric, score in metrics.items():
            if not metric.startswith("num_") and not metric.startswith("avg_"):
                print(f"  {metric.upper()}: {score:.4f}")
        
        print(f"\nç»Ÿè®¡ä¿¡æ¯:")
        if "avg_prediction_length" in metrics:
            print(f"  å¹³å‡é¢„æµ‹é•¿åº¦: {metrics['avg_prediction_length']:.1f} tokens")
        if "avg_reference_length" in metrics:
            print(f"  å¹³å‡å‚è€ƒé•¿åº¦: {metrics['avg_reference_length']:.1f} tokens")
        
        print(f"\nç»“æœæ–‡ä»¶: {results_file}")
        if args.save_predictions:
            print(f"é¢„æµ‹æ–‡ä»¶: {os.path.join(args.output_dir, 'predictions.jsonl')}")
        print("="*60)
        
    except Exception as e:
        logger.error(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        raise

if __name__ == "__main__":
    main()
EOF
    
    log_success "è¯„ä¼°è„šæœ¬åˆ›å»ºå®Œæˆ"
}

# è¿è¡Œè¯„ä¼°
run_evaluation() {
    log_info "å¼€å§‹è¯„ä¼°..."
    
    # æ„å»ºPythonå‘½ä»¤
    python_cmd="python ${OUTPUT_DIR}/eval_model.py"
    python_cmd+=" --model-path \"$MODEL_PATH\""
    python_cmd+=" --data-path \"$DATA_PATH\""
    python_cmd+=" --output-dir \"$OUTPUT_DIR\""
    python_cmd+=" --batch-size $BATCH_SIZE"
    python_cmd+=" --max-tokens $MAX_TOKENS"
    python_cmd+=" --temperature $TEMPERATURE"
    python_cmd+=" --top-p $TOP_P"
    python_cmd+=" --device $DEVICE"
    python_cmd+=" --metrics \"$METRICS\""
    
    if [[ "$SAVE_PREDICTIONS" == "true" ]]; then
        python_cmd+=" --save-predictions"
    fi
    
    if [[ "$VERBOSE" == "true" ]]; then
        log_info "æ‰§è¡Œå‘½ä»¤: $python_cmd"
    fi
    
    # æ‰§è¡Œè¯„ä¼°
    eval $python_cmd
    
    if [[ $? -eq 0 ]]; then
        log_success "è¯„ä¼°å®Œæˆï¼"
        log_info "ç»“æœä¿å­˜åœ¨: $OUTPUT_DIR"
        
        # æ˜¾ç¤ºç®€è¦ç»“æœ
        if [[ -f "$OUTPUT_DIR/evaluation_results.json" ]]; then
            echo ""
            echo "=== å¿«é€ŸæŸ¥çœ‹ç»“æœ ==="
            python -c "
import json
try:
    with open('$OUTPUT_DIR/evaluation_results.json', 'r') as f:
        results = json.load(f)
    print(f\"âœ… è¯„ä¼°æˆåŠŸå®Œæˆ\")
    print(f\"ğŸ“Š æ ·æœ¬æ•°: {results['num_samples']}\")
    print(f\"ğŸ¯ ä¸»è¦æŒ‡æ ‡:\")
    for metric, score in results['metrics'].items():
        if metric in ['bleu', 'rouge-1', 'rouge-l', 'exact_match']:
            print(f\"   {metric.upper()}: {score:.4f}\")
    print(f\"ğŸ“ è¯¦ç»†ç»“æœ: $OUTPUT_DIR/evaluation_results.json\")
except Exception as e:
    print(f\"âŒ æ— æ³•è¯»å–ç»“æœæ–‡ä»¶: {e}\")
"
        fi
    else
        log_error "è¯„ä¼°å¤±è´¥"
        exit 1
    fi
}

# ä¸»å‡½æ•°
main() {
    echo ""
    log_info "==================== Qwen æ¨¡å‹è¯„ä¼°è„šæœ¬ ===================="
    echo ""
    
    # æ˜¾ç¤ºé…ç½®
    log_info "è¯„ä¼°é…ç½®:"
    echo "  ğŸ“ æ¨¡å‹è·¯å¾„: $MODEL_PATH"
    echo "  ğŸ“„ æ•°æ®è·¯å¾„: $DATA_PATH"
    echo "  ğŸ“‚ è¾“å‡ºç›®å½•: $OUTPUT_DIR"
    echo "  ğŸ”¢ æ‰¹å¤„ç†å¤§å°: $BATCH_SIZE"
    echo "  ğŸ¯ æœ€å¤§tokenæ•°: $MAX_TOKENS"
    echo "  ğŸŒ¡ï¸  ç”Ÿæˆæ¸©åº¦: $TEMPERATURE"
    echo "  ğŸ“Š è¯„ä¼°æŒ‡æ ‡: $METRICS"
    echo "  ğŸ’» è®¡ç®—è®¾å¤‡: $DEVICE"
    if [[ "$SAVE_PREDICTIONS" == "true" ]]; then
        echo "  ğŸ’¾ ä¿å­˜é¢„æµ‹: æ˜¯"
    fi
    echo ""
    
    # æ‰§è¡Œæ­¥éª¤
    validate_args
    check_dependencies
    setup_environment
    create_eval_script
    run_evaluation
    
    echo ""
    log_success "ğŸ‰ è¯„ä¼°æµç¨‹å…¨éƒ¨å®Œæˆï¼"
    echo ""
}

# é”™è¯¯å¤„ç†
trap 'log_error "è„šæœ¬æ‰§è¡Œå¤±è´¥ï¼Œè¡Œå·: $LINENO"' ERR

# æ£€æŸ¥æ˜¯å¦ç›´æ¥æ‰§è¡Œ
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    # æ‰§è¡Œä¸»å‡½æ•°
    main "$@"
fi 