#!/usr/bin/env python3
"""
Qwen æ¨¡å‹è¯„ä¼°è„šæœ¬
æ”¯æŒå¤šç§è¯„ä¼°æŒ‡æ ‡å’Œçµæ´»çš„é…ç½®é€‰é¡¹
"""

import os
import sys
import json
import argparse
import logging
import time
from pathlib import Path
from typing import List, Dict, Any, Optional

import torch
import pandas as pd
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    GenerationConfig
)
from tqdm import tqdm

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def apply_transformers_patch():
    """åº”ç”¨transformersåº“çš„å…¼å®¹æ€§ä¿®å¤"""
    try:
        # å°è¯•å¯¼å…¥ç°æœ‰çš„ä¿®å¤å‡½æ•°
        from fine_tune_qwen import apply_transformers_patch as patch_func
        patch_func()
        logger.info("âœ… å·²åº”ç”¨transformersä¿®å¤è¡¥ä¸")
        return True
    except ImportError:
        logger.warning("âš ï¸ æ— æ³•å¯¼å…¥fine_tune_qwenæ¨¡å—ï¼Œå°è¯•å†…ç½®ä¿®å¤...")
        try:
            import transformers.modeling_utils as modeling_utils
            
            # ä¿å­˜åŸå§‹æ–¹æ³•
            original_post_init = modeling_utils.PreTrainedModel.post_init
            
            def patched_post_init(self):
                """ä¿®å¤åçš„post_initæ–¹æ³•"""
                try:
                    # æ£€æŸ¥å¹¶ä¿®å¤å¯èƒ½çš„Noneå€¼
                    if hasattr(self.config, 'pretraining_tp') and self.config.pretraining_tp is None:
                        self.config.pretraining_tp = 1
                    
                    # ä¿®å¤å¼ é‡å¹¶è¡Œæ ·å¼é”™è¯¯
                    tensor_parallel_attrs = ['tensor_parallel_style', 'parallel_style']
                    supported_styles = ['tp', 'dp', 'pp', 'cp']
                    
                    for attr in tensor_parallel_attrs:
                        if hasattr(self.config, attr):
                            current_value = getattr(self.config, attr)
                            if current_value is not None and current_value not in supported_styles:
                                setattr(self.config, attr, 'tp')
                    
                    # å…¶ä»–å¸¸è§ä¿®å¤
                    fixes = {
                        'attention_dropout': 0.0,
                        'rope_scaling': None,
                        'attn_implementation': 'eager',
                        'use_sliding_window': False,
                        'sliding_window': 4096
                    }
                    
                    for key, default_val in fixes.items():
                        if hasattr(self.config, key) and getattr(self.config, key) is None:
                            setattr(self.config, key, default_val)
                    
                    # è°ƒç”¨åŸå§‹æ–¹æ³•
                    return original_post_init(self)
                    
                except Exception as e:
                    logger.warning(f"post_initä¿®å¤è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                    pass
            
            # åº”ç”¨patch
            modeling_utils.PreTrainedModel.post_init = patched_post_init
            logger.info("âœ… å·²åº”ç”¨å†…ç½®transformersä¿®å¤è¡¥ä¸")
            return True
            
        except Exception as e:
            logger.warning(f"âŒ æ— æ³•åº”ç”¨transformersè¡¥ä¸: {e}")
            return False
    except Exception as e:
        logger.warning(f"âŒ transformersè¡¥ä¸åº”ç”¨å¤±è´¥: {e}")
        return False

def load_model_and_tokenizer(model_path: str, device: str = "auto", torch_dtype: str = "float16"):
    """åŠ è½½æ¨¡å‹å’Œtokenizer"""
    logger.info(f"ğŸ”„ åŠ è½½æ¨¡å‹: {model_path}")
    
    # åº”ç”¨ä¿®å¤è¡¥ä¸
    apply_transformers_patch()
    
    try:
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            model_path, 
            trust_remote_code=True,
            padding_side="left"
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # è®¾ç½®æ•°æ®ç±»å‹
        dtype_map = {
            "float16": torch.float16,
            "float32": torch.float32,
            "bfloat16": torch.bfloat16
        }
        torch_dtype_obj = dtype_map.get(torch_dtype, torch.float16)
        
        # åŠ è½½æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch_dtype_obj,
            device_map=device if device != "cpu" else None,
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        if device == "cpu":
            model = model.to("cpu")
        
        logger.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (è®¾å¤‡: {model.device.type})")
        return model, tokenizer
        
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise

def load_eval_data(data_path: str) -> List[Dict]:
    """åŠ è½½è¯„ä¼°æ•°æ®"""
    logger.info(f"ğŸ“„ åŠ è½½è¯„ä¼°æ•°æ®: {data_path}")
    
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"è¯„ä¼°æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
    
    data = []
    try:
        with open(data_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line:
                    try:
                        item = json.loads(line)
                        data.append(item)
                    except json.JSONDecodeError as e:
                        logger.warning(f"ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
                        continue
    except Exception as e:
        logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
        raise
    
    logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(data)} æ¡è¯„ä¼°æ•°æ®")
    return data

def build_prompt(item: Dict) -> str:
    """æ ¹æ®æ•°æ®æ ¼å¼æ„å»ºæç¤º"""
    # æ ¼å¼1: instruction + input + output
    if "instruction" in item:
        instruction = item["instruction"]
        input_text = item.get("input", "")
        if input_text.strip():
            return f"æŒ‡ä»¤: {instruction}\nè¾“å…¥: {input_text}\nå›ç­”: "
        else:
            return f"æŒ‡ä»¤: {instruction}\nå›ç­”: "
    
    # æ ¼å¼2: question + answer
    elif "question" in item:
        return f"é—®é¢˜: {item['question']}\nå›ç­”: "
    
    # æ ¼å¼3: prompt + response
    elif "prompt" in item:
        return item["prompt"]
    
    # æ ¼å¼4: é€šç”¨è¾“å…¥
    else:
        return str(item.get("input", item.get("text", "")))

def extract_reference(item: Dict) -> str:
    """æå–å‚è€ƒç­”æ¡ˆ"""
    # ä¼˜å…ˆçº§: output > answer > response > target
    for key in ["output", "answer", "response", "target"]:
        if key in item and item[key]:
            return str(item[key])
    
    logger.warning(f"âš ï¸ æ— æ³•æ‰¾åˆ°å‚è€ƒç­”æ¡ˆ: {item}")
    return ""

def generate_response(
    model, 
    tokenizer, 
    prompt: str, 
    max_tokens: int = 512,
    temperature: float = 0.7,
    top_p: float = 0.9,
    repetition_penalty: float = 1.1
) -> str:
    """ç”Ÿæˆæ¨¡å‹å›å¤"""
    try:
        # ç¼–ç è¾“å…¥
        inputs = tokenizer(
            prompt, 
            return_tensors="pt", 
            padding=True, 
            truncation=True, 
            max_length=2048
        )
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        if model.device.type != "cpu":
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        # ç”Ÿæˆé…ç½®
        generation_config = GenerationConfig(
            max_new_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            do_sample=temperature > 0,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            repetition_penalty=repetition_penalty,
            no_repeat_ngram_size=3
        )
        
        # ç”Ÿæˆå›å¤
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                generation_config=generation_config
            )
        
        # è§£ç è¾“å‡ºï¼Œå»æ‰è¾“å…¥éƒ¨åˆ†
        input_length = inputs['input_ids'].shape[1]
        generated_tokens = outputs[0][input_length:]
        response = tokenizer.decode(generated_tokens, skip_special_tokens=True)
        
        return response.strip()
        
    except Exception as e:
        logger.warning(f"âš ï¸ ç”Ÿæˆå¤±è´¥: {e}")
        return ""

def generate_batch_responses(
    model, 
    tokenizer, 
    prompts: List[str], 
    max_tokens: int = 512,
    temperature: float = 0.7,
    top_p: float = 0.9,
    repetition_penalty: float = 1.1
) -> List[str]:
    """æ‰¹é‡ç”Ÿæˆæ¨¡å‹å›å¤"""
    try:
        # æ‰¹é‡ç¼–ç è¾“å…¥
        inputs = tokenizer(
            prompts, 
            return_tensors="pt", 
            padding=True, 
            truncation=True, 
            max_length=2048
        )
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        if model.device.type != "cpu":
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        # ç”Ÿæˆé…ç½®
        generation_config = GenerationConfig(
            max_new_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            do_sample=temperature > 0,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            repetition_penalty=repetition_penalty,
            no_repeat_ngram_size=3
        )
        
        # æ‰¹é‡ç”Ÿæˆå›å¤
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                generation_config=generation_config
            )
        
        # è§£ç è¾“å‡ºï¼Œå»æ‰è¾“å…¥éƒ¨åˆ†
        responses = []
        input_lengths = inputs['input_ids'].shape[1]
        
        for i, output in enumerate(outputs):
            # å¯¹äºæ‰¹é‡ç”Ÿæˆï¼Œæ¯ä¸ªæ ·æœ¬çš„è¾“å…¥é•¿åº¦å¯èƒ½ä¸åŒ
            # ä½¿ç”¨attention_maskæ‰¾åˆ°å®é™…çš„è¾“å…¥é•¿åº¦
            actual_input_length = inputs['attention_mask'][i].sum().item()
            generated_tokens = output[actual_input_length:]
            response = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            responses.append(response.strip())
        
        return responses
        
    except Exception as e:
        logger.warning(f"âš ï¸ æ‰¹é‡ç”Ÿæˆå¤±è´¥: {e}")
        # å›é€€åˆ°å•ä¸ªç”Ÿæˆ
        return [generate_response(model, tokenizer, prompt, max_tokens, temperature, top_p) 
                for prompt in prompts]

def evaluate_generation(
    model, 
    tokenizer, 
    eval_data: List[Dict],
    batch_size: int = 4,
    max_tokens: int = 512,
    temperature: float = 0.7,
    top_p: float = 0.9,
    save_predictions: bool = False,
    output_dir: str = "./eval_results"
) -> Dict[str, Any]:
    """æ‰§è¡Œç”Ÿæˆè¯„ä¼°"""
    logger.info(f"ğŸš€ å¼€å§‹ç”Ÿæˆè¯„ä¼° (batch_size={batch_size})...")
    
    predictions = []
    references = []
    inputs_list = []
    
    # å‡†å¤‡æ‰€æœ‰æ•°æ®
    all_prompts = []
    all_references = []
    
    for item in eval_data:
        prompt = build_prompt(item)
        reference = extract_reference(item)
        all_prompts.append(prompt)
        all_references.append(reference)
    
    # æ‰¹é‡å¤„ç†
    total_batches = (len(all_prompts) + batch_size - 1) // batch_size
    
    for i in tqdm(range(0, len(all_prompts), batch_size), 
                  desc="æ‰¹é‡ç”Ÿæˆ", unit="batch", total=total_batches):
        batch_prompts = all_prompts[i:i + batch_size]
        batch_references = all_references[i:i + batch_size]
        
        # æ‰¹é‡ç”Ÿæˆ
        if len(batch_prompts) == 1:
            # å•ä¸ªæ ·æœ¬ä½¿ç”¨åŸæ¥çš„æ–¹æ³•
            batch_predictions = [generate_response(
                model, tokenizer, batch_prompts[0], 
                max_tokens, temperature, top_p
            )]
        else:
            # å¤šä¸ªæ ·æœ¬ä½¿ç”¨æ‰¹é‡ç”Ÿæˆ
            batch_predictions = generate_batch_responses(
                model, tokenizer, batch_prompts,
                max_tokens, temperature, top_p
            )
        
        # æ”¶é›†ç»“æœ
        predictions.extend(batch_predictions)
        references.extend(batch_references)
        inputs_list.extend(batch_prompts)
    
    # ä¿å­˜é¢„æµ‹ç»“æœ
    if save_predictions:
        results_file = os.path.join(output_dir, "predictions.jsonl")
        with open(results_file, 'w', encoding='utf-8') as f:
            for i, (inp, pred, ref) in enumerate(zip(inputs_list, predictions, references)):
                result = {
                    "id": i,
                    "input": inp,
                    "prediction": pred,
                    "reference": ref,
                    "original_data": eval_data[i]
                }
                f.write(json.dumps(result, ensure_ascii=False) + '\n')
        logger.info(f"ğŸ’¾ é¢„æµ‹ç»“æœä¿å­˜åˆ°: {results_file}")
    
    return {
        "predictions": predictions,
        "references": references,
        "inputs": inputs_list,
        "num_samples": len(predictions)
    }

def calculate_bleu_score(predictions: List[str], references: List[str]) -> float:
    """è®¡ç®—BLEUåˆ†æ•°"""
    try:
        from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
        import nltk
        nltk.download('punkt', quiet=True)
        
        bleu_scores = []
        smoothing = SmoothingFunction().method1
        
        for pred, ref in zip(predictions, references):
            pred_tokens = pred.lower().split()
            ref_tokens = [ref.lower().split()]
            
            if len(pred_tokens) > 0 and len(ref_tokens[0]) > 0:
                score = sentence_bleu(ref_tokens, pred_tokens, smoothing_function=smoothing)
                bleu_scores.append(score)
        
        return sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0
        
    except Exception as e:
        logger.warning(f"BLEUè®¡ç®—å¤±è´¥: {e}")
        return 0.0

def calculate_rouge_scores(predictions: List[str], references: List[str]) -> Dict[str, float]:
    """è®¡ç®—ROUGEåˆ†æ•°"""
    try:
        from rouge import Rouge
        rouge = Rouge()
        
        scores = rouge.get_scores(predictions, references, avg=True)
        return {
            "rouge-1": scores["rouge-1"]["f"],
            "rouge-2": scores["rouge-2"]["f"],
            "rouge-l": scores["rouge-l"]["f"]
        }
        
    except Exception as e:
        logger.warning(f"ROUGEè®¡ç®—å¤±è´¥: {e}")
        # ç®€å•çš„ROUGE-Lè¿‘ä¼¼
        try:
            def lcs_length(s1, s2):
                m, n = len(s1), len(s2)
                dp = [[0] * (n + 1) for _ in range(m + 1)]
                for i in range(1, m + 1):
                    for j in range(1, n + 1):
                        if s1[i-1] == s2[j-1]:
                            dp[i][j] = dp[i-1][j-1] + 1
                        else:
                            dp[i][j] = max(dp[i-1][j], dp[i][j-1])
                return dp[m][n]
            
            rouge_l_scores = []
            for pred, ref in zip(predictions, references):
                pred_tokens = pred.lower().split()
                ref_tokens = ref.lower().split()
                if len(ref_tokens) > 0:
                    lcs_len = lcs_length(pred_tokens, ref_tokens)
                    rouge_l = lcs_len / len(ref_tokens)
                    rouge_l_scores.append(rouge_l)
            
            return {
                "rouge-1": 0.0,
                "rouge-2": 0.0,
                "rouge-l": sum(rouge_l_scores) / len(rouge_l_scores) if rouge_l_scores else 0.0
            }
            
        except Exception as e2:
            logger.warning(f"ç®€å•ROUGEè®¡ç®—ä¹Ÿå¤±è´¥: {e2}")
            return {"rouge-1": 0.0, "rouge-2": 0.0, "rouge-l": 0.0}

def calculate_metrics(predictions: List[str], references: List[str], metrics: List[str]) -> Dict[str, float]:
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    logger.info("ğŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    
    # è¿‡æ»¤æœ‰æ•ˆçš„é¢„æµ‹å’Œå‚è€ƒ
    valid_pairs = [(p.strip(), r.strip()) for p, r in zip(predictions, references) 
                   if p.strip() and r.strip()]
    
    if not valid_pairs:
        logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹-å‚è€ƒå¯¹")
        return {"error": "no_valid_pairs"}
    
    valid_predictions, valid_references = zip(*valid_pairs)
    results = {}
    
    # BLEUåˆ†æ•°
    if "bleu" in metrics:
        results["bleu"] = calculate_bleu_score(valid_predictions, valid_references)
    
    # ROUGEåˆ†æ•°
    if "rouge" in metrics:
        rouge_scores = calculate_rouge_scores(valid_predictions, valid_references)
        results.update(rouge_scores)
    
    # ç²¾ç¡®åŒ¹é…
    if "exact_match" in metrics:
        exact_matches = sum(1 for p, r in zip(valid_predictions, valid_references) 
                           if p.lower().strip() == r.lower().strip())
        results["exact_match"] = exact_matches / len(valid_predictions)
    
    # ç»Ÿè®¡ä¿¡æ¯
    results["num_valid_pairs"] = len(valid_pairs)
    results["avg_prediction_length"] = sum(len(p.split()) for p in valid_predictions) / len(valid_predictions)
    results["avg_reference_length"] = sum(len(r.split()) for r in valid_references) / len(valid_references)
    
    return results

def print_results(results: Dict[str, Any], output_dir: str):
    """æ‰“å°è¯„ä¼°ç»“æœ"""
    print("\n" + "="*60)
    print("ğŸ¯ æ¨¡å‹è¯„ä¼°ç»“æœ")
    print("="*60)
    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {results['model_path']}")
    print(f"ğŸ“„ æ•°æ®è·¯å¾„: {results['data_path']}")
    print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {results['num_samples']}")
    
    metrics = results['metrics']
    if "num_valid_pairs" in metrics:
        print(f"âœ… æœ‰æ•ˆæ ·æœ¬æ•°: {metrics['num_valid_pairs']}")
    print(f"â° æ—¶é—´æˆ³: {results['timestamp']}")
    
    print(f"\nğŸ“ˆ è¯„ä¼°æŒ‡æ ‡:")
    main_metrics = ["bleu", "rouge-1", "rouge-2", "rouge-l", "exact_match"]
    for metric in main_metrics:
        if metric in metrics:
            print(f"  {metric.upper()}: {metrics[metric]:.4f}")
    
    print(f"\nğŸ“‹ ç»Ÿè®¡ä¿¡æ¯:")
    if "avg_prediction_length" in metrics:
        print(f"  å¹³å‡é¢„æµ‹é•¿åº¦: {metrics['avg_prediction_length']:.1f} tokens")
    if "avg_reference_length" in metrics:
        print(f"  å¹³å‡å‚è€ƒé•¿åº¦: {metrics['avg_reference_length']:.1f} tokens")
    
    print(f"\nğŸ’¾ ç»“æœæ–‡ä»¶: {os.path.join(output_dir, 'evaluation_results.json')}")
    print("="*60)

def main():
    parser = argparse.ArgumentParser(
        description="Qwen æ¨¡å‹è¯„ä¼°è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python eval_model.py -m ./output/checkpoint-best -d ./data/test.jsonl
  python eval_model.py -m model_path -d data_path --metrics bleu,rouge,exact_match --save-predictions
        """
    )
    
    # å¿…éœ€å‚æ•°
    parser.add_argument("-m", "--model-path", required=True,
                       help="å¾®è°ƒåçš„æ¨¡å‹è·¯å¾„")
    parser.add_argument("-d", "--data-path", required=True,
                       help="è¯„ä¼°æ•°æ®æ–‡ä»¶è·¯å¾„ (JSONLæ ¼å¼)")
    
    # å¯é€‰å‚æ•°
    parser.add_argument("-o", "--output-dir", default="./eval_results",
                       help="è¯„ä¼°ç»“æœè¾“å‡ºç›®å½• (é»˜è®¤: ./eval_results)")
    parser.add_argument("-b", "--batch-size", type=int, default=4,
                       help="æ‰¹å¤„ç†å¤§å° (é»˜è®¤: 4)")
    parser.add_argument("--max-tokens", type=int, default=512,
                       help="æœ€å¤§ç”Ÿæˆtokenæ•° (é»˜è®¤: 512)")
    parser.add_argument("--temperature", type=float, default=0.7,
                       help="ç”Ÿæˆæ¸©åº¦ (é»˜è®¤: 0.7)")
    parser.add_argument("--top-p", type=float, default=0.9,
                       help="Top-pé‡‡æ · (é»˜è®¤: 0.9)")
    parser.add_argument("--device", default="auto",
                       help="è®¡ç®—è®¾å¤‡: cpu/cuda/auto (é»˜è®¤: auto)")
    parser.add_argument("--torch-dtype", default="float16",
                       choices=["float16", "float32", "bfloat16"],
                       help="æ¨¡å‹æ•°æ®ç±»å‹ (é»˜è®¤: float16)")
    parser.add_argument("--metrics", default="bleu,rouge,exact_match",
                       help="è¯„ä¼°æŒ‡æ ‡ (é»˜è®¤: bleu,rouge,exact_match)")
    parser.add_argument("--save-predictions", action="store_true",
                       help="ä¿å­˜è¯¦ç»†çš„é¢„æµ‹ç»“æœ")
    parser.add_argument("--verbose", action="store_true",
                       help="è¯¦ç»†è¾“å‡º")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    try:
        # åŠ è½½æ¨¡å‹å’Œæ•°æ®
        model, tokenizer = load_model_and_tokenizer(
            args.model_path, args.device, args.torch_dtype
        )
        eval_data = load_eval_data(args.data_path)
        
        # æ‰§è¡Œè¯„ä¼°
        eval_results = evaluate_generation(
            model, tokenizer, eval_data,
            batch_size=args.batch_size,
            max_tokens=args.max_tokens,
            temperature=args.temperature,
            top_p=args.top_p,
            save_predictions=args.save_predictions,
            output_dir=args.output_dir
        )
        
        # è®¡ç®—æŒ‡æ ‡
        metrics_list = [m.strip() for m in args.metrics.split(',')]
        metrics = calculate_metrics(
            eval_results["predictions"],
            eval_results["references"],
            metrics_list
        )
        
        # ç»„ç»‡ç»“æœ
        results = {
            "model_path": args.model_path,
            "data_path": args.data_path,
            "num_samples": eval_results["num_samples"],
            "metrics": metrics,
            "config": {
                "batch_size": args.batch_size,
                "max_tokens": args.max_tokens,
                "temperature": args.temperature,
                "top_p": args.top_p,
                "device": args.device,
                "torch_dtype": args.torch_dtype,
                "metrics": args.metrics
            },
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        # ä¿å­˜ç»“æœ
        results_file = os.path.join(args.output_dir, "evaluation_results.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°ç»“æœ
        print_results(results, args.output_dir)
        
        if args.save_predictions:
            predictions_file = os.path.join(args.output_dir, "predictions.jsonl")
            print(f"ğŸ“ é¢„æµ‹è¯¦æƒ…: {predictions_file}")
        
        logger.info("ğŸ‰ è¯„ä¼°å®Œæˆï¼")
        
    except KeyboardInterrupt:
        logger.info("âš ï¸ ç”¨æˆ·ä¸­æ–­è¯„ä¼°")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main() 