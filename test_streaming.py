#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµå¼ç”Ÿæˆæµ‹è¯•è„šæœ¬
éªŒè¯æµå¼ç”ŸæˆåŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import sys
import time

def test_streaming_basic():
    """æµ‹è¯•åŸºæœ¬çš„æµå¼ç”ŸæˆåŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•åŸºæœ¬æµå¼ç”Ÿæˆ...")
    
    try:
        # æ¨¡æ‹Ÿæµå¼ç”Ÿæˆ
        message = "ä½ å¥½ï¼Œä½ æ˜¯è°ï¼Ÿ"
        words = ["æˆ‘", "æ˜¯", "ä¸€ä¸ª", "AI", "åŠ©æ‰‹", "ï¼Œ", "å¾ˆé«˜å…´", "ä¸ºæ‚¨", "æœåŠ¡", "ï¼"]
        
        print(f"è¾“å…¥: {message}")
        print("æµå¼è¾“å‡º: ", end="", flush=True)
        
        current_response = ""
        for word in words:
            current_response += word
            print(word, end="", flush=True)
            time.sleep(0.1)  # æ¨¡æ‹Ÿç”Ÿæˆå»¶è¿Ÿ
        
        print()
        print(f"æœ€ç»ˆå›å¤: {current_response}")
        print("âœ… åŸºæœ¬æµå¼ç”Ÿæˆæµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ åŸºæœ¬æµå¼ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_gradio_streaming():
    """æµ‹è¯•Gradioæµå¼ç”Ÿæˆç»„ä»¶"""
    print("\nğŸ§ª æµ‹è¯•Gradioæµå¼ç”Ÿæˆç»„ä»¶...")
    
    try:
        from gradio_inference import ModelInference
        
        # åˆ›å»ºæ¨¡å‹æ¨ç†å®ä¾‹
        inference = ModelInference()
        
        # æµ‹è¯•æœªåŠ è½½æ¨¡å‹çš„æƒ…å†µ
        print("æµ‹è¯•æœªåŠ è½½æ¨¡å‹çš„æµå¼ç”Ÿæˆ...")
        stream_gen = inference.generate_response("æµ‹è¯•", stream=True)
        response = next(stream_gen)
        
        if "è¯·å…ˆåŠ è½½æ¨¡å‹" in response:
            print("âœ… æœªåŠ è½½æ¨¡å‹æ—¶çš„é”™è¯¯å¤„ç†æ­£ç¡®")
        else:
            print(f"âš ï¸  é¢„æœŸé”™è¯¯æ¶ˆæ¯ï¼Œä½†å¾—åˆ°: {response}")
        
        print("âœ… Gradioæµå¼ç”Ÿæˆç»„ä»¶æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ Gradioæµå¼ç”Ÿæˆç»„ä»¶æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_response_cleaning():
    """æµ‹è¯•å›å¤æ¸…ç†åŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•å›å¤æ¸…ç†åŠŸèƒ½...")
    
    try:
        from gradio_inference import ModelInference
        
        inference = ModelInference()
        
        # æµ‹è¯•æ•°æ®
        test_cases = [
            ("<|im_start|>assistant\næˆ‘æ˜¯AIåŠ©æ‰‹<|im_end|>", "æˆ‘æ˜¯AIåŠ©æ‰‹"),
            ("user\nä½ å¥½\nassistant\nä½ å¥½ï¼", "ä½ å¥½ï¼"),
            ("æˆ‘æ˜¯æ¥è‡ªé˜¿é‡Œäº‘çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼Œæˆ‘å«é€šä¹‰åƒé—®ã€‚", "æˆ‘æ˜¯æ¥è‡ªé˜¿é‡Œäº‘çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼Œæˆ‘å«é€šä¹‰åƒé—®ã€‚"),
        ]
        
        for input_text, expected in test_cases:
            cleaned = inference._extract_and_clean_streaming_response(
                input_text, "<|im_start|>user\nä½ å¥½<|im_end|>\n<|im_start|>assistant\n", "ä½ å¥½"
            )
            print(f"è¾“å…¥: {repr(input_text)}")
            print(f"é¢„æœŸ: {repr(expected)}")
            print(f"å®é™…: {repr(cleaned)}")
            print()
        
        print("âœ… å›å¤æ¸…ç†åŠŸèƒ½æµ‹è¯•å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âŒ å›å¤æ¸…ç†åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ æµå¼ç”ŸæˆåŠŸèƒ½æµ‹è¯•å¼€å§‹")
    print("=" * 50)
    
    success_count = 0
    total_tests = 3
    
    # è¿è¡Œå„é¡¹æµ‹è¯•
    if test_streaming_basic():
        success_count += 1
    
    if test_gradio_streaming():
        success_count += 1
    
    if test_response_cleaning():
        success_count += 1
    
    print("\n" + "=" * 50)
    print(f"æµ‹è¯•ç»“æœ: {success_count}/{total_tests} é€šè¿‡")
    
    if success_count == total_tests:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æµå¼ç”ŸæˆåŠŸèƒ½æ­£å¸¸")
        print("ğŸ’¡ ç°åœ¨å¯ä»¥è¿è¡Œ: ./run_gradio.sh")
        return 0
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é—®é¢˜")
        print("å»ºè®®:")
        print("1. ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–: pip install -r requirements.txt")
        print("2. æ£€æŸ¥Gradioç‰ˆæœ¬å…¼å®¹æ€§")
        print("3. å°è¯•å…³é—­æµå¼ç”Ÿæˆä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼")
        return 1

if __name__ == "__main__":
    sys.exit(main()) 