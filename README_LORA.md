# LoRA/QLoRA å¾®è°ƒä½¿ç”¨æŒ‡å—

æœ¬é¡¹ç›®ç°å·²æ”¯æŒ LoRA (Low-Rank Adaptation) å’Œ QLoRA (Quantized LoRA) å¾®è°ƒæ–¹æ³•ï¼Œå¯ä»¥æ˜¾è‘—å‡å°‘æ˜¾å­˜å ç”¨å’Œè®­ç»ƒå‚æ•°é‡ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements_stable.txt
```

ç¡®ä¿å®‰è£…äº†ä»¥ä¸‹å…³é”®ä¾èµ–ï¼š
- `peft>=0.6.2` - LoRAå®ç°
- `bitsandbytes>=0.41.0` - é‡åŒ–æ”¯æŒ
- `transformers>=4.37.0` - æ¨¡å‹åº“

### 2. é€‰æ‹©å¾®è°ƒæ–¹å¼

#### æ–¹å¼ä¸€ï¼šå…¨å‚æ•°å¾®è°ƒï¼ˆåŸå§‹æ–¹å¼ï¼‰
- **ç‰¹ç‚¹**ï¼šå¾®è°ƒæ‰€æœ‰æ¨¡å‹å‚æ•°
- **æ˜¾å­˜éœ€æ±‚**ï¼šæœ€é«˜
- **è®­ç»ƒæ•ˆæœ**ï¼šé€šå¸¸æœ€å¥½
- **ä½¿ç”¨åœºæ™¯**ï¼šæœ‰å……è¶³GPUèµ„æºæ—¶

#### æ–¹å¼äºŒï¼šLoRA å¾®è°ƒ
- **ç‰¹ç‚¹**ï¼šåªè®­ç»ƒä½ç§©é€‚é…å™¨å‚æ•°ï¼ˆ~1-5%çš„åŸå§‹å‚æ•°ï¼‰
- **æ˜¾å­˜éœ€æ±‚**ï¼šä¸­ç­‰
- **è®­ç»ƒæ•ˆæœ**ï¼šæ¥è¿‘å…¨å‚æ•°å¾®è°ƒ
- **ä½¿ç”¨åœºæ™¯**ï¼šå¹³è¡¡æ€§èƒ½å’Œèµ„æºçš„é¦–é€‰

#### æ–¹å¼ä¸‰ï¼šQLoRA å¾®è°ƒ
- **ç‰¹ç‚¹**ï¼š4/8ä½é‡åŒ– + LoRA
- **æ˜¾å­˜éœ€æ±‚**ï¼šæœ€ä½ï¼ˆå¯å‡å°‘50-75%æ˜¾å­˜ï¼‰
- **è®­ç»ƒæ•ˆæœ**ï¼šç•¥ä½äºLoRAä½†ä»ç„¶å¾ˆå¥½
- **ä½¿ç”¨åœºæ™¯**ï¼šæ˜¾å­˜æœ‰é™çš„ç¯å¢ƒ

## ğŸ“‹ é…ç½®æ–‡ä»¶ç¤ºä¾‹

### LoRA å¾®è°ƒé…ç½®
```bash
python fine_tune_qwen.py --config_file configs/train_config_lora.json
```

å…³é”®å‚æ•°ï¼š
- `use_lora: true` - å¯ç”¨LoRA
- `lora_r: 64` - LoRA rankï¼Œè¶Šå¤§æ•ˆæœè¶Šå¥½ä½†å‚æ•°è¶Šå¤š
- `lora_alpha: 16` - LoRA scalingå‚æ•°
- `lora_dropout: 0.1` - LoRA dropoutç‡
- `learning_rate: 1e-4` - LoRAå»ºè®®ä½¿ç”¨è¾ƒé«˜å­¦ä¹ ç‡

### QLoRA å¾®è°ƒé…ç½®

#### 4ä½é‡åŒ– (æ¨è)
```bash
python fine_tune_qwen.py --config_file configs/train_config_qlora.json
```

#### 8ä½é‡åŒ–
```bash 
python fine_tune_qwen.py --config_file configs/train_config_qlora_8bit.json
```

å…³é”®å‚æ•°ï¼š
- `use_qlora: true` - å¯ç”¨QLoRA
- `quantization_bit: 4` - é‡åŒ–ä½æ•° (4 æˆ– 8)
- `learning_rate: 2e-4` - QLoRAå»ºè®®ä½¿ç”¨æ›´é«˜å­¦ä¹ ç‡

## ğŸ¯ ç›®æ ‡æ¨¡å—é€‰æ‹©

LoRAå¯ä»¥é€‰æ‹©æ€§åœ°åº”ç”¨åˆ°ç‰¹å®šæ¨¡å—ï¼š

### æ¨èé…ç½®
```json
"lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj"
```

### è½»é‡é…ç½®ï¼ˆæ˜¾å­˜æ›´å°‘ï¼‰
```json
"lora_target_modules": "q_proj,k_proj,v_proj,o_proj"
```

### è‡ªåŠ¨å‘ç°ï¼ˆæ¨èï¼‰
```json
"lora_target_modules": null
```
- ç³»ç»Ÿä¼šè‡ªåŠ¨å‘ç°æ‰€æœ‰çº¿æ€§å±‚

## ğŸ“Š æ˜¾å­˜å¯¹æ¯”

ä»¥ Qwen2.5-0.5B ä¸ºä¾‹ï¼š

| æ–¹æ³• | æ˜¾å­˜å ç”¨ | å¯è®­ç»ƒå‚æ•° | ç›¸å¯¹æ€§èƒ½ |
|------|----------|------------|----------|
| å…¨å‚æ•° | ~8GB | 100% | 100% |
| LoRA (r=64) | ~6GB | ~3% | 95-98% |
| QLoRA 4bit (r=64) | ~3GB | ~3% | 90-95% |
| QLoRA 8bit (r=64) | ~4GB | ~3% | 92-96% |

## ğŸ”§ è®­ç»ƒå‘½ä»¤

### å‘½ä»¤è¡Œå‚æ•°æ–¹å¼
```bash
python fine_tune_qwen.py \
    --use_lora \
    --lora_r 64 \
    --lora_alpha 16 \
    --data_path data/store_xhs_sft_samples.jsonl \
    --output_dir ./output_lora \
    --learning_rate 1e-4
```

### é…ç½®æ–‡ä»¶æ–¹å¼ï¼ˆæ¨èï¼‰
```bash
# å…¨å‚æ•°å¾®è°ƒ
python fine_tune_qwen.py --config_file configs/train_config_full.json

# LoRAå¾®è°ƒ
python fine_tune_qwen.py --config_file configs/train_config_lora.json

# QLoRAå¾®è°ƒ  
python fine_tune_qwen.py --config_file configs/train_config_qlora.json
```

## ğŸ“¤ æ¨¡å‹ä¿å­˜ä¸åŠ è½½

### ä¿å­˜
è®­ç»ƒå®Œæˆåï¼ŒLoRAé€‚é…å™¨ä¼šä¿å­˜åˆ°è¾“å‡ºç›®å½•ï¼š
```
output_qwen_lora/
â”œâ”€â”€ adapter_config.json
â”œâ”€â”€ adapter_model.safetensors
â””â”€â”€ tokenizer files...
```

### åŠ è½½æ¨ç†
```bash
# åŸºç¡€æ¨ç†
python inference_lora.py \
    --base_model ./models/Qwen2.5-0.5B-Instruct \
    --lora_model ./output_qwen_lora

# é‡åŒ–æ¨ç†
python inference_lora.py \
    --base_model ./models/Qwen2.5-0.5B-Instruct \
    --lora_model ./output_qwen_qlora \
    --load_in_4bit

# äº¤äº’å¼å¯¹è¯
python inference_lora.py \
    --base_model ./models/Qwen2.5-0.5B-Instruct \
    --lora_model ./output_qwen_lora \
    --chat
```

## ğŸ’¡ è°ƒä¼˜å»ºè®®

### LoRA å‚æ•°è°ƒä¼˜
- **rank (r)**ï¼š16-128ï¼Œè¶Šå¤§æ•ˆæœè¶Šå¥½ä½†å‚æ•°è¶Šå¤š
- **alpha**ï¼šé€šå¸¸è®¾ä¸º r/2 åˆ° 2*r
- **dropout**ï¼š0.05-0.1
- **å­¦ä¹ ç‡**ï¼š1e-4 åˆ° 5e-4ï¼Œæ¯”å…¨å‚æ•°å¾®è°ƒé«˜

### QLoRA ç‰¹æ®Šè€ƒè™‘
- 4ä½é‡åŒ–æ¨èç”¨äºæ˜¾å­˜æé™æƒ…å†µ
- 8ä½é‡åŒ–åœ¨ç²¾åº¦å’Œæ˜¾å­˜é—´å¹³è¡¡
- å­¦ä¹ ç‡å¯ä»¥è®¾ç½®æ›´é«˜ (2e-4 åˆ° 1e-3)

### ç›®æ ‡æ¨¡å—é€‰æ‹©
- **å…¨è¦†ç›–**ï¼š`q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj`
- **æ³¨æ„åŠ›å±‚**ï¼š`q_proj,k_proj,v_proj,o_proj`
- **å‰é¦ˆå±‚**ï¼š`gate_proj,up_proj,down_proj`

## â“ å¸¸è§é—®é¢˜

### Q: LoRAå’ŒQLoRAé€‰æ‹©å“ªä¸ªï¼Ÿ
A: 
- æ˜¾å­˜å……è¶³ â†’ LoRA
- æ˜¾å­˜ç´§å¼  â†’ QLoRA
- è¿½æ±‚æœ€ä½³æ•ˆæœ â†’ LoRA
- æ˜¾å­˜æé™ â†’ QLoRA 4bit

### Q: å¦‚ä½•ç¡®å®šæœ€ä½³rankå€¼ï¼Ÿ
A: 
- å°æ¨¡å‹(1Bä»¥ä¸‹)ï¼š16-64
- ä¸­æ¨¡å‹(1-7B)ï¼š64-128  
- å¤§æ¨¡å‹(7B+)ï¼š128-256

### Q: è®­ç»ƒåå¦‚ä½•åˆå¹¶æ¨¡å‹ï¼Ÿ
A: 
```python
from peft import PeftModel
model = PeftModel.from_pretrained(base_model, lora_path)
merged_model = model.merge_and_unload()
merged_model.save_pretrained("./merged_model")
```

### Q: QLoRAè®­ç»ƒå¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ
A: 
- ä½¿ç”¨æ›´å¤§çš„batch size
- å‡å°‘gradient accumulation steps
- ç¡®ä¿ä½¿ç”¨äº†åˆé€‚çš„CUDAç‰ˆæœ¬

## ğŸ“š æ›´å¤šèµ„æº

- [LoRAè®ºæ–‡](https://arxiv.org/abs/2106.09685)
- [QLoRAè®ºæ–‡](https://arxiv.org/abs/2305.14314)  
- [PEFTåº“æ–‡æ¡£](https://huggingface.co/docs/peft)
- [bitsandbytesæ–‡æ¡£](https://github.com/TimDettmers/bitsandbytes)

---

**æç¤º**ï¼šé¦–æ¬¡ä½¿ç”¨å»ºè®®ä»LoRAå¼€å§‹ï¼Œç†Ÿæ‚‰æµç¨‹åå†å°è¯•QLoRAã€‚ 