#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨ transformers å¾®è°ƒ Qwen2.5-0.5B æ¨¡å‹
ç”¨äºå°çº¢ä¹¦é£æ ¼åº—é“ºæ–‡æ¡ˆç”Ÿæˆä»»åŠ¡
"""

import os
import json
import torch
from dataclasses import dataclass, field
from typing import Optional, Dict, Any, List
import logging

# è®¾ç½®å›½å†…ç¯å¢ƒçš„é•œåƒæºå’Œè­¦å‘ŠæŠ‘åˆ¶
os.environ.setdefault('HF_ENDPOINT', 'https://hf-mirror.com')
os.environ.setdefault('TOKENIZERS_PARALLELISM', 'false')
os.environ.setdefault('TRANSFORMERS_NO_ADVISORY_WARNINGS', 'true')

# ç¦ç”¨TensorFlowç›¸å…³è­¦å‘Š
os.environ.setdefault('TF_ENABLE_ONEDNN_OPTS', '0')
os.environ.setdefault('TF_CPP_MIN_LOG_LEVEL', '2')

# DeepSpeedç›¸å…³ç¯å¢ƒå˜é‡å°†åœ¨mainå‡½æ•°ä¸­æ ¹æ®é…ç½®åŠ¨æ€è®¾ç½®
os.environ.setdefault('DEEPSPEED_LOG_LEVEL', 'WARNING')

# ç¦ç”¨Pythonè­¦å‘Š
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥å¿…è¦çš„åº“
try:
    import transformers
    from transformers import (
        AutoTokenizer,
        AutoModelForCausalLM,
        TrainingArguments,
        Trainer,
        DataCollatorForSeq2Seq,
        set_seed,
        BitsAndBytesConfig,
        AutoConfig
    )
    from torch.utils.data import Dataset
    from transformers.trainer_utils import IntervalStrategy, SaveStrategy
    
    from peft import (
        LoraConfig,
        TaskType,
        get_peft_model,
        prepare_model_for_kbit_training
    )
    import bitsandbytes as bnb
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·å®‰è£…ä¾èµ–: pip install torch transformers peft bitsandbytes")
    raise

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# æŠ‘åˆ¶DeepSpeedå’Œå…¶ä»–ç¬¬ä¸‰æ–¹åº“çš„å†—ä½™æ—¥å¿—
logging.getLogger('deepspeed').setLevel(logging.WARNING)
logging.getLogger('transformers.modeling_utils').setLevel(logging.WARNING)
logging.getLogger('transformers.configuration_utils').setLevel(logging.WARNING)
logging.getLogger('transformers.tokenization_utils_base').setLevel(logging.WARNING)
logging.getLogger('accelerate.utils.other').setLevel(logging.ERROR)  # æŠ‘åˆ¶å†…æ ¸ç‰ˆæœ¬è­¦å‘Š

@dataclass
class ModelArguments:
    """æ¨¡å‹ç›¸å…³å‚æ•°"""
    model_name_or_path: Optional[str] = field(
        default="Qwen/Qwen2.5-0.5B-Instruct",
        metadata={"help": "æ¨¡å‹åç§°æˆ–è·¯å¾„"}
    )
    cache_dir: Optional[str] = field(
        default=None,
        metadata={"help": "æ¨¡å‹ç¼“å­˜ç›®å½•"}
    )
    use_lora: bool = field(
        default=False,
        metadata={"help": "æ˜¯å¦ä½¿ç”¨LoRAå¾®è°ƒ"}
    )
    use_qlora: bool = field(
        default=False,
        metadata={"help": "æ˜¯å¦ä½¿ç”¨QLoRAå¾®è°ƒï¼ˆéœ€è¦é‡åŒ–ï¼‰"}
    )
    lora_r: int = field(
        default=64,
        metadata={"help": "LoRAçš„rank"}
    )
    lora_alpha: int = field(
        default=16,
        metadata={"help": "LoRAçš„alphaå‚æ•°"}
    )
    lora_dropout: float = field(
        default=0.1,
        metadata={"help": "LoRAçš„dropoutç‡"}
    )
    lora_target_modules: Optional[str] = field(
        default="q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj",
        metadata={"help": "LoRAç›®æ ‡æ¨¡å—ï¼Œé€—å·åˆ†éš”"}
    )
    quantization_bit: int = field(
        default=4,
        metadata={"help": "é‡åŒ–ä½æ•°ï¼Œæ”¯æŒ4æˆ–8ä½"}
    )
    use_deepspeed: bool = field(
        default=False,
        metadata={"help": "æ˜¯å¦ä½¿ç”¨DeepSpeedè¿›è¡Œè®­ç»ƒ"}
    )
    deepspeed_stage: int = field(
        default=2,
        metadata={"help": "DeepSpeed ZeROé˜¶æ®µï¼Œæ”¯æŒ1,2,3"}
    )
    cpu_offload: bool = field(
        default=False,
        metadata={"help": "æ˜¯å¦å¯ç”¨CPU offloadä¼˜åŒ–å™¨çŠ¶æ€"}
    )

@dataclass
class DataArguments:
    """æ•°æ®ç›¸å…³å‚æ•°"""
    data_path: str = field(
        default="data/zhc_store_recommend_doubao.jsonl",
        metadata={"help": "è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒå•ä¸ªæ–‡ä»¶æˆ–å¤šä¸ªæ–‡ä»¶ï¼ˆé€—å·åˆ†éš”ï¼‰"}
    )
    max_seq_length: int = field(
        default=2048,
        metadata={"help": "æœ€å¤§åºåˆ—é•¿åº¦"}
    )
    eval_data_path: Optional[str] = field(
        default=None,
        metadata={"help": "éªŒè¯æ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œå¯é€‰"}
    )

@dataclass
class CustomTrainingArguments(TrainingArguments):
    """è®­ç»ƒç›¸å…³å‚æ•°"""
    output_dir: str = field(default="./output_qwen")
    overwrite_output_dir: bool = field(default=True)
    per_device_train_batch_size: int = field(default=2)
    per_device_eval_batch_size: int = field(default=2)
    gradient_accumulation_steps: int = field(default=8)
    learning_rate: float = field(default=2e-5)
    num_train_epochs: int = field(default=3)
    lr_scheduler_type: str = field(default="cosine")
    warmup_steps: int = field(default=100)
    logging_steps: int = field(default=10)
    save_steps: int = field(default=500)
    eval_steps: int = field(default=500)
    evaluation_strategy: IntervalStrategy = field(default=IntervalStrategy.STEPS)
    save_strategy: SaveStrategy = field(default=SaveStrategy.STEPS)
    save_total_limit: int = field(default=3)
    load_best_model_at_end: bool = field(default=True)
    metric_for_best_model: str = field(default="eval_loss")
    greater_is_better: bool = field(default=False)
    dataloader_num_workers: int = field(default=4)
    remove_unused_columns: bool = field(default=False)
    fp16: bool = field(default=True)
    label_names: Optional[List[str]] = field(default_factory=lambda: ["labels"])
    deepspeed: Optional[str] = field(default=None)
    report_to: str = field(default="tensorboard")

class SFTDataset(Dataset):
    """SFTæ•°æ®é›†ç±»"""
    
    def __init__(self, data_path: str, tokenizer, max_seq_length: int = 2048):
        super().__init__()
        self.tokenizer = tokenizer
        self.max_seq_length = max_seq_length
        self.data = self.load_data(data_path)
        
    def load_data(self, data_path: str) -> List[Dict]:
        """åŠ è½½JSONLæ ¼å¼çš„æ•°æ®"""
        data = []
        
        # æ”¯æŒé€—å·åˆ†éš”çš„å¤šä¸ªæ–‡ä»¶
        if ',' in data_path:
            file_paths = [path.strip() for path in data_path.split(',') if path.strip()]
        else:
            file_paths = [data_path.strip()]
        
        logger.info(f"å‡†å¤‡åŠ è½½ {len(file_paths)} ä¸ªæ•°æ®æ–‡ä»¶")
        
        for file_path in file_paths:
            if not os.path.exists(file_path):
                logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {file_path}")
                continue
                
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if line:
                        try:
                            item = json.loads(line)
                            if all(key in item for key in ['instruction', 'input', 'output']):
                                data.append(item)
                            else:
                                logger.warning(f"{file_path} ç¬¬{line_num}è¡Œç¼ºå°‘å¿…è¦å­—æ®µ")
                        except json.JSONDecodeError as e:
                            logger.warning(f"{file_path} ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
        
        if not data:
            raise ValueError("æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®")
            
        logger.info(f"æˆåŠŸåŠ è½½ {len(data)} æ¡è®­ç»ƒæ•°æ®")
        return data
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # æ„å»ºå¯¹è¯æ ¼å¼
        instruction = item['instruction']
        input_text = item['input']
        output_text = item['output']
        
        # ç»„åˆè¾“å…¥æ–‡æœ¬
        if input_text.strip():
            prompt = f"{instruction}\n\n{input_text}"
        else:
            prompt = instruction
            
        # æ„å»ºå®Œæ•´å¯¹è¯
        conversation = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n{output_text}<|im_end|>"
        
        # ç¼–ç 
        model_inputs = self.tokenizer(
            conversation,
            truncation=True,
            max_length=self.max_seq_length,
            padding=False,
            return_tensors=None
        )
        
        # åˆ›å»ºæ ‡ç­¾ï¼Œåªè®¡ç®—assistantéƒ¨åˆ†çš„æŸå¤±
        labels = model_inputs["input_ids"].copy()
        
        # æ‰¾åˆ°assistantå¼€å§‹çš„ä½ç½®
        assistant_start = conversation.find("<|im_start|>assistant\n") + len("<|im_start|>assistant\n")
        user_part = conversation[:assistant_start]
        user_tokens = self.tokenizer(user_part, add_special_tokens=False)["input_ids"]
        
        # å°†ç”¨æˆ·éƒ¨åˆ†çš„æ ‡ç­¾è®¾ä¸º-100ï¼ˆä¸è®¡ç®—æŸå¤±ï¼‰
        for i in range(min(len(user_tokens), len(labels))):
            labels[i] = -100
            
        model_inputs["labels"] = labels
        return model_inputs

def get_model_path(model_name: str, cache_dir: str = "./models") -> str:
    """è·å–æ¨¡å‹è·¯å¾„ï¼Œä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹"""
    logger.info(f"å‡†å¤‡åŠ è½½æ¨¡å‹: {model_name}")
    
    # æ£€æŸ¥æœ¬åœ°æ¨¡å‹
    local_paths = [
        f"{cache_dir}/{model_name}",
        f"{cache_dir}/{model_name.split('/')[-1]}",
        f"models/{model_name.split('/')[-1]}",
        model_name
    ]
    
    for local_path in local_paths:
        if os.path.exists(local_path) and os.path.exists(os.path.join(local_path, "config.json")):
            logger.info(f"ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {local_path}")
            return os.path.abspath(local_path)
    
    logger.info(f"ä»HuggingFaceä¸‹è½½æ¨¡å‹: {model_name}")
    return model_name

def create_quantization_config(model_args: ModelArguments) -> Optional[BitsAndBytesConfig]:
    """åˆ›å»ºé‡åŒ–é…ç½®"""
    if not model_args.use_qlora:
        return None
    
    if model_args.quantization_bit == 4:
        return BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
        )
    elif model_args.quantization_bit == 8:
        return BitsAndBytesConfig(load_in_8bit=True)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„é‡åŒ–ä½æ•°: {model_args.quantization_bit}")

def create_lora_config(model_args: ModelArguments) -> Optional[LoraConfig]:
    """åˆ›å»ºLoRAé…ç½®"""
    if not (model_args.use_lora or model_args.use_qlora):
        return None
    
    target_modules = model_args.lora_target_modules.split(',') if model_args.lora_target_modules else None
    
    return LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=model_args.lora_r,
        lora_alpha=model_args.lora_alpha,
        lora_dropout=model_args.lora_dropout,
        target_modules=target_modules,
        bias="none",
    )

def find_all_linear_names(model):
    """æ‰¾åˆ°æ¨¡å‹ä¸­æ‰€æœ‰çš„çº¿æ€§å±‚åç§°"""
    cls = bnb.nn.Linear4bit if hasattr(bnb.nn, 'Linear4bit') else torch.nn.Linear
    lora_module_names = set()
    for name, module in model.named_modules():
        if isinstance(module, cls):
            names = name.split('.')
            lora_module_names.add(names[0] if len(names) == 1 else names[-1])
    
    if 'lm_head' in lora_module_names:
        lora_module_names.remove('lm_head')
    return list(lora_module_names)

def create_deepspeed_config(model_args: ModelArguments, training_args: CustomTrainingArguments) -> Dict[str, Any]:
    """åˆ›å»ºDeepSpeedé…ç½®"""
    if not model_args.use_deepspeed:
        return None
    
    # åŸºç¡€é…ç½®
    config = {
        "bf16": {
            "enabled": not training_args.fp16
        },
        "fp16": {
            "enabled": training_args.fp16,
            "loss_scale": 0,
            "loss_scale_window": 1000,
            "hysteresis": 2,
            "min_loss_scale": 1
        },
        "optimizer": {
            "type": "AdamW",
            "params": {
                "lr": training_args.learning_rate,
                "betas": [0.9, 0.999],
                "eps": 1e-8,
                "weight_decay": training_args.weight_decay if hasattr(training_args, 'weight_decay') else 0.0
            }
        },
        "scheduler": {
            "type": "WarmupLR",
            "params": {
                "warmup_min_lr": 0,
                "warmup_max_lr": training_args.learning_rate,
                "warmup_num_steps": training_args.warmup_steps
            }
        },
        "zero_optimization": {
            "stage": model_args.deepspeed_stage,
            "offload_optimizer": {
                "device": "cpu" if (model_args.deepspeed_stage == 3 or model_args.cpu_offload) else "none"
            },
            "offload_param": {
                "device": "cpu" if model_args.deepspeed_stage == 3 else "none"
            },
            "overlap_comm": True,
            "contiguous_gradients": True,
            "sub_group_size": 1e9,
            "reduce_bucket_size": "auto",
            "stage3_prefetch_bucket_size": "auto",
            "stage3_param_persistence_threshold": "auto",
            "stage3_max_live_parameters": 1e9,
            "stage3_max_reuse_distance": 1e9,
            "gather_16bit_weights_on_model_save": True
        },
        "gradient_accumulation_steps": training_args.gradient_accumulation_steps,
        "gradient_clipping": 1.0,
        "steps_per_print": training_args.logging_steps,
        "train_batch_size": training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps * torch.cuda.device_count(),
        "train_micro_batch_size_per_gpu": training_args.per_device_train_batch_size,
        "wall_clock_breakdown": False
    }
    
    # å¦‚æœä½¿ç”¨LoRAï¼Œè°ƒæ•´é…ç½®
    if model_args.use_lora or model_args.use_qlora:
        config["zero_optimization"]["stage"] = min(model_args.deepspeed_stage, 2)  # LoRAä¸æ”¯æŒstage 3
        
    logger.info(f"DeepSpeedé…ç½®: ZeRO Stage {config['zero_optimization']['stage']}")
    return config

def main():
    # è§£æå‚æ•°
    import sys
    parser = transformers.HfArgumentParser((ModelArguments, DataArguments, CustomTrainingArguments))
    
    if len(sys.argv) >= 3 and sys.argv[1] == '--config_file':
        config_file = sys.argv[2]
        logger.info(f"ä½¿ç”¨é…ç½®æ–‡ä»¶: {config_file}")
        
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # è¿‡æ»¤æ‰æ³¨é‡Šå­—æ®µ
        filtered_config = {k: v for k, v in config.items() if not k.startswith('_')}
        model_args, data_args, training_args = parser.parse_dict(filtered_config)
    else:
        model_args, data_args, training_args = parser.parse_args_into_dataclasses()

    # æ ¹æ®é…ç½®è®¾ç½®DeepSpeedç›¸å…³ç¯å¢ƒå˜é‡
    if model_args.use_deepspeed:
        logger.info("å¯ç”¨DeepSpeedè®­ç»ƒ")
        os.environ.pop('ACCELERATE_USE_DEEPSPEED', None)  # ç§»é™¤å¯èƒ½çš„ç¦ç”¨è®¾ç½®
        os.environ.pop('TRANSFORMERS_NO_DEEPSPEED', None)
    else:
        logger.info("ä½¿ç”¨æ ‡å‡†è®­ç»ƒï¼ˆä¸ä½¿ç”¨DeepSpeedï¼‰")
        os.environ.setdefault('ACCELERATE_USE_DEEPSPEED', 'false')
        os.environ.setdefault('TRANSFORMERS_NO_DEEPSPEED', 'true')

    # éªŒè¯å’Œä¿®å¤è®­ç»ƒå‚æ•°
    if training_args.load_best_model_at_end:
        if data_args.eval_data_path is None:
            logger.warning("æ²¡æœ‰è®¾ç½®éªŒè¯æ•°æ®è·¯å¾„ï¼Œä½†å¯ç”¨äº†load_best_model_at_endï¼Œå°†ç¦ç”¨æ­¤é€‰é¡¹")
            training_args.load_best_model_at_end = False
        else:
            # ç¡®ä¿è¯„ä¼°ç­–ç•¥å’Œä¿å­˜ç­–ç•¥åŒ¹é…
            if training_args.evaluation_strategy == IntervalStrategy.NO:
                logger.info("æ£€æµ‹åˆ°è¯„ä¼°ç­–ç•¥ä¸ºNOï¼Œä½†éœ€è¦è¿›è¡Œè¯„ä¼°ï¼Œè®¾ç½®ä¸ºSTEPS")
                training_args.evaluation_strategy = IntervalStrategy.STEPS
            
            # ç¡®ä¿è¯„ä¼°å’Œä¿å­˜ç­–ç•¥åŒ¹é…
            if training_args.evaluation_strategy != training_args.save_strategy:
                logger.info(f"è°ƒæ•´è¯„ä¼°ç­–ç•¥ä»¥åŒ¹é…ä¿å­˜ç­–ç•¥: {training_args.save_strategy}")
                training_args.evaluation_strategy = training_args.save_strategy
    
    # è®¾ç½®éšæœºç§å­
    set_seed(training_args.seed if hasattr(training_args, 'seed') else 42)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(training_args.output_dir, exist_ok=True)
    
    # è®¾ç½®TensorBoardæ—¥å¿—ç›®å½•
    if training_args.report_to == "tensorboard":
        tensorboard_log_dir = os.path.join(training_args.output_dir, "tensorboard_logs")
        os.makedirs(tensorboard_log_dir, exist_ok=True)
        os.environ['TENSORBOARD_LOG_DIR'] = tensorboard_log_dir
        logger.info(f"TensorBoardæ—¥å¿—å°†ä¿å­˜åˆ°: {tensorboard_log_dir}")
        logger.info("å¯åŠ¨TensorBoardæŸ¥çœ‹è®­ç»ƒè¿›åº¦: tensorboard --logdir=" + tensorboard_log_dir)
    
    # åˆ›å»ºDeepSpeedé…ç½®
    deepspeed_config = create_deepspeed_config(model_args, training_args)
    if deepspeed_config:
        # ä¿å­˜DeepSpeedé…ç½®æ–‡ä»¶
        deepspeed_config_path = os.path.join(training_args.output_dir, "deepspeed_config.json")
        with open(deepspeed_config_path, 'w', encoding='utf-8') as f:
            json.dump(deepspeed_config, f, indent=2, ensure_ascii=False)
        training_args.deepspeed = deepspeed_config_path
        logger.info(f"DeepSpeedé…ç½®å·²ä¿å­˜åˆ°: {deepspeed_config_path}")
    
    # è·å–æ¨¡å‹è·¯å¾„
    model_path = get_model_path(model_args.model_name_or_path, model_args.cache_dir)
    
    # åŠ è½½tokenizer
    logger.info("åŠ è½½tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True,
        cache_dir=model_args.cache_dir
    )
    
    # ç¡®ä¿æœ‰pad token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åˆ›å»ºé‡åŒ–é…ç½®
    quantization_config = create_quantization_config(model_args)
    
    # åŠ è½½æ¨¡å‹
    logger.info("åŠ è½½æ¨¡å‹...")
    model_kwargs = {
        "trust_remote_code": True,
        "cache_dir": model_args.cache_dir,
        "torch_dtype": torch.float16,
    }
    
    # DeepSpeedä¼šè‡ªåŠ¨å¤„ç†device_mapï¼Œæ‰€ä»¥åªåœ¨éDeepSpeedæ¨¡å¼ä¸‹è®¾ç½®
    if not model_args.use_deepspeed:
        model_kwargs["device_map"] = "auto"
    
    if quantization_config is not None:
        model_kwargs["quantization_config"] = quantization_config
    
    model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs)
    
    # å¦‚æœä½¿ç”¨QLoRAï¼Œå‡†å¤‡æ¨¡å‹è¿›è¡Œk-bitè®­ç»ƒ
    if model_args.use_qlora:
        logger.info("å‡†å¤‡æ¨¡å‹è¿›è¡Œk-bitè®­ç»ƒ...")
        model = prepare_model_for_kbit_training(model)
    
    # å¦‚æœä½¿ç”¨LoRAæˆ–QLoRAï¼Œåº”ç”¨LoRAé…ç½®
    if model_args.use_lora or model_args.use_qlora:
        logger.info("åº”ç”¨LoRAé…ç½®...")
        lora_config = create_lora_config(model_args)
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®štarget_modulesï¼Œè‡ªåŠ¨å‘ç°
        if lora_config.target_modules is None:
            lora_config.target_modules = find_all_linear_names(model)
            logger.info(f"è‡ªåŠ¨å‘ç°çš„ç›®æ ‡æ¨¡å—: {lora_config.target_modules}")
        
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()
    else:
        # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        model.gradient_checkpointing_enable()
    
    # åˆ›å»ºæ•°æ®é›†
    logger.info("åˆ›å»ºè®­ç»ƒæ•°æ®é›†...")
    train_dataset = SFTDataset(
        data_path=data_args.data_path,
        tokenizer=tokenizer,
        max_seq_length=data_args.max_seq_length
    )
    
    # åˆ›å»ºéªŒè¯æ•°æ®é›†
    if data_args.eval_data_path:
        logger.info(f"åˆ›å»ºéªŒè¯æ•°æ®é›†: {data_args.eval_data_path}")
        eval_dataset = SFTDataset(
            data_path=data_args.eval_data_path,
            tokenizer=tokenizer,
            max_seq_length=data_args.max_seq_length
        )
    else:
        logger.info("ä½¿ç”¨è®­ç»ƒæ•°æ®ä½œä¸ºéªŒè¯é›†")
        eval_dataset = train_dataset
    
    # æ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=model,
        label_pad_token_id=-100,
        pad_to_multiple_of=8
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
    )
    
    # å¼€å§‹è®­ç»ƒ
    logger.info("å¼€å§‹è®­ç»ƒ...")
    if training_args.report_to == "tensorboard":
        tensorboard_log_dir = os.path.join(training_args.output_dir, "tensorboard_logs")
        logger.info("ğŸ“Š TensorBoardå·²å¯ç”¨ï¼")
        logger.info(f"ğŸ“ˆ æŸ¥çœ‹è®­ç»ƒè¿›åº¦: tensorboard --logdir={tensorboard_log_dir}")
        logger.info("ğŸŒ ç„¶ååœ¨æµè§ˆå™¨æ‰“å¼€: http://localhost:6006")
    
    trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    logger.info("ä¿å­˜æ¨¡å‹...")
    if model_args.use_lora or model_args.use_qlora:
        # ä¿å­˜LoRAé€‚é…å™¨
        model.save_pretrained(training_args.output_dir)
        logger.info(f"LoRAé€‚é…å™¨å·²ä¿å­˜åˆ°: {training_args.output_dir}")
    else:
        # ä¿å­˜å®Œæ•´æ¨¡å‹
        trainer.save_model()
    
    tokenizer.save_pretrained(training_args.output_dir)
    logger.info("è®­ç»ƒå®Œæˆï¼")

if __name__ == "__main__":
    main() 