#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨ ModelScope å¾®è°ƒ Qwen3-0.5B æ¨¡å‹
ç”¨äºå°çº¢ä¹¦é£æ ¼åº—é“ºæ–‡æ¡ˆç”Ÿæˆä»»åŠ¡
"""

import os
import json
import torch
from dataclasses import dataclass, field
from typing import Optional, Dict, Any, List
import logging

# è®¾ç½®å›½å†…ç¯å¢ƒçš„é•œåƒæº
def setup_china_mirror():
    """ä¸ºå¤§é™†ç¯å¢ƒè®¾ç½®é•œåƒæº"""
    try:
        # è®¾ç½®HuggingFaceé•œåƒ
        os.environ.setdefault('HF_ENDPOINT', 'https://hf-mirror.com')
        print("âœ… å·²è®¾ç½®HuggingFaceé•œåƒæº: https://hf-mirror.com")
        return True
    except Exception as e:
        print(f"è®¾ç½®é•œåƒæºå¤±è´¥: {e}")
        return False

# é¦–å…ˆå°è¯•è®¾ç½®é•œåƒæº
setup_china_mirror()

# è®¾ç½®å…¼å®¹æ€§ç¯å¢ƒå˜é‡
os.environ.setdefault('TOKENIZERS_PARALLELISM', 'false')
os.environ.setdefault('TRANSFORMERS_NO_ADVISORY_WARNINGS', 'true')

# è®¾ç½®å…¼å®¹æ€§å¯¼å…¥
try:
    import transformers
    from transformers import (
        AutoTokenizer,
        AutoModelForCausalLM,
        TrainingArguments,
        Trainer,
        DataCollatorForSeq2Seq,
        set_seed,
        BitsAndBytesConfig
    )
    from torch.utils.data import Dataset
    from transformers.trainer_utils import IntervalStrategy, SaveStrategy
    
    # ModelScopeå°†åœ¨éœ€è¦æ—¶å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¯åŠ¨æ—¶çš„å…¼å®¹æ€§é—®é¢˜
    print("ğŸ“¦ ModelScope: å°†åœ¨éœ€è¦æ—¶åŠ¨æ€å¯¼å…¥")
    
    from peft import (
        LoraConfig,
        TaskType,
        get_peft_model,
        PeftModel,
        prepare_model_for_kbit_training
    )
    import bitsandbytes as bnb
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿å®‰è£…äº†æ­£ç¡®ç‰ˆæœ¬çš„ä¾èµ–åŒ…:")
    print("pip install torch==2.1.0 transformers>=4.36.2 peft bitsandbytes")
    print("å¤§é™†ç”¨æˆ·å¯é€‰æ‹©å®‰è£…: pip install modelscope")
    raise

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ModelArguments:
    """æ¨¡å‹ç›¸å…³å‚æ•°"""
    model_name_or_path: Optional[str] = field(
        default="Qwen/Qwen2.5-0.5B-Instruct",
        metadata={"help": "æ¨¡å‹åç§°æˆ–è·¯å¾„"}
    )
    cache_dir: Optional[str] = field(
        default=None,
        metadata={"help": "æ¨¡å‹ç¼“å­˜ç›®å½•"}
    )
    use_lora: bool = field(
        default=False,
        metadata={"help": "æ˜¯å¦ä½¿ç”¨LoRAå¾®è°ƒ"}
    )
    use_qlora: bool = field(
        default=False,
        metadata={"help": "æ˜¯å¦ä½¿ç”¨QLoRAå¾®è°ƒï¼ˆéœ€è¦é‡åŒ–ï¼‰"}
    )
    lora_r: int = field(
        default=64,
        metadata={"help": "LoRAçš„rank"}
    )
    lora_alpha: int = field(
        default=16,
        metadata={"help": "LoRAçš„alphaå‚æ•°"}
    )
    lora_dropout: float = field(
        default=0.1,
        metadata={"help": "LoRAçš„dropoutç‡"}
    )
    lora_target_modules: Optional[str] = field(
        default="q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj",
        metadata={"help": "LoRAç›®æ ‡æ¨¡å—ï¼Œé€—å·åˆ†éš”"}
    )
    quantization_bit: int = field(
        default=4,
        metadata={"help": "é‡åŒ–ä½æ•°ï¼Œæ”¯æŒ4æˆ–8ä½"}
    )

@dataclass
class DataArguments:
    """æ•°æ®ç›¸å…³å‚æ•°"""
    data_path: str = field(
        default="store_xhs_sft_samples.jsonl",
        metadata={"help": "è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒå•ä¸ªæ–‡ä»¶æˆ–å¤šä¸ªæ–‡ä»¶ï¼ˆé€—å·åˆ†éš”ï¼‰"}
    )
    max_seq_length: int = field(
        default=2048,
        metadata={"help": "æœ€å¤§åºåˆ—é•¿åº¦"}
    )
    eval_data_path: Optional[str] = field(
        default=None,
        metadata={"help": "éªŒè¯æ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œå¯é€‰ï¼Œæ”¯æŒå•ä¸ªæ–‡ä»¶æˆ–å¤šä¸ªæ–‡ä»¶ï¼ˆé€—å·åˆ†éš”ï¼‰"}
    )

@dataclass
class CustomTrainingArguments(TrainingArguments):
    """è®­ç»ƒç›¸å…³å‚æ•°"""
    output_dir: str = field(default="./output_qwen")
    overwrite_output_dir: bool = field(default=True)
    per_device_train_batch_size: int = field(default=2)
    per_device_eval_batch_size: int = field(default=2)
    gradient_accumulation_steps: int = field(default=8)
    learning_rate: float = field(default=2e-5)
    num_train_epochs: int = field(default=3)
    lr_scheduler_type: str = field(default="cosine")
    warmup_steps: int = field(default=100)
    logging_steps: int = field(default=10)
    save_steps: int = field(default=500)
    eval_steps: int = field(default=500)
    evaluation_strategy: IntervalStrategy = field(default=IntervalStrategy.STEPS)
    eval_strategy: IntervalStrategy = field(default=IntervalStrategy.STEPS)
    save_strategy: SaveStrategy = field(default=SaveStrategy.STEPS)
    save_total_limit: int = field(default=3)
    load_best_model_at_end: bool = field(default=True)
    metric_for_best_model: str = field(default="eval_loss")
    greater_is_better: bool = field(default=False)
    dataloader_num_workers: int = field(default=4)
    remove_unused_columns: bool = field(default=False)
    fp16: bool = field(default=True)
    deepspeed: Optional[str] = field(default=None)

class SFTDataset(Dataset):
    """SFTæ•°æ®é›†ç±»"""
    
    def __init__(self, data_path: str, tokenizer, max_seq_length: int = 2048):
        super().__init__()
        self.tokenizer = tokenizer
        self.max_seq_length = max_seq_length
        self.data = self.load_data(data_path)
        
    def load_data(self, data_path: str) -> List[Dict]:
        """åŠ è½½JSONLæ ¼å¼çš„æ•°æ®ï¼Œæ”¯æŒå•ä¸ªæ–‡ä»¶æˆ–å¤šä¸ªæ–‡ä»¶"""
        data = []
        
        # è§£ææ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒé€—å·åˆ†éš”çš„å¤šä¸ªæ–‡ä»¶
        if ',' in data_path:
            file_paths = [path.strip() for path in data_path.split(',') if path.strip()]
        else:
            file_paths = [data_path.strip()]
        
        logger.info(f"å‡†å¤‡åŠ è½½ {len(file_paths)} ä¸ªæ•°æ®æ–‡ä»¶: {file_paths}")
        
        total_loaded = 0
        for file_path in file_paths:
            if not os.path.exists(file_path):
                logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {file_path}")
                continue
                
            file_data = []
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line_num, line in enumerate(f, 1):
                        line = line.strip()
                        if line:
                            try:
                                item = json.loads(line)
                                if all(key in item for key in ['instruction', 'input', 'output']):
                                    file_data.append(item)
                                else:
                                    logger.warning(f"{file_path} ç¬¬{line_num}è¡Œç¼ºå°‘å¿…è¦å­—æ®µï¼Œè·³è¿‡")
                            except json.JSONDecodeError as e:
                                logger.warning(f"{file_path} ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
                                continue
                
                logger.info(f"ä» {file_path} åŠ è½½äº† {len(file_data)} æ¡æ•°æ®")
                data.extend(file_data)
                total_loaded += len(file_data)
                
            except Exception as e:
                logger.error(f"åŠ è½½æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
                raise
        
        if total_loaded == 0:
            raise ValueError("æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®")
            
        logger.info(f"æ€»å…±æˆåŠŸåŠ è½½ {total_loaded} æ¡è®­ç»ƒæ•°æ®")
        return data
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # æ„å»ºå¯¹è¯æ ¼å¼
        instruction = item['instruction']
        input_text = item['input']
        output_text = item['output']
        
        # ç»„åˆè¾“å…¥æ–‡æœ¬
        if input_text.strip():
            prompt = f"{instruction}\n\n{input_text}"
        else:
            prompt = instruction
            
        # æ„å»ºå®Œæ•´å¯¹è¯
        conversation = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n{output_text}<|im_end|>"
        
        # ç¼–ç 
        model_inputs = self.tokenizer(
            conversation,
            truncation=True,
            max_length=self.max_seq_length,
            padding=False,
            return_tensors=None
        )
        
        # åˆ›å»ºæ ‡ç­¾ï¼Œåªè®¡ç®—assistantéƒ¨åˆ†çš„æŸå¤±
        labels = model_inputs["input_ids"].copy()
        
        # æ‰¾åˆ°assistantå¼€å§‹çš„ä½ç½®
        assistant_start = conversation.find("<|im_start|>assistant\n") + len("<|im_start|>assistant\n")
        user_part = conversation[:assistant_start]
        user_tokens = self.tokenizer(user_part, add_special_tokens=False)["input_ids"]
        
        # å°†ç”¨æˆ·éƒ¨åˆ†çš„æ ‡ç­¾è®¾ä¸º-100ï¼ˆä¸è®¡ç®—æŸå¤±ï¼‰
        for i in range(min(len(user_tokens), len(labels))):
            labels[i] = -100
            
        model_inputs["labels"] = labels
        
        return model_inputs

def get_model_path(model_name: str, cache_dir: str = "./models") -> str:
    """è·å–æ¨¡å‹è·¯å¾„ï¼Œä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œå…¶æ¬¡ä½¿ç”¨HuggingFaceé•œåƒ"""
    logger.info(f"å‡†å¤‡åŠ è½½æ¨¡å‹: {model_name}")
    
    # åˆ›å»ºç¼“å­˜ç›®å½•
    os.makedirs(cache_dir, exist_ok=True)
    
    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰æœ¬åœ°æ¨¡å‹
    local_model_paths = [
        f"{cache_dir}/{model_name}",
        f"{cache_dir}/{model_name.split('/')[-1]}",  # è‡ªå®šä¹‰ç¼“å­˜ç›®å½•
        f"models/{model_name.split('/')[-1]}",  # models/Qwen2.5-0.5B-Instruct
        model_name  # å¦‚æœå·²ç»æ˜¯æœ¬åœ°è·¯å¾„
    ]
    
    for local_path in local_model_paths:
        if os.path.exists(local_path) and os.path.exists(os.path.join(local_path, "config.json")):
            logger.info(f"âœ… å‘ç°æœ¬åœ°æ¨¡å‹: {local_path}")
            return os.path.abspath(local_path)
    
    # å¦‚æœæ²¡æœ‰æœ¬åœ°æ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨æ¨¡å‹åç§°ï¼Œè®©transformerså¤„ç†ä¸‹è½½
    # HuggingFaceé•œåƒå·²åœ¨å¯åŠ¨æ—¶è®¾ç½®
    logger.info("ğŸ“¥ å°†ä»HuggingFaceé•œåƒä¸‹è½½æ¨¡å‹...")
    logger.info(f"   é•œåƒæº: {os.environ.get('HF_ENDPOINT', 'https://huggingface.co')}")
    logger.info("   å¦‚æœä¸‹è½½å¤±è´¥ï¼Œè¯·è¿è¡Œ: ./download_model.sh")
    
    return model_name

def create_quantization_config(model_args: ModelArguments) -> Optional[BitsAndBytesConfig]:
    """åˆ›å»ºé‡åŒ–é…ç½®"""
    if not model_args.use_qlora:
        return None
    
    if model_args.quantization_bit == 4:
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
        )
    elif model_args.quantization_bit == 8:
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„é‡åŒ–ä½æ•°: {model_args.quantization_bit}")
    
    logger.info(f"ä½¿ç”¨{model_args.quantization_bit}ä½é‡åŒ–")
    return quantization_config

def create_lora_config(model_args: ModelArguments) -> Optional[LoraConfig]:
    """åˆ›å»ºLoRAé…ç½®"""
    if not (model_args.use_lora or model_args.use_qlora):
        return None
    
    target_modules = model_args.lora_target_modules.split(',') if model_args.lora_target_modules else None
    
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=model_args.lora_r,
        lora_alpha=model_args.lora_alpha,
        lora_dropout=model_args.lora_dropout,
        target_modules=target_modules,
        bias="none",
    )
    
    logger.info(f"LoRAé…ç½®: r={model_args.lora_r}, alpha={model_args.lora_alpha}, "
                f"dropout={model_args.lora_dropout}, target_modules={target_modules}")
    return lora_config

def find_all_linear_names(model):
    """æ‰¾åˆ°æ¨¡å‹ä¸­æ‰€æœ‰çš„çº¿æ€§å±‚åç§°"""
    cls = bnb.nn.Linear4bit if hasattr(bnb.nn, 'Linear4bit') else torch.nn.Linear
    lora_module_names = set()
    for name, module in model.named_modules():
        if isinstance(module, cls):
            names = name.split('.')
            lora_module_names.add(names[0] if len(names) == 1 else names[-1])
    
    if 'lm_head' in lora_module_names:
        lora_module_names.remove('lm_head')
    return list(lora_module_names)

def main():
    # è§£æå‚æ•°
    parser = transformers.HfArgumentParser((ModelArguments, DataArguments, CustomTrainingArguments))
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é…ç½®æ–‡ä»¶å‚æ•°
    import sys
    if len(sys.argv) >= 3 and sys.argv[1] == '--config_file':
        config_file = sys.argv[2]
        logger.info(f"ä½¿ç”¨é…ç½®æ–‡ä»¶: {config_file}")
        
        # è¯»å–JSONé…ç½®æ–‡ä»¶å¹¶è¿‡æ»¤æ‰éå‚æ•°å­—æ®µ
        import json
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # è¿‡æ»¤æ‰ä»¥ä¸‹åˆ’çº¿å¼€å¤´çš„å­—æ®µï¼ˆé€šå¸¸æ˜¯æ³¨é‡Šæˆ–æ–‡æ¡£å­—æ®µï¼‰
        filtered_config = {k: v for k, v in config.items() if not k.startswith('_')}
        logger.info(f"è¿‡æ»¤åçš„é…ç½®å‚æ•°: {list(filtered_config.keys())}")
        
        model_args, data_args, training_args = parser.parse_dict(filtered_config)
    else:
        model_args, data_args, training_args = parser.parse_args_into_dataclasses()
    
    # è®¾ç½®éšæœºç§å­
    set_seed(training_args.seed if hasattr(training_args, 'seed') else 42)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(training_args.output_dir, exist_ok=True)
    
    # è·å–æ¨¡å‹è·¯å¾„
    logger.info("å‡†å¤‡åŠ è½½æ¨¡å‹...")
    model_path = get_model_path(model_args.model_name_or_path, model_args.cache_dir)
    
    # åŠ è½½tokenizer
    logger.info("åŠ è½½tokenizer...")
    logger.info(f"æ¨¡å‹è·¯å¾„: {model_path}")
    logger.info(f"ç¼“å­˜ç›®å½•: {model_args.cache_dir}")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            use_fast=False,
            cache_dir=model_args.cache_dir
        )
        logger.info("âœ… TokenizeråŠ è½½æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ TokenizeråŠ è½½å¤±è´¥: {e}")
        logger.info("å°è¯•è§£å†³æ–¹æ¡ˆ...")
        
        # å°è¯•ä½¿ç”¨ä¸åŒçš„tokenizeré…ç½®
        try:
            logger.info("å°è¯•æ–¹æ¡ˆ1: ä½¿ç”¨fast tokenizer")
            tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True,
                use_fast=True,
                cache_dir=model_args.cache_dir
            )
            logger.info("âœ… Fast tokenizeråŠ è½½æˆåŠŸ")
        except Exception as e2:
            logger.error(f"âŒ Fast tokenizerä¹Ÿå¤±è´¥: {e2}")
            
            # å¦‚æœæ˜¯Qwenæ¨¡å‹ï¼Œå°è¯•ä½¿ç”¨å¤‡é€‰tokenizer
            if "Qwen" in model_path:
                try:
                    logger.info("å°è¯•æ–¹æ¡ˆ2: ä½¿ç”¨Qwen2Tokenizer")
                    from transformers import Qwen2Tokenizer
                    tokenizer = Qwen2Tokenizer.from_pretrained(
                        model_path,
                        cache_dir=model_args.cache_dir
                    )
                    logger.info("âœ… Qwen2TokenizeråŠ è½½æˆåŠŸ")
                except Exception as e3:
                    logger.error(f"âŒ Qwen2Tokenizerä¹Ÿå¤±è´¥: {e3}")
                    
                    logger.error("æ‰€æœ‰tokenizeråŠ è½½æ–¹æ¡ˆéƒ½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´")
                    logger.info("å»ºè®®è§£å†³æ–¹æ¡ˆ:")
                    logger.info("1. æ¸…ç©ºç¼“å­˜ç›®å½•: rm -rf ./models")
                    logger.info("2. é‡æ–°ä¸‹è½½æ¨¡å‹: ./download_model.sh")
                    logger.info("3. æˆ–ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„")
                    raise e3
            else:
                raise e2
    
    # ç¡®ä¿æœ‰pad token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åˆ›å»ºé‡åŒ–é…ç½®
    quantization_config = create_quantization_config(model_args)
    
    # åŠ è½½æ¨¡å‹
    logger.info("åŠ è½½æ¨¡å‹...")
    
    # åŸºç¡€å‚æ•°
    base_kwargs = {
        "trust_remote_code": True,
        "cache_dir": model_args.cache_dir,
    }
    
    if quantization_config is not None:
        base_kwargs["quantization_config"] = quantization_config
        base_kwargs["torch_dtype"] = torch.float16
    else:
        base_kwargs["torch_dtype"] = torch.float16 if training_args.fp16 else torch.float32
    
    # å°è¯•ä¸åŒçš„åŠ è½½æ–¹æ¡ˆ
    loading_strategies = [
        # æ–¹æ¡ˆ1: åŸºç¡€åŠ è½½ï¼Œä¸ä½¿ç”¨ device_map å’Œ attn_implementation
        base_kwargs,
        
        # æ–¹æ¡ˆ2: æ·»åŠ  low_cpu_mem_usage
        {**base_kwargs, "low_cpu_mem_usage": True},
        
        # æ–¹æ¡ˆ3: ä½¿ç”¨ device_map="cpu"
        {**base_kwargs, "device_map": "cpu"},
        
        # æ–¹æ¡ˆ4: æ·»åŠ  attn_implementation
        {**base_kwargs, "attn_implementation": "eager"},
        
        # æ–¹æ¡ˆ5: å®Œæ•´å‚æ•°
        {**base_kwargs, "device_map": "auto", "attn_implementation": "eager"},
    ]
    
    model = None
    for i, kwargs in enumerate(loading_strategies, 1):
        try:
            logger.info(f"å°è¯•åŠ è½½æ–¹æ¡ˆ {i}: {list(kwargs.keys())}")
            model = AutoModelForCausalLM.from_pretrained(model_path, **kwargs)
            logger.info(f"âœ… æ–¹æ¡ˆ {i} åŠ è½½æˆåŠŸ")
            break
        except Exception as e:
            logger.warning(f"âŒ æ–¹æ¡ˆ {i} å¤±è´¥: {e}")
            if i == len(loading_strategies):
                logger.error(f"æ‰€æœ‰åŠ è½½æ–¹æ¡ˆéƒ½å¤±è´¥ï¼Œæœ€åé”™è¯¯: {e}")
                raise e
    
    # å¦‚æœä½¿ç”¨QLoRAï¼Œå‡†å¤‡æ¨¡å‹è¿›è¡Œk-bitè®­ç»ƒ
    if model_args.use_qlora:
        logger.info("å‡†å¤‡æ¨¡å‹è¿›è¡Œk-bitè®­ç»ƒ...")
        model = prepare_model_for_kbit_training(model)
    
    # å¦‚æœä½¿ç”¨LoRAæˆ–QLoRAï¼Œåº”ç”¨LoRAé…ç½®
    if model_args.use_lora or model_args.use_qlora:
        logger.info("åº”ç”¨LoRAé…ç½®...")
        lora_config = create_lora_config(model_args)
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®štarget_modulesï¼Œè‡ªåŠ¨å‘ç°
        if lora_config.target_modules is None:
            logger.info("è‡ªåŠ¨å‘ç°LoRAç›®æ ‡æ¨¡å—...")
            lora_config.target_modules = find_all_linear_names(model)
            logger.info(f"å‘ç°çš„ç›®æ ‡æ¨¡å—: {lora_config.target_modules}")
        
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()
    else:
        # ç¡®ä¿æ¨¡å‹æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆä»…åœ¨éLoRAæ¨¡å¼ä¸‹ï¼‰
        model.gradient_checkpointing_enable()
    
    # åˆ›å»ºæ•°æ®é›†
    logger.info("åˆ›å»ºè®­ç»ƒæ•°æ®é›†...")
    train_dataset = SFTDataset(
        data_path=data_args.data_path,
        tokenizer=tokenizer,
        max_seq_length=data_args.max_seq_length
    )
    
    # åˆ›å»ºéªŒè¯æ•°æ®é›†
    if data_args.eval_data_path:
        logger.info(f"ä½¿ç”¨éªŒè¯æ•°æ®: {data_args.eval_data_path}")
        eval_dataset = SFTDataset(
            data_path=data_args.eval_data_path,
            tokenizer=tokenizer,
            max_seq_length=data_args.max_seq_length
        )
    else:
        logger.info("æœªæŒ‡å®šéªŒè¯æ•°æ®ï¼Œä½¿ç”¨è®­ç»ƒæ•°æ®ä½œä¸ºéªŒè¯é›†")
    eval_dataset = train_dataset
    
    # æ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=model,
        label_pad_token_id=-100,
        pad_to_multiple_of=8
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
    )
    
    # å¼€å§‹è®­ç»ƒ
    logger.info("å¼€å§‹è®­ç»ƒ...")
    trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    logger.info("ä¿å­˜æ¨¡å‹...")
    if model_args.use_lora or model_args.use_qlora:
        # ä¿å­˜LoRAé€‚é…å™¨
        model.save_pretrained(training_args.output_dir)
        logger.info(f"LoRAé€‚é…å™¨å·²ä¿å­˜åˆ°: {training_args.output_dir}")
        
        # å¦‚æœéœ€è¦ï¼Œä¹Ÿå¯ä»¥ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
        # merged_model = model.merge_and_unload()
        # merged_model.save_pretrained(os.path.join(training_args.output_dir, "merged_model"))
    else:
        # ä¿å­˜å®Œæ•´æ¨¡å‹
        trainer.save_model()
    
    tokenizer.save_pretrained(training_args.output_dir)
    
    logger.info("è®­ç»ƒå®Œæˆï¼")

if __name__ == "__main__":
    main() 