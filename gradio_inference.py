#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Gradio Webç•Œé¢ç”¨äºå¾®è°ƒæ¨¡å‹æ¨ç†
æ”¯æŒLoRAã€å®Œæ•´å¾®è°ƒ(Full FT)å’ŒQLoRAæ¨¡å‹
"""

import os
import json
import torch
import gradio as gr
import logging
import threading
import time
from typing import Optional, Tuple, List
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig
from peft import PeftModel, PeftConfig
import gc

# è®¾ç½®å…¼å®¹æ€§ç¯å¢ƒå˜é‡
os.environ.setdefault('TOKENIZERS_PARALLELISM', 'false')
os.environ.setdefault('TRANSFORMERS_NO_ADVISORY_WARNINGS', 'true')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def apply_transformers_patch():
    """
    åº”ç”¨monkey patchæ¥ä¿®å¤transformersåº“çš„NoneTypeé”™è¯¯
    """
    try:
        import transformers.modeling_utils as modeling_utils
        
        # ä¿å­˜åŸå§‹çš„post_initæ–¹æ³•
        original_post_init = modeling_utils.PreTrainedModel.post_init
        
        def patched_post_init(self):
            """ä¿®å¤åçš„post_initæ–¹æ³•"""
            try:
                # æ£€æŸ¥å¹¶ä¿®å¤å¯èƒ½çš„Noneå€¼
                if hasattr(self.config, 'pretraining_tp') and self.config.pretraining_tp is None:
                    self.config.pretraining_tp = 1
                
                # ä¿®å¤å¼ é‡å¹¶è¡Œæ ·å¼é”™è¯¯
                tensor_parallel_attrs = ['tensor_parallel_style', 'parallel_style']
                supported_styles = ['tp', 'dp', 'pp', 'cp']
                
                for attr in tensor_parallel_attrs:
                    if hasattr(self.config, attr):
                        current_value = getattr(self.config, attr)
                        if current_value is not None and current_value not in supported_styles:
                            setattr(self.config, attr, 'tp')
                
                # æ£€æŸ¥å…¶ä»–å¯èƒ½çš„Noneå€¼å’Œæ— æ•ˆå€¼
                config_fixes = {
                    'attn_implementation': 'eager',
                    'rope_scaling': None,
                    'use_sliding_window': False,
                    'sliding_window': 4096,
                    'max_window_layers': 28,
                    'attention_dropout': 0.0,
                    'tensor_parallel_style': 'tp',
                    'parallel_style': 'tp', 
                    'tensor_parallel': False,
                    'sequence_parallel': False,
                }
                
                for key, default_value in config_fixes.items():
                    if hasattr(self.config, key) and getattr(self.config, key) is None:
                        setattr(self.config, key, default_value)
                
                return original_post_init(self)
                
            except Exception as e:
                logger.warning(f"post_initä¿®å¤è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                pass
        
        # åº”ç”¨patch
        modeling_utils.PreTrainedModel.post_init = patched_post_init
        logger.info("âœ… å·²åº”ç”¨transformers post_initä¿®å¤è¡¥ä¸")
        return True
        
    except Exception as e:
        logger.warning(f"âŒ æ— æ³•åº”ç”¨transformersè¡¥ä¸: {e}")
        return False

def load_model_with_patch(model_path: str, **kwargs):
    """
    åŠ è½½æ¨¡å‹å¹¶ä¿®å¤å¯èƒ½çš„é…ç½®é—®é¢˜
    """
    try:
        # åº”ç”¨transformersè¡¥ä¸
        apply_transformers_patch()
        
        logger.info(f"æ­£åœ¨åŠ è½½é…ç½®: {model_path}")
        config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
        
        # åŸºç¡€ä¿®å¤
        config_fixes = {
            'attn_implementation': 'eager',
            'pretraining_tp': 1,
            'torch_dtype': torch.float16,
            'use_cache': True,
            'attention_dropout': 0.0,
            'hidden_dropout': 0.0,
            'rope_scaling': None,
            'tie_word_embeddings': False,
            '_name_or_path': model_path,
            'tensor_parallel_style': 'tp',
            'parallel_style': 'tp',
            'tensor_parallel': False,
            'sequence_parallel': False,
        }
        
        # åº”ç”¨ä¿®å¤
        for key, default_value in config_fixes.items():
            if not hasattr(config, key) or getattr(config, key) is None:
                setattr(config, key, default_value)
        
        # ä½¿ç”¨ä¿®å¤åçš„é…ç½®
        kwargs['config'] = config
        kwargs.setdefault('trust_remote_code', True)
        kwargs.setdefault('torch_dtype', torch.float16)
        
        model = AutoModelForCausalLM.from_pretrained(model_path, **kwargs)
        logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        
        return model
        
    except Exception as e:
        logger.error(f"ä½¿ç”¨è¡¥ä¸æ–¹æ³•åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        raise e

class ModelInference:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.model_type = None
        self.loaded_model_path = None
        self.loaded_lora_path = None
        
    def load_model(self, model_path: str, model_type: str, lora_path: Optional[str] = None, 
                   quantization: str = "none") -> Tuple[str, str]:
        """
        åŠ è½½æ¨¡å‹
        
        Args:
            model_path: åŸºç¡€æ¨¡å‹è·¯å¾„
            model_type: æ¨¡å‹ç±»å‹ (lora, full_ft, qlora)
            lora_path: LoRAé€‚é…å™¨è·¯å¾„ï¼ˆä»…å½“model_typeä¸ºloraæˆ–qloraæ—¶éœ€è¦ï¼‰
            quantization: é‡åŒ–ç±»å‹ (none, 4bit, 8bit)
            
        Returns:
            åŠ è½½çŠ¶æ€ä¿¡æ¯
        """
        try:
            # æ¸…ç†ä¹‹å‰çš„æ¨¡å‹
            if self.model is not None:
                del self.model
                del self.tokenizer
                gc.collect()
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
            
            # åŠ è½½tokenizer
            logger.info("åŠ è½½tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True,
                use_fast=False
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # è®¾ç½®é‡åŒ–é…ç½®
            quantization_config = None
            if quantization == "4bit":
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                )
            elif quantization == "8bit":
                quantization_config = BitsAndBytesConfig(load_in_8bit=True)
            
            # è®¾ç½®æ¨¡å‹åŠ è½½å‚æ•°
            model_kwargs = {
                "torch_dtype": torch.float16,
                "device_map": "auto",
                "trust_remote_code": True,
                "low_cpu_mem_usage": True,
            }
            
            if quantization_config is not None:
                model_kwargs["quantization_config"] = quantization_config
            
            # åŠ è½½åŸºç¡€æ¨¡å‹
            logger.info(f"åŠ è½½åŸºç¡€æ¨¡å‹: {model_path}")
            if model_type == "full_ft":
                # å®Œæ•´å¾®è°ƒæ¨¡å‹ç›´æ¥åŠ è½½
                self.model = load_model_with_patch(model_path, **model_kwargs)
            else:
                # LoRAæˆ–QLoRAéœ€è¦å…ˆåŠ è½½åŸºç¡€æ¨¡å‹
                self.model = load_model_with_patch(model_path, **model_kwargs)
                
                # åŠ è½½LoRAé€‚é…å™¨
                if lora_path and os.path.exists(lora_path):
                    logger.info(f"åŠ è½½LoRAé€‚é…å™¨: {lora_path}")
                    self.model = PeftModel.from_pretrained(self.model, lora_path)
                else:
                    return "âŒ é”™è¯¯", f"LoRAè·¯å¾„ä¸å­˜åœ¨æˆ–æœªæä¾›: {lora_path}"
            
            self.model_type = model_type
            self.loaded_model_path = model_path
            self.loaded_lora_path = lora_path
            
            # æ¨¡å‹ä¿¡æ¯
            device_info = f"è®¾å¤‡: {next(self.model.parameters()).device}"
            model_info = f"æ¨¡å‹ç±»å‹: {model_type}, é‡åŒ–: {quantization}"
            
            success_msg = f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼\n{model_info}\n{device_info}"
            logger.info(success_msg)
            
            return "âœ… æˆåŠŸ", success_msg
            
        except Exception as e:
            error_msg = f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return "âŒ å¤±è´¥", error_msg
    
    def generate_response(self, prompt: str, max_length: int = 512, temperature: float = 0.7,
                         top_p: float = 0.9, top_k: int = 50, repetition_penalty: float = 1.1, debug: bool = False, stream: bool = False):
        """
        ç”Ÿæˆå›å¤ï¼Œæ”¯æŒæµå¼æˆ–ä¸€æ¬¡æ€§ç”Ÿæˆ
        
        Args:
            stream: æ˜¯å¦ä½¿ç”¨æµå¼ç”Ÿæˆ
            
        Returns:
            å¦‚æœstream=Trueï¼Œè¿”å›ç”Ÿæˆå™¨ï¼›å¦åˆ™è¿”å›å­—ç¬¦ä¸²
        """
        if self.model is None or self.tokenizer is None:
            if stream:
                yield "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"
                return
            else:
                return "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"
        
        try:
            # æ„å»ºå¯¹è¯æ ¼å¼
            conversation = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
            
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(conversation, return_tensors="pt").to(self.model.device)
            input_length = inputs.input_ids.shape[1]
            
            # ç”Ÿæˆå‚æ•°
            generate_kwargs = {
                "max_length": min(max_length, 2048),
                "temperature": temperature,
                "top_p": top_p,
                "top_k": top_k,
                "repetition_penalty": repetition_penalty,
                "do_sample": True if temperature > 0 else False,
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
                "no_repeat_ngram_size": 3,
            }
            
            if stream:
                # æµå¼ç”Ÿæˆ - æš‚æ—¶ç¦ç”¨ï¼Œé¿å…å…¼å®¹æ€§é—®é¢˜
                logger.warning("æµå¼ç”Ÿæˆæš‚æ—¶ç¦ç”¨ï¼Œä½¿ç”¨æ™®é€šæ¨¡å¼")
                # return self._generate_stream(inputs, generate_kwargs, conversation, prompt, input_length, debug)
            
            # ä½¿ç”¨æ™®é€šæ¨¡å¼ç”Ÿæˆ
                # ä¸€æ¬¡æ€§ç”Ÿæˆ
                with torch.no_grad():
                    outputs = self.model.generate(
                        inputs.input_ids,
                        attention_mask=inputs.attention_mask,
                        **generate_kwargs
                    )
                
                # è§£ç è¾“å‡º
                generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # è°ƒè¯•è¾“å‡º
            if debug:
                print(f"ğŸ” åŸå§‹ç”Ÿæˆæ–‡æœ¬: {repr(generated_text)}")
                print(f"ğŸ” è¾“å…¥æç¤º: {repr(conversation)}")
            
            # æå–assistantçš„å›å¤ - æ”¹è¿›çš„æå–é€»è¾‘
            response = generated_text
            
            # æ–¹æ³•1ï¼šåŸºäºå¯¹è¯æ ¼å¼æå–
            if "<|im_start|>assistant\n" in generated_text:
                parts = generated_text.split("<|im_start|>assistant\n", 1)
                if len(parts) > 1:
                    response = parts[1]
                    # ç§»é™¤ç»“æŸæ ‡è®°
                    if "<|im_end|>" in response:
                        response = response.split("<|im_end|>")[0]
            
            # æ–¹æ³•2ï¼šå¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†æ ¼å¼ï¼Œå°è¯•ç§»é™¤è¾“å…¥éƒ¨åˆ†
            elif conversation in generated_text:
                response = generated_text.replace(conversation, "").strip()
            
            # æ–¹æ³•3ï¼šå¦‚æœåŒ…å«ç”¨æˆ·è¾“å…¥ï¼Œç§»é™¤ç”¨æˆ·è¾“å…¥éƒ¨åˆ†
            elif prompt in response:
                # æ‰¾åˆ°ç”¨æˆ·è¾“å…¥åçš„å†…å®¹
                prompt_index = response.find(prompt)
                if prompt_index != -1:
                    # ä»ç”¨æˆ·è¾“å…¥ç»“æŸåå¼€å§‹æå–
                    after_prompt = response[prompt_index + len(prompt):].strip()
                    if after_prompt:
                        response = after_prompt
            
            # æ¸…ç†å›å¤æ–‡æœ¬
            response = self._clean_response(response, conversation, prompt)
            
            # è°ƒè¯•è¾“å‡º
            if debug:
                print(f"ğŸ” æ¸…ç†åå›å¤: {repr(response)}")
            
            # å¦‚æœå›å¤ä¸ºç©ºæˆ–å¤ªçŸ­ï¼Œè¿”å›å‹å¥½æ¶ˆæ¯
            if not response or len(response.strip()) < 2:
                return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆæœ‰æ•ˆçš„å›å¤ã€‚è¯·å°è¯•è°ƒæ•´å‚æ•°æˆ–é‡æ–°è¾“å…¥ã€‚"
            
            return response
                
        except Exception as e:
            error_msg = f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"
            logger.error(error_msg)
            if stream:
                yield error_msg
            else:
                return error_msg
    
    def _generate_stream(self, inputs, generate_kwargs, conversation, prompt, input_length, debug):
        """
        æµå¼ç”Ÿæˆå›å¤ï¼Œä½¿ç”¨ç®€å•çš„é€æ­¥ç”Ÿæˆå®ç°
        """
        try:
            # è®¡ç®—æœ€å¤§æ–°tokenæ•°
            max_new_tokens = min(generate_kwargs.get("max_length", 512) - input_length, 1024)
            
            current_ids = inputs.input_ids.clone()
            current_attention = inputs.attention_mask.clone()
            current_response = ""
            
            # é€æ­¥ç”Ÿæˆ
            with torch.no_grad():
                for step in range(max_new_tokens):
                    # ç”Ÿæˆä¸‹ä¸€ä¸ªtoken
                    outputs = self.model.generate(
                        current_ids,
                        attention_mask=current_attention,
                        max_new_tokens=1,
                        temperature=generate_kwargs["temperature"],
                        top_p=generate_kwargs["top_p"],
                        top_k=generate_kwargs["top_k"],
                        repetition_penalty=generate_kwargs["repetition_penalty"],
                        do_sample=generate_kwargs["do_sample"],
                        pad_token_id=generate_kwargs["pad_token_id"],
                        eos_token_id=generate_kwargs["eos_token_id"],
                    )
                    
                    # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†ç»“æŸtoken
                    new_token_id = outputs[0, -1].item()
                    if new_token_id == self.tokenizer.eos_token_id:
                        break
                    
                    # æ›´æ–°åºåˆ—
                    current_ids = outputs
                    current_attention = torch.ones_like(current_ids)
                    
                    # è§£ç åˆ°ç›®å‰ä¸ºæ­¢çš„ç”Ÿæˆå†…å®¹
                    generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                    
                    # æå–å›å¤éƒ¨åˆ†
                    new_response = self._extract_and_clean_streaming_response(
                        generated_text, conversation, prompt
                    )
                    
                    # åªæœ‰å½“å›å¤æœ‰å®é™…æ›´æ–°æ—¶æ‰yield
                    if new_response != current_response and len(new_response.strip()) > 0:
                        current_response = new_response
                        
                        # è°ƒè¯•è¾“å‡º
                        if debug:
                            new_token = self.tokenizer.decode([new_token_id], skip_special_tokens=True)
                            logger.info(f"ğŸ” æ–°token: {repr(new_token)} -> å½“å‰å›å¤: {repr(current_response[:50])}...")
                        
                        yield current_response
                        
                        # å°å»¶è¿Ÿè®©ç•Œé¢æ›´æ–°
                        time.sleep(0.05)
            
            # æœ€ç»ˆæ¸…ç†
            if current_response:
                final_response = self._clean_response(current_response, conversation, prompt)
                if final_response != current_response and len(final_response.strip()) > 0:
                    yield final_response
            
            # å¦‚æœæ²¡æœ‰ç”Ÿæˆä»»ä½•å†…å®¹
            if not current_response.strip():
                yield "æŠ±æ­‰ï¼Œæ¨¡å‹æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆå†…å®¹ï¼Œè¯·é‡è¯•ã€‚"
            
        except Exception as e:
            error_msg = f"âŒ æµå¼ç”Ÿæˆå¤±è´¥: {str(e)}"
            logger.error(error_msg)
            yield error_msg
    
    def _extract_and_clean_streaming_response(self, buffer, conversation, prompt):
        """
        ä»æµå¼ç”Ÿæˆçš„ç¼“å†²åŒºä¸­æå–å¹¶æ¸…ç†å›å¤
        """
        # ç§»é™¤å¯èƒ½çš„å¯¹è¯æ ¼å¼æ ‡è®°
        response = buffer
        
        # ç§»é™¤å¸¸è§çš„æ ‡è®°
        markers_to_remove = [
            "<|im_start|>", "<|im_end|>", 
            "user\n", "assistant\n", "system\n",
            "User:", "Assistant:", "System:"
        ]
        
        for marker in markers_to_remove:
            response = response.replace(marker, "")
        
        # ç§»é™¤å¼€å¤´çš„å†’å·å’Œç©ºæ ¼
        response = response.lstrip(': \n\t')
        
        # å¦‚æœåŒ…å«ç”¨æˆ·è¾“å…¥ï¼Œå°è¯•ç§»é™¤
        if prompt in response:
            parts = response.split(prompt, 1)
            if len(parts) > 1:
                response = parts[1].strip()
        
        # ç§»é™¤å¤šä½™çš„æ¢è¡Œ
        response = response.strip()
        
        return response
    
    def _extract_response_part(self, generated_text, conversation, prompt):
        """
        ä»ç”Ÿæˆçš„æ–‡æœ¬ä¸­æå–å›å¤éƒ¨åˆ†
        """
        response = generated_text
        
        # æ–¹æ³•1ï¼šåŸºäºå¯¹è¯æ ¼å¼æå–
        if "<|im_start|>assistant\n" in generated_text:
            parts = generated_text.split("<|im_start|>assistant\n", 1)
            if len(parts) > 1:
                response = parts[1]
                # ç§»é™¤ç»“æŸæ ‡è®°
                if "<|im_end|>" in response:
                    response = response.split("<|im_end|>")[0]
        
        # æ–¹æ³•2ï¼šå¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†æ ¼å¼ï¼Œå°è¯•ç§»é™¤è¾“å…¥éƒ¨åˆ†
        elif conversation in generated_text:
            response = generated_text.replace(conversation, "").strip()
        
        # æ–¹æ³•3ï¼šå¦‚æœåŒ…å«ç”¨æˆ·è¾“å…¥ï¼Œç§»é™¤ç”¨æˆ·è¾“å…¥éƒ¨åˆ†
        elif prompt in response:
            prompt_index = response.find(prompt)
            if prompt_index != -1:
                after_prompt = response[prompt_index + len(prompt):].strip()
                if after_prompt:
                    response = after_prompt
        
        # åŸºæœ¬æ¸…ç†
        response = response.replace("<|im_start|>", "")
        response = response.replace("<|im_end|>", "")
        response = response.replace("user\n", "")
        response = response.replace("assistant\n", "")
        response = response.strip()
        
        return response
    
    def _clean_response(self, response: str, conversation: str, prompt: str) -> str:
        """
        æ¸…ç†å›å¤æ–‡æœ¬ï¼Œç§»é™¤ä¸å¿…è¦çš„æ ¼å¼æ ‡è®°å’Œé‡å¤å†…å®¹
        """
        # ç§»é™¤å¯¹è¯æ ¼å¼æ ‡è®°
        response = response.replace("<|im_start|>", "")
        response = response.replace("<|im_end|>", "")
        response = response.replace("user\n", "")
        response = response.replace("assistant\n", "")
        
        # ç§»é™¤å¯èƒ½çš„ç³»ç»Ÿè§’è‰²æ ‡è®°
        response = response.replace("system\n", "")
        response = response.replace("System:", "")
        response = response.replace("User:", "")
        response = response.replace("Assistant:", "")
        
        # ç§»é™¤é‡å¤çš„è¾“å…¥å†…å®¹
        if prompt in response:
            response = response.replace(prompt, "").strip()
        
        # ç§»é™¤å¯èƒ½çš„å¯¹è¯å¼€å§‹ç¬¦å·
        if response.startswith("user\n") or response.startswith("assistant\n"):
            lines = response.split('\n', 1)
            if len(lines) > 1:
                response = lines[1]
        
        # ç§»é™¤å¤šä½™çš„æ¢è¡Œå’Œç©ºæ ¼
        response = response.strip()
        
        # ç§»é™¤å¼€å¤´çš„å†’å·å’Œç©ºæ ¼
        response = response.lstrip(': \n\t')
        
        # å¦‚æœå›å¤å¤ªé•¿ï¼Œå¯èƒ½åŒ…å«äº†å¤šè½®å¯¹è¯ï¼Œåªå–ç¬¬ä¸€éƒ¨åˆ†
        if len(response) > 1000:
            # æŸ¥æ‰¾å¯èƒ½çš„å¯¹è¯åˆ†å‰²ç‚¹
            split_markers = [
                "<|im_start|>user",
                "<|im_start|>assistant", 
                "user\n",
                "assistant\n",
                "\n\nuser:",
                "\n\nassistant:"
            ]
            
            for marker in split_markers:
                if marker in response:
                    response = response.split(marker)[0].strip()
                    break
        
        return response
    
    def get_model_info(self) -> str:
        """è·å–å½“å‰æ¨¡å‹ä¿¡æ¯"""
        if self.model is None:
            return "âŒ æœªåŠ è½½æ¨¡å‹"
        
        try:
            # åŸºæœ¬ä¿¡æ¯
            info = []
            info.append(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯")
            info.append(f"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
            info.append(f"ç±»å‹: {self.model_type}")
            info.append(f"åŸºç¡€æ¨¡å‹: {self.loaded_model_path}")
            
            if self.loaded_lora_path:
                info.append(f"LoRAè·¯å¾„: {self.loaded_lora_path}")
            
            # è®¾å¤‡ä¿¡æ¯
            device = next(self.model.parameters()).device
            info.append(f"è®¾å¤‡: {device}")
            
            # å‚æ•°ç»Ÿè®¡
            if hasattr(self.model, 'print_trainable_parameters'):
                # PEFTæ¨¡å‹
                trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
                total_params = sum(p.numel() for p in self.model.parameters())
                info.append(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
                info.append(f"æ€»å‚æ•°: {total_params:,}")
                info.append(f"è®­ç»ƒå‚æ•°æ¯”ä¾‹: {100 * trainable_params / total_params:.2f}%")
            else:
                # æ™®é€šæ¨¡å‹
                total_params = sum(p.numel() for p in self.model.parameters())
                info.append(f"æ€»å‚æ•°: {total_params:,}")
            
            return "\n".join(info)
            
        except Exception as e:
            return f"âŒ è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {str(e)}"

# å…¨å±€æ¨¡å‹å®ä¾‹
model_inference = ModelInference()

def create_gradio_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    
    # è‡ªå®šä¹‰CSS
    custom_css = """
    .gradio-container {
        max-width: 1200px !important;
        margin: auto !important;
    }
    .model-info {
        background-color: #f0f0f0;
        padding: 10px;
        border-radius: 5px;
        font-family: monospace;
        white-space: pre-wrap;
    }
    .status-success {
        color: #28a745;
        font-weight: bold;
    }
    .status-error {
        color: #dc3545;
        font-weight: bold;
    }
    """
    
    with gr.Blocks(css=custom_css, title="ğŸ¤– å¾®è°ƒæ¨¡å‹æ¨ç†ç³»ç»Ÿ") as demo:
        gr.Markdown("# ğŸ¤– å¾®è°ƒæ¨¡å‹æ¨ç†ç³»ç»Ÿ")
        gr.Markdown("æ”¯æŒLoRAã€å®Œæ•´å¾®è°ƒ(Full FT)å’ŒQLoRAæ¨¡å‹çš„åŠ è½½ä¸æ¨ç†")
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("## ğŸ“‚ æ¨¡å‹é…ç½®")
                
                # æ¨¡å‹ç±»å‹é€‰æ‹©
                model_type = gr.Radio(
                    choices=["lora", "full_ft", "qlora"],
                    value="lora",
                    label="æ¨¡å‹ç±»å‹",
                    info="é€‰æ‹©å¾®è°ƒæ¨¡å‹ç±»å‹"
                )
                
                # åŸºç¡€æ¨¡å‹è·¯å¾„
                base_model_path = gr.Textbox(
                    value="Qwen/Qwen2.5-0.5B-Instruct",
                    label="åŸºç¡€æ¨¡å‹è·¯å¾„",
                    info="HuggingFaceæ¨¡å‹åç§°æˆ–æœ¬åœ°è·¯å¾„"
                )
                
                # LoRAé€‚é…å™¨è·¯å¾„
                lora_path = gr.Textbox(
                    value="./output_qwen",
                    label="LoRAé€‚é…å™¨è·¯å¾„",
                    info="LoRA/QLoRAé€‚é…å™¨çš„ä¿å­˜è·¯å¾„",
                    visible=True
                )
                
                # é‡åŒ–é€‰é¡¹
                quantization = gr.Radio(
                    choices=["none", "4bit", "8bit"],
                    value="none",
                    label="é‡åŒ–ç±»å‹",
                    info="é€‰æ‹©é‡åŒ–æ–¹å¼ä»¥èŠ‚çœæ˜¾å­˜"
                )
                
                # åŠ è½½æŒ‰é’®
                load_btn = gr.Button("ğŸ”„ åŠ è½½æ¨¡å‹", variant="primary")
                
                # åŠ è½½çŠ¶æ€
                load_status = gr.Textbox(
                    label="åŠ è½½çŠ¶æ€",
                    interactive=False,
                    lines=3
                )
                
                # æ¨¡å‹ä¿¡æ¯
                model_info_display = gr.Textbox(
                    label="æ¨¡å‹ä¿¡æ¯",
                    interactive=False,
                    lines=8,
                    elem_classes=["model-info"]
                )
                
                # è·å–æ¨¡å‹ä¿¡æ¯æŒ‰é’®
                info_btn = gr.Button("ğŸ“Š è·å–æ¨¡å‹ä¿¡æ¯")
                
            with gr.Column(scale=2):
                gr.Markdown("## ğŸ’¬ å¯¹è¯ç”Ÿæˆ")
                
                # èŠå¤©å†å²
                chatbot = gr.Chatbot(
                    label="å¯¹è¯å†å²",
                    height=400,
                    avatar_images=["ğŸ‘¤", "ğŸ¤–"],
                    type="tuples"
                )
                
                # ç”¨æˆ·è¾“å…¥
                user_input = gr.Textbox(
                    label="è¾“å…¥æ¶ˆæ¯",
                    placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",
                    lines=2
                )
                
                with gr.Row():
                    send_btn = gr.Button("ğŸ“¤ å‘é€", variant="primary")
                    clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯")
                
                gr.Markdown("## âš™ï¸ ç”Ÿæˆå‚æ•°")
                
                with gr.Row():
                    max_length = gr.Slider(
                        minimum=50,
                        maximum=2048,
                        value=512,
                        step=50,
                        label="æœ€å¤§é•¿åº¦"
                    )
                    
                    temperature = gr.Slider(
                        minimum=0.1,
                        maximum=2.0,
                        value=0.7,
                        step=0.1,
                        label="æ¸©åº¦"
                    )
                
                with gr.Row():
                    top_p = gr.Slider(
                        minimum=0.1,
                        maximum=1.0,
                        value=0.9,
                        step=0.05,
                        label="Top-p"
                    )
                    
                    top_k = gr.Slider(
                        minimum=1,
                        maximum=100,
                        value=50,
                        step=1,
                        label="Top-k"
                    )
                
                repetition_penalty = gr.Slider(
                    minimum=1.0,
                    maximum=2.0,
                    value=1.1,
                    step=0.1,
                    label="é‡å¤æƒ©ç½š"
                )
                
                # è°ƒè¯•é€‰é¡¹
                with gr.Row():
                    debug_mode = gr.Checkbox(
                        label="ğŸ” è°ƒè¯•æ¨¡å¼",
                        value=False,
                        info="æ˜¾ç¤ºè¯¦ç»†çš„ç”Ÿæˆè¿‡ç¨‹ä¿¡æ¯"
                    )
                    
                    stream_mode = gr.Checkbox(
                        label="âš¡ æµå¼ç”Ÿæˆ",
                        value=False,  # é»˜è®¤å…³é—­ï¼Œé¿å…å…¼å®¹æ€§é—®é¢˜
                        info="æš‚æ—¶ç¦ç”¨ï¼Œä½¿ç”¨æ™®é€šç”Ÿæˆæ¨¡å¼",
                        interactive=False  # æš‚æ—¶ç¦ç”¨äº¤äº’
                    )
        
        # æ§åˆ¶LoRAè·¯å¾„æ˜¾ç¤º
        def update_lora_visibility(model_type_value):
            return gr.update(visible=model_type_value in ["lora", "qlora"])
        
        model_type.change(
            fn=update_lora_visibility,
            inputs=[model_type],
            outputs=[lora_path]
        )
        
        # åŠ è½½æ¨¡å‹
        def load_model_wrapper(model_type_val, base_model_val, lora_path_val, quantization_val):
            if model_type_val in ["lora", "qlora"] and not lora_path_val:
                return "âŒ é”™è¯¯", "è¯·æä¾›LoRAé€‚é…å™¨è·¯å¾„"
            
            status, message = model_inference.load_model(
                base_model_val, 
                model_type_val, 
                lora_path_val if model_type_val in ["lora", "qlora"] else None,
                quantization_val
            )
            return message
        
        load_btn.click(
            fn=load_model_wrapper,
            inputs=[model_type, base_model_path, lora_path, quantization],
            outputs=[load_status]
        )
        
        # è·å–æ¨¡å‹ä¿¡æ¯
        info_btn.click(
            fn=lambda: model_inference.get_model_info(),
            outputs=[model_info_display]
        )
        
        # å‘é€æ¶ˆæ¯
        def send_message(history, message, max_len, temp, top_p_val, top_k_val, rep_penalty, debug, stream):
            if not message.strip():
                return history, ""
            
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²è®°å½•
            history.append([message, ""])
            
            try:
                if stream:
                    # æµå¼ç”Ÿæˆ
                    try:
                        response_generator = model_inference.generate_response(
                            message, max_len, temp, top_p_val, top_k_val, rep_penalty, debug, stream=True
                        )
                        
                        # æ£€æŸ¥æ˜¯å¦çœŸçš„æ˜¯ç”Ÿæˆå™¨
                        if hasattr(response_generator, '__iter__') and hasattr(response_generator, '__next__'):
                            # å…ˆè¿”å›ç©ºçš„å›å¤ï¼Œç„¶åé€æ­¥æ›´æ–°
                            yield history, ""
                            
                            for partial_response in response_generator:
                                # ç¡®ä¿partial_responseæ˜¯å­—ç¬¦ä¸²
                                if not isinstance(partial_response, str):
                                    partial_response = str(partial_response)
                                
                                # åœ¨è°ƒè¯•æ¨¡å¼ä¸‹ï¼Œæ·»åŠ é¢å¤–ä¿¡æ¯
                                display_response = partial_response
                                if debug and not partial_response.startswith("âŒ"):
                                    debug_info = f"\n\n[è°ƒè¯•ä¿¡æ¯] æ¨¡å‹ç±»å‹: {model_inference.model_type or 'æœªåŠ è½½'}"
                                    if model_inference.loaded_model_path:
                                        debug_info += f"\n[è°ƒè¯•ä¿¡æ¯] åŸºç¡€æ¨¡å‹: {model_inference.loaded_model_path}"
                                    if model_inference.loaded_lora_path:
                                        debug_info += f"\n[è°ƒè¯•ä¿¡æ¯] LoRAè·¯å¾„: {model_inference.loaded_lora_path}"
                                    display_response = partial_response + debug_info
                                
                                # æ›´æ–°å¯¹è¯å†å²ä¸­çš„æœ€åä¸€æ¡æ¶ˆæ¯
                                history[-1][1] = display_response
                                yield history, ""
                        else:
                            # å¦‚æœä¸æ˜¯ç”Ÿæˆå™¨ï¼Œå½“ä½œæ™®é€šå“åº”å¤„ç†
                            response = str(response_generator)
                            history[-1][1] = response
                            yield history, ""
                            
                    except Exception as stream_error:
                        logger.error(f"æµå¼ç”Ÿæˆé”™è¯¯: {stream_error}")
                        # å›é€€åˆ°éæµå¼æ¨¡å¼
                        history[-1][1] = "âš ï¸ æµå¼ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨æ™®é€šæ¨¡å¼..."
                        yield history, ""
                        
                        response = model_inference.generate_response(
                            message, max_len, temp, top_p_val, top_k_val, rep_penalty, debug, stream=False
                        )
                        
                        if not isinstance(response, str):
                            response = str(response)
                        
                        history[-1][1] = response
                        yield history, ""
                        
                else:
                    # ä¸€æ¬¡æ€§ç”Ÿæˆ - å…ˆæ˜¾ç¤ºæ­£åœ¨ç”Ÿæˆçš„çŠ¶æ€
                    history[-1][1] = "ğŸ¤” æ­£åœ¨æ€è€ƒä¸­..."
                    yield history, ""
                    
                    response = model_inference.generate_response(
                        message, max_len, temp, top_p_val, top_k_val, rep_penalty, debug, stream=False
                    )
                    
                    # ç¡®ä¿å“åº”æ˜¯å­—ç¬¦ä¸²
                    if not isinstance(response, str):
                        response = str(response)
                    
                    # åœ¨è°ƒè¯•æ¨¡å¼ä¸‹ï¼Œæ·»åŠ é¢å¤–ä¿¡æ¯
                    if debug and not response.startswith("âŒ"):
                        response += f"\n\n[è°ƒè¯•ä¿¡æ¯] æ¨¡å‹ç±»å‹: {model_inference.model_type or 'æœªåŠ è½½'}"
                        if model_inference.loaded_model_path:
                            response += f"\n[è°ƒè¯•ä¿¡æ¯] åŸºç¡€æ¨¡å‹: {model_inference.loaded_model_path}"
                        if model_inference.loaded_lora_path:
                            response += f"\n[è°ƒè¯•ä¿¡æ¯] LoRAè·¯å¾„: {model_inference.loaded_lora_path}"
                    
                    # æ›´æ–°å¯¹è¯å†å²
                    history[-1][1] = response
                    yield history, ""
                    
            except Exception as e:
                error_msg = f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"
                logger.error(f"å‘é€æ¶ˆæ¯é”™è¯¯: {e}")
                history[-1][1] = error_msg
                yield history, ""
        
        send_btn.click(
            fn=send_message,
            inputs=[chatbot, user_input, max_length, temperature, top_p, top_k, repetition_penalty, debug_mode, stream_mode],
            outputs=[chatbot, user_input]
        )
        
        # å›è½¦å‘é€
        user_input.submit(
            fn=send_message,
            inputs=[chatbot, user_input, max_length, temperature, top_p, top_k, repetition_penalty, debug_mode, stream_mode],
            outputs=[chatbot, user_input]
        )
        
        # æ¸…ç©ºå¯¹è¯
        clear_btn.click(
            fn=lambda: [],
            outputs=[chatbot]
        )
        
        # ç¤ºä¾‹è¾“å…¥
        gr.Markdown("## ğŸ“ ç¤ºä¾‹è¾“å…¥")
        example_inputs = [
            "è¯·ä¸ºä¸€å®¶å’–å•¡åº—å†™ä¸€æ®µå°çº¢ä¹¦é£æ ¼çš„æ–‡æ¡ˆ",
            "å¦‚ä½•åˆ¶ä½œä¸€æ¯å®Œç¾çš„æ‹¿é“å’–å•¡ï¼Ÿ",
            "æ¨èå‡ å®¶ä¸Šæµ·çš„ç½‘çº¢å’–å•¡åº—",
            "å†™ä¸€ä¸ªå…³äºæ˜¥å¤©çš„çŸ­è¯—",
            "è§£é‡Šä¸€ä¸‹äººå·¥æ™ºèƒ½çš„åŸºæœ¬æ¦‚å¿µ"
        ]
        
        examples = gr.Examples(
            examples=example_inputs,
            inputs=user_input,
            label="ç‚¹å‡»ç¤ºä¾‹å¿«é€Ÿè¾“å…¥"
        )
        
        # é¡µé¢åº•éƒ¨ä¿¡æ¯
        gr.Markdown("""
        ---
        ### ğŸ“– ä½¿ç”¨è¯´æ˜
        1. **é€‰æ‹©æ¨¡å‹ç±»å‹**ï¼šLoRAã€å®Œæ•´å¾®è°ƒ(Full FT)æˆ–QLoRA
        2. **é…ç½®æ¨¡å‹è·¯å¾„**ï¼šè®¾ç½®åŸºç¡€æ¨¡å‹å’ŒLoRAé€‚é…å™¨è·¯å¾„
        3. **é€‰æ‹©é‡åŒ–æ–¹å¼**ï¼šå¯é€‰æ‹©4bitæˆ–8bité‡åŒ–ä»¥èŠ‚çœæ˜¾å­˜
        4. **åŠ è½½æ¨¡å‹**ï¼šç‚¹å‡»"åŠ è½½æ¨¡å‹"æŒ‰é’®
        5. **å¼€å§‹å¯¹è¯**ï¼šåœ¨è¾“å…¥æ¡†ä¸­è¾“å…¥é—®é¢˜å¹¶å‘é€
        6. **è°ƒæ•´å‚æ•°**ï¼šæ ¹æ®éœ€è¦è°ƒæ•´ç”Ÿæˆå‚æ•°
        
        ### ğŸ’¡ æç¤º
        - é¦–æ¬¡åŠ è½½æ¨¡å‹å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ä¸‹è½½
        - é‡åŒ–å¯ä»¥æ˜¾è‘—å‡å°‘æ˜¾å­˜ä½¿ç”¨ï¼Œä½†å¯èƒ½ç•¥å¾®å½±å“è´¨é‡
        - æ¸©åº¦è¶Šé«˜ç”Ÿæˆè¶Šéšæœºï¼Œè¶Šä½è¶Šç¡®å®š
        - Top-på’ŒTop-kæ§åˆ¶ç”Ÿæˆçš„å¤šæ ·æ€§
        
        ### âš¡ æµå¼ç”Ÿæˆï¼ˆæš‚æ—¶ç¦ç”¨ï¼‰
        - **å½“å‰çŠ¶æ€**ï¼šæµå¼ç”ŸæˆåŠŸèƒ½æš‚æ—¶ç¦ç”¨ï¼Œä½¿ç”¨æ™®é€šç”Ÿæˆæ¨¡å¼
        - **ç”Ÿæˆè¿‡ç¨‹**ï¼šä¼šæ˜¾ç¤º"æ­£åœ¨æ€è€ƒä¸­..."ç„¶åæ˜¾ç¤ºå®Œæ•´å›å¤
        - **ç¨³å®šæ€§**ï¼šæ™®é€šæ¨¡å¼æ›´ç¨³å®šï¼Œé¿å…å…¼å®¹æ€§é—®é¢˜
        - **åç»­ä¼˜åŒ–**ï¼šå°†åœ¨åç»­ç‰ˆæœ¬ä¸­é‡æ–°å¯ç”¨æµå¼ç”Ÿæˆ
        
        ### ğŸ”§ é—®é¢˜æ’æŸ¥
        - **å¦‚æœå›å¤åŒ…å«å¤šä½™å†…å®¹**ï¼šå¼€å¯è°ƒè¯•æ¨¡å¼æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯
        - **å¦‚æœå›å¤ä¸æ˜¯æœŸæœ›çš„é£æ ¼**ï¼šæ£€æŸ¥æ˜¯å¦åŠ è½½äº†æ­£ç¡®çš„å¾®è°ƒæ¨¡å‹
        - **å¦‚æœæ¨¡å‹å›å¤"æˆ‘æ˜¯é€šä¹‰åƒé—®"**ï¼šè¯´æ˜åŠ è½½çš„æ˜¯åŸºç¡€æ¨¡å‹ï¼Œè¯·æ£€æŸ¥LoRAè·¯å¾„
        - **å›å¤æ ¼å¼å¼‚å¸¸**ï¼šå°è¯•è°ƒæ•´æ¸©åº¦å’Œé‡å¤æƒ©ç½šå‚æ•°
        - **æµå¼ç”Ÿæˆå¡ä½**ï¼šå…³é—­æµå¼ç”Ÿæˆæˆ–é‡æ–°åŠ è½½æ¨¡å‹
        """)
    
    return demo

def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºç•Œé¢
    demo = create_gradio_interface()
    
    # å¯åŠ¨æœåŠ¡
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )

if __name__ == "__main__":
    main() 