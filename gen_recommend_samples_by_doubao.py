import os
import json
import time
import pandas as pd
from typing import List
from pathlib import Path
from volcenginesdkarkruntime import Ark


# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["ARK_API_KEY"] = "a50666ff-2b30-4ed1-aee1-f7468b57d1df"


def build_recommend_samples_from_csv(csv_path: str) -> List[dict]:
    """ä»åº—é“ºCSVæ„å»ºæ¨èæ ¼å¼æ ·æœ¬"""
    df = pd.read_csv(csv_path)
    recommend_samples = []

    for _, row in df.iterrows():
        if pd.isna(row["åº—é“ºåç§°"]) or pd.isna(row["äºŒçº§ä¸šæ€"]) or pd.isna(row["åº—é“ºæè¿°"]):
            continue

        category = row["äºŒçº§ä¸šæ€"]
        name = row["åº—é“ºåç§°"]
        floor = row["åº—é“ºåœ°å€"] if pd.notna(row["åº—é“ºåœ°å€"]) else "æœªçŸ¥æ¥¼å±‚"
        desc = row["åº—é“ºæè¿°"].strip()

        instruction = f"è¯·ä¸ºæˆ‘æ¨èä¸€å®¶{category}åº—é“º"
        input_text = ""

        output_text = f"""ğŸŒŸ ã€{name}ã€‘ï¼ˆ{floor}ï¼‰ {category}çˆ±å¥½è€…å¿…å»æ‰“å¡åœ°ï¼
æ¨èè¯­ï¼šä¸€å¥è¯æ€»ç»“åº—é“ºæ¨èè¯­ï¼ˆç”± LLM ç”Ÿæˆï¼‰
ğŸ“ äº®ç‚¹ï¼š{desc}...
ğŸ”¥ å¿…è¯•æ¨èï¼šè¯·è¡¥å……åº—å†…æ‹›ç‰Œèœã€é¥®å“æˆ–æœåŠ¡ç‰¹è‰²ã€‚
ğŸ’¡ å°è´´å£«ï¼šè¯·è¡¥å……é€‚åˆåœºæ™¯ã€æœåŠ¡äº®ç‚¹ã€å¥½è¯„å…³é”®è¯ç­‰ã€‚
ğŸ’¡ å¥½æ­æ¡£ï¼šè¯·æ¨èä¸€å®¶å¯ä¸ä¹‹æ­é…çš„åº—é“ºï¼Œå¹¶è¯´æ˜æ­é…ç†ç”±ã€‚"""

        sample = {
            "instruction": instruction,
            "input": input_text,
            "output": output_text
        }
        recommend_samples.append(sample)

    return recommend_samples


def generate_recommendation_content_with_doubao(
    samples: List[dict],
    output_path: str,
    max_samples: int = None,
    sleep_time: float = 0.3
) -> List[dict]:
    """è°ƒç”¨ Doubao API è¡¥å…¨æ¨èå†…å®¹ï¼ˆäº®ç‚¹/å¿…è¯•æ¨è/å°è´´å£«ï¼‰"""
    client = Ark(api_key=os.environ.get("ARK_API_KEY"))

    if max_samples:
        samples = samples[:max_samples]

    # load filled samples
    if os.path.exists(output_path):
        with open(output_path, "r", encoding="utf-8") as fin:
            filled_samples = [json.loads(line) for line in fin]
    else:
        filled_samples = []
    
    # fill samples
    for i, sample in enumerate(samples):
        if i < len(filled_samples):
            print(f"[{i+1}/{len(samples)}] âœ“ {sample['instruction']}")
            continue

        try:
            title_line = sample["output"].splitlines()[0]
            name = title_line.split("ã€")[1].split("ã€‘")[0]
            floor = title_line.split("ï¼‰")[0].split("ï¼ˆ")[-1]
            category = sample["instruction"].replace("è¯·ä¸ºæˆ‘æ¨èä¸€å®¶", "").replace("åº—é“º", "").strip()
            base_desc = sample["output"].split("äº®ç‚¹ï¼š")[1].split("...")[0]

            # ç”Ÿæˆ instruction å’Œ input
            instruction = f"æ ¹æ®æä¾›çš„åº—é“ºä¿¡æ¯ï¼Œä»¥å°çº¢ä¹¦é£æ ¼ç”Ÿæˆä¸€æ®µæ¨èå†…å®¹ï¼Œè¦æ±‚åŒ…å«ï¼šä¸€å¥è¯æ¨èè¯­ã€äº®ç‚¹ã€å¿…è¯•æ¨èã€å°è´´å£«ã€‚"
            sample["instruction"] = instruction

            input_text = f"""åº—é“ºåç§°ï¼š{name}
åœ°å€ï¼š{floor}
å“ç±»ï¼š{category}
ç®€ä»‹ï¼š{base_desc}
"""
            sample["input"] = input_text

            # ç”Ÿæˆ prompt
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ“…é•¿ç¾é£Ÿæ¨èçš„å°åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯è¡¥å……æ¨èå†…å®¹ï¼š
åº—é“ºåç§°ï¼š{name}
åœ°å€ï¼š{floor}
å“ç±»ï¼š{category}
ç®€ä»‹ï¼š{base_desc}
è¯·è¡¥å……ä»¥ä¸‹å†…å®¹ï¼š
(è¯·æ·»åŠ ä¸€å¥ç®€çŸ­çš„ã€é«˜åº¦æ€»ç»“çš„ã€å¸å¼•äººçš„æ¨èå¼€å¤´æè¿°è¿™å®¶åº—é“ºï¼Œç”¨äºæ”¾åœ¨è¿™é‡Œã€‚)
ğŸ“ äº®ç‚¹ï¼š
ğŸ”¥ å¿…è¯•æ¨èï¼š
ğŸ’¡ å°è´´å£«ï¼š
ï¼ˆè¯·ç”Ÿæˆå¸¦æ ‡ç­¾çš„å¤šè¡Œå†…å®¹ï¼‰"""
            
            response = client.chat.completions.create(
                model="doubao-1.5-pro-32k-250115",
                messages=[{"role": "user", "content": prompt}]
            )
            content = response.choices[0].message.content.strip()

#             output_text = f"""ğŸŒŸ ã€{name}ã€‘ï¼ˆ{floor}ï¼‰ {category}çˆ±å¥½è€…å¿…å»æ‰“å¡åœ°ï¼
# {content}
# ğŸ’¡ å¥½æ­æ¡£ï¼š"""
            output_text = content
            sample["output"] = output_text
            filled_samples.append(sample)

            print(f"[{i+1}/{len(samples)}] âœ“ {name}")
            time.sleep(sleep_time)

            if i % 10 == 0:
                with open(output_path, "w", encoding="utf-8") as fout:
                    for sample in filled_samples:
                        fout.write(json.dumps(sample, ensure_ascii=False) + "\n")

        except Exception as e:
            print(f"[{i+1}] âš ï¸ Error: {e}")
            continue

    # ä¿å­˜æ–°æ ·æœ¬
    with open(output_path, "w", encoding="utf-8") as fout:
        for s in filled_samples:
            fout.write(json.dumps(s, ensure_ascii=False) + "\n")

    print(f"âœ… å®Œæˆ {len(filled_samples)} æ¡æ¨èæ ·æœ¬ç”Ÿæˆï¼Œä¿å­˜åœ¨ï¼š{output_path}")
    return filled_samples


def standardize_instruction_input(
    original_jsonl_path: str,
    updated_jsonl_path: str
):
    updated_samples = []

    with open(original_jsonl_path, "r", encoding="utf-8") as f:
        for line in f:
            sample = json.loads(line.strip())

            # ä»åŸå§‹ input ä¸­æå–å­—æ®µï¼ˆå‡è®¾ input ä¸­æ˜¯ç»“æ„åŒ–å†…å®¹ï¼‰
            input_text = sample.get("input", "")
            # remove suffix
            input_text = input_text.split("è¯·è¡¥å……ä»¥ä¸‹å†…å®¹ï¼š")[0]
            input_text, base_desc = input_text.split("ç®€ä»‹ï¼š")
            input_text = input_text.strip()
            input_text, category = input_text.split("å“ç±»ï¼š")
            input_text = input_text.strip()
            input_text, floor = input_text.split("åœ°å€ï¼š")
            input_text = input_text.strip()
            input_text, name = input_text.split("åº—é“ºåç§°ï¼š")
            
            # æ›¿æ¢ instruction + input
            sample["instruction"] = (
                "æ ¹æ®æä¾›çš„åº—é“ºä¿¡æ¯ï¼Œä»¥å°çº¢ä¹¦é£æ ¼ç”Ÿæˆä¸€æ®µæ¨èå†…å®¹ï¼Œ"
                "è¦æ±‚åŒ…å«ï¼šä¸€å¥è¯æ¨èè¯­ã€äº®ç‚¹ã€å¿…è¯•æ¨èã€å°è´´å£«ã€‚"
            )

            sample["input"] = f"""åº—é“ºåç§°ï¼š{name}
åœ°å€ï¼š{floor}
å“ç±»ï¼š{category}
ç®€ä»‹ï¼š{base_desc}
"""
            updated_samples.append(sample)

    with open(updated_jsonl_path, "w", encoding="utf-8") as f:
        for s in updated_samples:
            f.write(json.dumps(s, ensure_ascii=False) + "\n")

    print(f"âœ… å·²æ›´æ–°å¹¶ä¿å­˜è‡³ï¼š{updated_jsonl_path}ï¼Œå…±å¤„ç† {len(updated_samples)} æ¡æ ·æœ¬ã€‚")


# ---------------------------- éé¤é¥®ç±»åº—é“º ----------------------------

def generate_recommendation_reason_nonfood(
    csv_path: str,
    output_path: str,
    max_samples: int = None,
    sleep_time: float = 0.3
):
    """ä¸ºéç¾é£Ÿç±»é—¨åº—ç”Ÿæˆæ¨èç†ç”±ï¼Œé€‚é…æ­£å¼˜åŸæ±‡æ€»éé¤é¥®ç±»æ•°æ®"""
    client = Ark(api_key=os.environ.get("ARK_API_KEY"))
    df = pd.read_csv(csv_path)
    output_samples = []

    for i, row in df.iterrows():
        try:
            if pd.isna(row["åº—é“ºåç§°"]) or pd.isna(row["äºŒçº§ä¸šæ€"]) or pd.isna(row["äº§å“æœåŠ¡"]) or pd.isna(row["åº—é“ºæè¿°"]):
                continue

            name = row["åº—é“ºåç§°"]
            category = str(row["äºŒçº§ä¸šæ€"]).strip()
            product = str(row["äº§å“æœåŠ¡"]).strip()
            score = str(row["åº—é“ºè¯„åˆ†"]).strip()
            price = str(row["äººå‡ä»·æ ¼"]).strip()
            label = str(row["åº—é“ºæ ‡ç­¾"]).strip()
            desc = row["åº—é“ºæè¿°"].strip()

            prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ“…é•¿æ’°å†™æ¨èæ–‡æ¡ˆçš„å°åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä»¥ä¸‹åº—é“ºä¿¡æ¯ï¼Œç”Ÿæˆç®€æ´æ˜äº†çš„æ¨èç†ç”±ï¼Œè¦æ±‚åŒ…å«ï¼šåº—é“ºç‰¹è‰²ã€äº§å“äº®ç‚¹ã€é€‚ç”¨åœºæ™¯ï¼Œé£æ ¼ç®€æ´ã€æ­£å¼ã€è¡¨è¾¾è‡ªç„¶ã€‚

åº—é“ºåç§°ï¼š{name}
ä¸»è¥ç±»å‹ï¼š{category}ï¼Œ{product}

ç®€ä»‹ï¼š{desc}

æ¨èç†ç”±ï¼š(è¯·ä¿ç•™æ ‡ç­¾)"""

            response = client.chat.completions.create(
                model="doubao-1.5-pro-32k-250115",
                messages=[{"role": "user", "content": prompt}]
            )

            reason = response.choices[0].message.content.strip()

            sample = {
                "instruction": f"ä½ æ˜¯ä¸€ä¸ªæ“…é•¿æ’°å†™æ¨èæ–‡æ¡ˆçš„å°åŠ©æ‰‹ï¼Œè¯·æ ¹æ®æä¾›çš„åº—é“ºä¿¡æ¯ï¼Œç”Ÿæˆç®€æ´æ˜äº†çš„æ¨èç†ç”±ï¼Œè¦æ±‚çªå‡ºåº—é“ºç‰¹è‰²ã€äº§å“äº®ç‚¹åŠé€‚ç”¨åœºæ™¯ã€‚",
                "input": f"åº—é“ºåç§°ï¼š{name}\nä¸»è¥ç±»å‹ï¼š{category}ï¼Œ{product}\nç®€ä»‹ï¼š{desc}",
                "output": reason
            }
            output_samples.append(sample)

            print(f"[{len(output_samples)}] âœ“ {name}")
            time.sleep(sleep_time)

            if max_samples and len(output_samples) >= max_samples:
                break

            if i % 10 == 0:
                with open(output_path, "w", encoding="utf-8") as fout:
                    for sample in output_samples:
                        fout.write(json.dumps(sample, ensure_ascii=False) + "\n")

        except Exception as e:
            print(f"âš ï¸ Error at row {i}: {e}")
            continue

    with open(output_path, "w", encoding="utf-8") as f:
        for s in output_samples:
            f.write(json.dumps(s, ensure_ascii=False) + "\n")

    print(f"âœ… å…±ç”Ÿæˆ {len(output_samples)} æ¡æ¨èç†ç”±ï¼Œå·²ä¿å­˜è‡³ {output_path}")


# âœ… ä¸»æ‰§è¡Œé€»è¾‘ï¼ˆç»„åˆè°ƒç”¨ï¼‰
if __name__ == "__main__":
    csv_path = "/Users/aibee/hwp/eventgpt/omni-mllm/xiaohongshu_data/æ­£å¼˜åŸçŸ¥è¯†åº“æ±‡æ€»-é¤é¥®åº—é“ºåˆ—è¡¨.csv"
    output_path = "/Users/aibee/hwp/eventgpt/omni-mllm/xiaohongshu_data/zhc_store_recommend_doubao.jsonl"

    # recommend_samples = build_recommend_samples_from_csv(csv_path)
    # print(f"å…±æ„å»ºåŸå§‹æ¨èæ ·æœ¬ï¼š{len(recommend_samples)} æ¡")

    # # å¯é€‰ï¼šä»…æµ‹è¯•å‰ 5 æ¡
    # generate_recommendation_content_with_doubao(
    #     samples=recommend_samples,
    #     output_path=output_path,
    #     max_samples=None  # âš ï¸ æ”¹ä¸º None å¤„ç†å…¨éƒ¨
    # )

    # This is temporary bug fix code
    # standardize_instruction_input(
    #     original_jsonl_path=output_path,
    #     updated_jsonl_path="/Users/aibee/hwp/eventgpt/omni-mllm/xiaohongshu_data/zhc_store_recommend_doubao_refined.jsonl"
    # )

    # éé¤é¥®ç±»æ ‡ç­¾
    generate_recommendation_reason_nonfood(
        csv_path="/Users/aibee/hwp/eventgpt/omni-mllm/xiaohongshu_data/æ­£å¼˜åŸçŸ¥è¯†åº“æ±‡æ€»-åº—é“ºæ•°æ®é™¤é¤é¥®å¤–.csv",
        output_path="/Users/aibee/hwp/eventgpt/omni-mllm/xiaohongshu_data/zhc_store_recommend_reason_doubao.jsonl",
        max_samples=None  # âš ï¸ æ”¹ä¸º None å¤„ç†å…¨éƒ¨
    )

    

