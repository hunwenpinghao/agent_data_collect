#!/usr/bin/env python3
"""
è½¬æ¢æ¨¡å‹æ ¼å¼ä»¥é€‚é…ç«å±±æ–¹èˆŸ
"""

import os
import torch
import argparse
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

def convert_lora_to_full_model(base_model_path, lora_path, output_path):
    """å°†LoRAæ¨¡å‹åˆå¹¶ä¸ºå®Œæ•´æ¨¡å‹"""
    print(f"åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
    
    try:
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # åŠ è½½LoRAé€‚é…å™¨
        print(f"åŠ è½½LoRAé€‚é…å™¨: {lora_path}")
        
        try:
            model = PeftModel.from_pretrained(base_model, lora_path)
        except TypeError as e:
            if "unexpected keyword argument" in str(e):
                print(f"âŒ PEFTç‰ˆæœ¬å…¼å®¹æ€§é”™è¯¯: {e}")
                print("ğŸ’¡ è¯·å…ˆè¿è¡Œä¿®å¤è„šæœ¬: python deploy/fix_lora_config.py")
                return False
            else:
                raise e
        
        # åˆå¹¶LoRAæƒé‡
        print("åˆå¹¶LoRAæƒé‡...")
        model = model.merge_and_unload()
        
        # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
        print(f"ä¿å­˜åˆå¹¶åçš„æ¨¡å‹: {output_path}")
        os.makedirs(output_path, exist_ok=True)
        
        model.save_pretrained(output_path, safe_serialization=True)
        tokenizer.save_pretrained(output_path)
        
        print("âœ… æ¨¡å‹è½¬æ¢å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âŒ è½¬æ¢è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description="è½¬æ¢LoRAæ¨¡å‹ä¸ºå®Œæ•´æ¨¡å‹")
    parser.add_argument("--base_model", type=str, default="models/Qwen/Qwen2.5-0.5B-Instruct",
                       help="åŸºç¡€æ¨¡å‹è·¯å¾„")
    parser.add_argument("--lora_path", type=str, 
                       default="output_deepspeed/zhc_xhs_qwen2.5_0.5b_lora",
                       help="LoRAæ¨¡å‹è·¯å¾„")
    parser.add_argument("--output_path", type=str, default="./merged_model",
                       help="è¾“å‡ºè·¯å¾„")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.base_model):
        print(f"âŒ åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.base_model}")
        exit(1)
    
    if not os.path.exists(args.lora_path):
        print(f"âŒ LoRAè·¯å¾„ä¸å­˜åœ¨: {args.lora_path}")
        exit(1)
    
    # æ‰§è¡Œè½¬æ¢
    success = convert_lora_to_full_model(args.base_model, args.lora_path, args.output_path)
    
    if success:
        print(f"\nğŸ‰ è½¬æ¢æˆåŠŸï¼åˆå¹¶åçš„æ¨¡å‹ä¿å­˜åœ¨: {args.output_path}")
    else:
        print(f"\nâŒ è½¬æ¢å¤±è´¥ï¼")
        exit(1)

if __name__ == "__main__":
    main() 