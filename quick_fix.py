#!/usr/bin/env python3
"""
å¿«é€Ÿä¿®å¤è„šæœ¬ï¼šè§£å†³ Qwen æ¨¡å‹çš„ NoneType é”™è¯¯
ä½¿ç”¨æ–¹æ³•ï¼šåœ¨è¿è¡Œ fine_tune_qwen.py ä¹‹å‰å…ˆè¿è¡Œè¿™ä¸ªè„šæœ¬
"""

import os
import sys
import logging

logger = logging.getLogger(__name__)

def apply_quick_fix():
    """åº”ç”¨å¿«é€Ÿä¿®å¤"""
    try:
        # ä¿®å¤æ–¹æ¡ˆ1: ä¸´æ—¶ä¿®æ”¹transformersæºç 
        logger.info("åº”ç”¨transformerså¿«é€Ÿä¿®å¤...")
        
        import transformers.modeling_utils as modeling_utils
        
        # ä¿å­˜åŸå§‹æ–¹æ³•
        original_post_init = modeling_utils.PreTrainedModel.post_init
        
        def fixed_post_init(self):
            """ä¿®å¤åçš„post_initæ–¹æ³•"""
            try:
                # å¼ºåˆ¶è®¾ç½®æ‰€æœ‰å¯èƒ½ä¸ºNoneçš„é…ç½®
                if hasattr(self.config, 'pretraining_tp'):
                    if self.config.pretraining_tp is None:
                        self.config.pretraining_tp = 1
                
                # æ£€æŸ¥å…¶ä»–å¸¸è§çš„Noneå€¼
                none_fixes = {
                    'attention_dropout': 0.0,
                    'rope_scaling': None,
                    'attn_implementation': 'eager',
                    'use_sliding_window': False,
                    'sliding_window': 4096
                }
                
                for key, default_val in none_fixes.items():
                    if hasattr(self.config, key) and getattr(self.config, key) is None:
                        setattr(self.config, key, default_val)
                
                # è°ƒç”¨åŸå§‹æ–¹æ³•
                return original_post_init(self)
                
            except Exception as e:
                # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œç›´æ¥è·³è¿‡post_init
                logger.warning(f"è·³è¿‡post_init: {e}")
                pass
        
        # åº”ç”¨ä¿®å¤
        modeling_utils.PreTrainedModel.post_init = fixed_post_init
        
        logger.info("âœ… å¿«é€Ÿä¿®å¤åº”ç”¨æˆåŠŸï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ å¿«é€Ÿä¿®å¤å¤±è´¥: {e}")
        return False

def test_fix():
    """æµ‹è¯•ä¿®å¤æ˜¯å¦æœ‰æ•ˆ"""
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch
        
        # æµ‹è¯•å°æ¨¡å‹
        model_name = "Qwen/Qwen2.5-0.5B-Instruct"
        
        logger.info(f"æµ‹è¯•æ¨¡å‹: {model_name}")
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="cpu",
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        logger.info("âœ… æ¨¡å‹åŠ è½½æµ‹è¯•æˆåŠŸï¼")
        del model
        return True
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    logging.basicConfig(level=logging.INFO)
    
    logger.info("=" * 50)
    logger.info("Qwen æ¨¡å‹ NoneType é”™è¯¯å¿«é€Ÿä¿®å¤")
    logger.info("=" * 50)
    
    # åº”ç”¨ä¿®å¤
    if apply_quick_fix():
        logger.info("ğŸ”§ ä¿®å¤å·²åº”ç”¨ï¼Œç°åœ¨æµ‹è¯•...")
        
        if test_fix():
            logger.info("\nğŸ‰ ä¿®å¤æˆåŠŸï¼ç°åœ¨å¯ä»¥è¿è¡Œ fine_tune_qwen.py")
            
            # æä¾›ä½¿ç”¨å»ºè®®
            logger.info("\nğŸ“‹ ä½¿ç”¨å»ºè®®:")
            logger.info("1. åœ¨Pythonä¼šè¯ä¸­å…ˆè¿è¡Œ: exec(open('quick_fix.py').read())")
            logger.info("2. ç„¶åè¿è¡Œä½ çš„è®­ç»ƒè„šæœ¬")
            logger.info("3. æˆ–è€…ç›´æ¥è¿è¡Œ: python -c \"exec(open('quick_fix.py').read()); exec(open('fine_tune_qwen.py').read())\"")
            
            return True
        else:
            logger.error("\nâŒ ä¿®å¤æµ‹è¯•å¤±è´¥")
    else:
        logger.error("\nâŒ æ— æ³•åº”ç”¨ä¿®å¤")
    
    logger.error("\nå»ºè®®å°è¯•å…¶ä»–è§£å†³æ–¹æ¡ˆ:")
    logger.error("1. pip install transformers>=4.51.0 --upgrade")
    logger.error("2. pip install transformers==4.36.2 --force-reinstall")
    logger.error("3. ä½¿ç”¨ä¸åŒçš„æ¨¡å‹")
    
    return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 