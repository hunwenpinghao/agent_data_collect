#!/usr/bin/env python3
"""
LoRAæ¨¡å‹ä¸Šä¼ å‡†å¤‡è„šæœ¬
ç­›é€‰éœ€è¦ä¸Šä¼ åˆ°é­”æ­ç¤¾åŒºçš„æ–‡ä»¶ï¼Œæ’é™¤è®­ç»ƒä¸´æ—¶æ–‡ä»¶
"""

import os
import shutil
import argparse
from pathlib import Path
import json

def create_lora_readme(output_dir, model_name):
    """ä¸ºLoRAæ¨¡å‹åˆ›å»ºREADME.md"""
    readme_content = f"""---
license: apache-2.0
base_model: Qwen/Qwen2.5-0.5B-Instruct
tags:
- qwen
- lora
- fine-tuned
- chinese
library_name: peft
---

# {model_name}

## æ¨¡å‹æè¿°

è¿™æ˜¯åŸºäº Qwen2.5-0.5B-Instruct å¾®è°ƒçš„ LoRA é€‚é…å™¨æ¨¡å‹ï¼Œä¸“é—¨é’ˆå¯¹å°çº¢ä¹¦æ•°æ®è¿›è¡Œäº†ä¼˜åŒ–ã€‚

## æ¨¡å‹ä¿¡æ¯

- **åŸºç¡€æ¨¡å‹**: Qwen/Qwen2.5-0.5B-Instruct
- **å¾®è°ƒæ–¹æ³•**: LoRA (Low-Rank Adaptation)
- **LoRA å‚æ•°**: r=64, alpha=16, dropout=0.1
- **ç›®æ ‡æ¨¡å—**: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj

## ä½¿ç”¨æ–¹æ³•

### é€šè¿‡ ModelScope ä½¿ç”¨

```python
from modelscope import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# åŠ è½½åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨
base_model_path = "qwen/Qwen2.5-0.5B-Instruct"
model = AutoModelForCausalLM.from_pretrained(base_model_path)
tokenizer = AutoTokenizer.from_pretrained(base_model_path)

# åŠ è½½LoRAé€‚é…å™¨
model = PeftModel.from_pretrained(model, "hunwenpinghao/{model_name}")

# ç”Ÿæˆæ–‡æœ¬
def generate_text(prompt, max_length=200):
    inputs = tokenizer(prompt, return_tensors="pt")
    with torch.no_grad():
        outputs = model.generate(
            inputs.input_ids,
            max_length=max_length,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# ç¤ºä¾‹ä½¿ç”¨
result = generate_text("å°çº¢ä¹¦ç§è‰æ–‡æ¡ˆï¼š")
print(result)
```

### é€šè¿‡ transformers + peft ä½¿ç”¨

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch

# åŠ è½½æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")
model = PeftModel.from_pretrained(base_model, "hunwenpinghao/{model_name}")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")

# æ¨ç†
prompt = "ä»Šå¤©çš„å¿ƒæƒ…ï¼š"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=100)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

## è®­ç»ƒè¯¦æƒ…

- **è®­ç»ƒæ•°æ®**: å°çº¢ä¹¦ç›¸å…³æ•°æ®é›†
- **è®­ç»ƒæ¡†æ¶**: Transformers + PEFT
- **ç¡¬ä»¶**: GPU è®­ç»ƒ
- **ä¼˜åŒ–å™¨**: AdamW

## è®¸å¯è¯

Apache-2.0

## æ³¨æ„äº‹é¡¹

è¿™æ˜¯ä¸€ä¸ª LoRA é€‚é…å™¨æ¨¡å‹ï¼Œéœ€è¦ä¸åŸºç¡€æ¨¡å‹ Qwen/Qwen2.5-0.5B-Instruct ä¸€èµ·ä½¿ç”¨ã€‚

## å¼•ç”¨

```bibtex
@misc{{{model_name.replace('-', '_')},
  author = {{hunwenpinghao}},
  title = {{{model_name}}},
  year = {{2025}},
  publisher = {{ModelScope}},
  journal = {{ModelScope Repository}},
  howpublished = {{\\url{{https://modelscope.cn/models/hunwenpinghao/{model_name}}}}}
}}
```
"""
    
    readme_path = Path(output_dir) / "README.md"
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write(readme_content)
    print(f"âœ… å·²åˆ›å»º LoRA æ¨¡å‹è¯´æ˜: {readme_path}")

def prepare_lora_files(source_dir, output_dir, model_name="zhc_xhs_qwen2.5_0.5b_instruct"):
    """å‡†å¤‡LoRAæ¨¡å‹ä¸Šä¼ æ–‡ä»¶"""
    
    # éœ€è¦ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨
    required_files = [
        'adapter_model.safetensors',  # LoRAæƒé‡
        'adapter_config.json',        # LoRAé…ç½®
        'tokenizer.json',             # åˆ†è¯å™¨
        'tokenizer_config.json',      # åˆ†è¯å™¨é…ç½®
        'vocab.json',                 # è¯æ±‡è¡¨
        'merges.txt',                 # BPEåˆå¹¶è§„åˆ™
        'special_tokens_map.json',    # ç‰¹æ®Štokenæ˜ å°„
        'added_tokens.json',          # æ·»åŠ çš„token
        'configuration.json',         # æ¨¡å‹é…ç½®
    ]
    
    # å¯é€‰æ–‡ä»¶
    optional_files = [
        '.gitattributes',             # Gitå±æ€§
    ]
    
    # ä¸éœ€è¦çš„æ–‡ä»¶ï¼ˆè®­ç»ƒä¸´æ—¶æ–‡ä»¶ï¼‰
    skip_files = [
        'optimizer.pt',               # ä¼˜åŒ–å™¨çŠ¶æ€
        'scaler.pt',                  # æ¢¯åº¦ç¼©æ”¾å™¨
        'scheduler.pt',               # è°ƒåº¦å™¨çŠ¶æ€
        'rng_state.pth',             # éšæœºæ•°çŠ¶æ€
        'training_args.bin',          # è®­ç»ƒå‚æ•°
        'trainer_state.json',         # è®­ç»ƒå™¨çŠ¶æ€
        'README.md',                  # åŸå§‹READMEï¼ˆä¼šé‡æ–°ç”Ÿæˆï¼‰
    ]
    
    source_path = Path(source_dir)
    output_path = Path(output_dir)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ“ æºç›®å½•: {source_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print("=" * 60)
    
    total_size = 0
    copied_files = 0
    skipped_files = 0
    
    # å¤åˆ¶å¿…éœ€æ–‡ä»¶
    for filename in required_files:
        source_file = source_path / filename
        if source_file.exists():
            dest_file = output_path / filename
            shutil.copy2(source_file, dest_file)
            size = source_file.stat().st_size
            total_size += size
            copied_files += 1
            size_str = format_size(size)
            print(f"âœ… {filename:<30} {size_str:>10} - å·²å¤åˆ¶")
        else:
            print(f"âš ï¸  {filename:<30} {'ç¼ºå¤±':>10} - å¿…éœ€æ–‡ä»¶ç¼ºå¤±")
    
    # å¤åˆ¶å¯é€‰æ–‡ä»¶
    for filename in optional_files:
        source_file = source_path / filename
        if source_file.exists():
            dest_file = output_path / filename
            shutil.copy2(source_file, dest_file)
            size = source_file.stat().st_size
            total_size += size
            copied_files += 1
            size_str = format_size(size)
            print(f"âœ… {filename:<30} {size_str:>10} - å·²å¤åˆ¶ï¼ˆå¯é€‰ï¼‰")
    
    # æ˜¾ç¤ºè·³è¿‡çš„æ–‡ä»¶
    for filename in skip_files:
        source_file = source_path / filename
        if source_file.exists():
            size = source_file.stat().st_size
            size_str = format_size(size)
            skipped_files += 1
            print(f"â­ï¸  {filename:<30} {size_str:>10} - å·²è·³è¿‡ï¼ˆè®­ç»ƒæ–‡ä»¶ï¼‰")
    
    # è·³è¿‡ .git ç›®å½•
    git_dir = source_path / '.git'
    if git_dir.exists():
        print(f"â­ï¸  {'.git/':<30} {'ç›®å½•':>10} - å·²è·³è¿‡ï¼ˆç‰ˆæœ¬æ§åˆ¶ï¼‰")
        skipped_files += 1
    
    # åˆ›å»ºæ–°çš„README
    create_lora_readme(output_dir, model_name)
    copied_files += 1
    
    print("=" * 60)
    print(f"ğŸ“Š æ€»ç»“:")
    print(f"   å·²å¤åˆ¶æ–‡ä»¶: {copied_files}")
    print(f"   è·³è¿‡æ–‡ä»¶: {skipped_files}")
    print(f"   æ€»å¤§å°: {format_size(total_size)}")
    
    return output_path

def format_size(size_bytes):
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
    if size_bytes < 1024:
        return f"{size_bytes} B"
    elif size_bytes < 1024 * 1024:
        return f"{size_bytes / 1024:.1f} KB"
    elif size_bytes < 1024 * 1024 * 1024:
        return f"{size_bytes / (1024 * 1024):.1f} MB"
    else:
        return f"{size_bytes / (1024 * 1024 * 1024):.1f} GB"

def main():
    parser = argparse.ArgumentParser(description="å‡†å¤‡LoRAæ¨¡å‹ä¸Šä¼ æ–‡ä»¶")
    parser.add_argument("--source_dir", type=str,
                       default="../output_qwen/zhc_xhs_qwen2.5_0.5b_instruct",
                       help="LoRAæ¨¡å‹æºç›®å½•")
    parser.add_argument("--output_dir", type=str,
                       default="../upload_ready/zhc_xhs_qwen2.5_0.5b_instruct",
                       help="å‡†å¤‡ä¸Šä¼ çš„ç›®å½•")
    parser.add_argument("--model_name", type=str,
                       default="zhc_xhs_qwen2.5_0.5b_instruct",
                       help="æ¨¡å‹åç§°")
    
    args = parser.parse_args()
    
    print("ğŸ”§ LoRAæ¨¡å‹ä¸Šä¼ å‡†å¤‡å·¥å…·")
    print("=" * 30)
    
    if not Path(args.source_dir).exists():
        print(f"âŒ æºç›®å½•ä¸å­˜åœ¨: {args.source_dir}")
        return 1
    
    try:
        output_path = prepare_lora_files(args.source_dir, args.output_dir, args.model_name)
        
        print("\nğŸ‰ å‡†å¤‡å®Œæˆï¼")
        print(f"ğŸ“ ä¸Šä¼ ç›®å½•: {output_path}")
        print("\nğŸš€ ä¸‹ä¸€æ­¥:")
        print(f"   cd upload_to_modelscope")
        print(f"   python3 upload_to_modelscope.py --model_dir {output_path} --token YOUR_TOKEN")
        
        return 0
        
    except Exception as e:
        print(f"âŒ å‡†å¤‡å¤±è´¥: {e}")
        return 1

if __name__ == "__main__":
    exit(main()) 