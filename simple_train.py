#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆæœ¬çš„å¾®è°ƒè„šæœ¬ï¼Œç”¨äºæ’æŸ¥å…¼å®¹æ€§é—®é¢˜
"""

import os
import sys
import json

def check_imports():
    """æ£€æŸ¥å¿…è¦çš„åŒ…å¯¼å…¥"""
    print("ğŸ” æ£€æŸ¥ä¾èµ–åŒ…...")
    
    try:
        import torch
        print(f"âœ… PyTorch: {torch.__version__}")
    except ImportError as e:
        print(f"âŒ PyTorch å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    try:
        import transformers
        print(f"âœ… Transformers: {transformers.__version__}")
    except ImportError as e:
        print(f"âŒ Transformers å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    try:
        from modelscope import snapshot_download
        print(f"âœ… ModelScope: å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ ModelScope å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    # æ£€æŸ¥å…·ä½“çš„å¯¼å…¥é—®é¢˜
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        print(f"âœ… AutoTokenizer, AutoModelForCausalLM: å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ AutoTokenizer/AutoModelForCausalLM å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    try:
        from transformers import TrainingArguments, Trainer
        print(f"âœ… TrainingArguments, Trainer: å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ TrainingArguments/Trainer å¯¼å…¥å¤±è´¥: {e}")
        print("è¿™å¯èƒ½æ˜¯ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ï¼Œå»ºè®®ä½¿ç”¨ç¨³å®šç‰ˆæœ¬:")
        print("pip install torch==2.1.0 transformers==4.36.2")
        return False
    
    return True

def test_data_loading():
    """æµ‹è¯•æ•°æ®åŠ è½½åŠŸèƒ½"""
    print("\nğŸ“ æµ‹è¯•æ•°æ®åŠ è½½åŠŸèƒ½...")
    
    test_data = {
        "instruction": "æ ¹æ®ä»¥ä¸‹åº—é“ºä¿¡æ¯ï¼Œç”Ÿæˆä¸€æ®µå°çº¢ä¹¦é£æ ¼çš„æ–‡æ¡ˆï¼Œé£æ ¼ä¸ºã€Œç”œç¾ã€ï¼š",
        "input": "åº—é“ºåç§°ï¼šæ˜Ÿå·´å…‹\nå“ç±»ï¼šå’–å•¡",
        "output": "âœ¨æ˜Ÿå·´å…‹çš„å’–å•¡é¦™æ°”ï¼Œæ²»æ„ˆæ¯ä¸€ä¸ªåˆåï½"
    }
    
    # æµ‹è¯•å¤šæ–‡ä»¶è·¯å¾„è§£æ
    def parse_data_path(data_path: str):
        if ',' in data_path:
            file_paths = [path.strip() for path in data_path.split(',') if path.strip()]
        else:
            file_paths = [data_path.strip()]
        return file_paths
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        "single_file.jsonl",
        "file1.jsonl,file2.jsonl",
        "file1.jsonl, file2.jsonl, file3.jsonl",
        " file1.jsonl , file2.jsonl,file3.jsonl "
    ]
    
    for test_case in test_cases:
        result = parse_data_path(test_case)
        print(f"è¾“å…¥: '{test_case}' -> è¾“å‡º: {result}")
    
    print("âœ… æ•°æ®è·¯å¾„è§£æåŠŸèƒ½æ­£å¸¸")

def test_model_download():
    """æµ‹è¯•æ¨¡å‹ä¸‹è½½åŠŸèƒ½"""
    print("\nğŸ“¥ æµ‹è¯•æ¨¡å‹ä¸‹è½½åŠŸèƒ½...")
    
    try:
        from modelscope import snapshot_download
        
        model_name = "qwen/Qwen3-0.5B-Instruct"
        print(f"å‡†å¤‡ä¸‹è½½æ¨¡å‹: {model_name}")
        print("æ³¨æ„: è¿™åªæ˜¯æµ‹è¯•å¯¼å…¥åŠŸèƒ½ï¼Œä¸ä¼šå®é™…ä¸‹è½½")
        print("âœ… æ¨¡å‹ä¸‹è½½åŠŸèƒ½å¯ç”¨")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹ä¸‹è½½åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")

def provide_solutions():
    """æä¾›è§£å†³æ–¹æ¡ˆ"""
    print("\nğŸ› ï¸ å¸¸è§é—®é¢˜è§£å†³æ–¹æ¡ˆ:")
    print("=" * 50)
    
    solutions = [
        {
            "é—®é¢˜": "LRScheduler æœªå®šä¹‰é”™è¯¯",
            "åŸå› ": "transformers ä¸ PyTorch ç‰ˆæœ¬ä¸å…¼å®¹",
            "è§£å†³æ–¹æ¡ˆ": [
                "pip uninstall torch transformers -y",
                "pip install torch==2.1.0 transformers==4.36.2",
                "æˆ–ä½¿ç”¨: pip install -r requirements_stable.txt"
            ]
        },
        {
            "é—®é¢˜": "ModuleNotFoundError",
            "åŸå› ": "ç¼ºå°‘å¿…è¦çš„ä¾èµ–åŒ…",
            "è§£å†³æ–¹æ¡ˆ": [
                "pip install -r requirements_stable.txt",
                "æˆ–æ‰‹åŠ¨å®‰è£…: pip install torch transformers modelscope"
            ]
        },
        {
            "é—®é¢˜": "CUDA ç›¸å…³é”™è¯¯",
            "åŸå› ": "CUDA ç‰ˆæœ¬ä¸åŒ¹é…",
            "è§£å†³æ–¹æ¡ˆ": [
                "æ£€æŸ¥ CUDA ç‰ˆæœ¬: nvidia-smi",
                "å®‰è£…å¯¹åº”çš„ PyTorch ç‰ˆæœ¬",
                "æˆ–ä½¿ç”¨ CPU æ¨¡å¼: CUDA_VISIBLE_DEVICES=\"\""
            ]
        }
    ]
    
    for i, solution in enumerate(solutions, 1):
        print(f"\n{i}. {solution['é—®é¢˜']}")
        print(f"   åŸå› : {solution['åŸå› ']}")
        print(f"   è§£å†³æ–¹æ¡ˆ:")
        for step in solution['è§£å†³æ–¹æ¡ˆ']:
            print(f"   - {step}")

def main():
    print("ğŸš€ Qwen3-0.5B å¾®è°ƒç¯å¢ƒè¯Šæ–­å·¥å…·")
    print("=" * 50)
    
    # æ£€æŸ¥ Python ç‰ˆæœ¬
    print(f"ğŸ Python ç‰ˆæœ¬: {sys.version}")
    
    # æ£€æŸ¥å¯¼å…¥
    if not check_imports():
        print("\nâŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œè¯·å‚è€ƒä¸‹é¢çš„è§£å†³æ–¹æ¡ˆ")
        provide_solutions()
        return False
    
    # æµ‹è¯•æ•°æ®åŠ è½½
    test_data_loading()
    
    # æµ‹è¯•æ¨¡å‹ä¸‹è½½
    test_model_download()
    
    print("\nğŸ‰ ç¯å¢ƒæ£€æŸ¥å®Œæˆï¼")
    print("âœ… æ‰€æœ‰ä¾èµ–åŒ…æ­£å¸¸ï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒ")
    print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("   1. è¿è¡Œä¸»è®­ç»ƒè„šæœ¬: python3 fine_tune_qwen.py --config_file train_config.json")
    print("   2. ä½¿ç”¨å¤šæ–‡ä»¶è®­ç»ƒ: python3 fine_tune_qwen.py --config_file train_config_multi_files.json")
    print("   3. å¦‚é‡é—®é¢˜å¯ä½¿ç”¨ Docker: ./build_docker.sh run")
    
    return True

if __name__ == "__main__":
    main() 